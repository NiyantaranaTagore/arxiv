[
    {
        "arxiv_id": "2502.02587v1",
        "title": "Spatio-temporal transformer to support automatic sign language translation",
        "abstract": "Sign Language Translation (SLT) systems support hearing-impaired people communication by finding equivalences between signed and spoken languages. This task is however challenging due to multiple sign variations, complexity in language and inherent richness of expressions. Computational approaches have evidenced capabilities to support SLT. Nonetheless, these approaches remain limited to cover gestures variability and support long sequence translations. This paper introduces a Transformer-based architecture that encodes spatio-temporal motion gestures, preserving both local and long-range spatial information through the use of multiple convolutional and attention mechanisms. The proposed approach was validated on the Colombian Sign Language Translation Dataset (CoL-SLTD) outperforming baseline approaches, and achieving a BLEU4 of 46.84%. Additionally, the proposed approach was validated on the RWTH-PHOENIX-Weather-2014T (PHOENIX14T), achieving a BLEU4 score of 30.77%, demonstrating its robustness and effectiveness in handling real-world variations",
        "authors": [
            "Christian Ruiz",
            "Fabio Martinez"
        ],
        "submitted_date": "2025-02-04T18:59:19+00:00",
        "pdf_url": "https://arxiv.org/pdf/2502.02587v1",
        "similarity_score": 0.9597651958465576
    },
    {
        "arxiv_id": "2503.19668v1",
        "title": "A multitask transformer to sign language translation using motion gesture primitives",
        "abstract": "The absence of effective communication the deaf population represents the main social gap in this community. Furthermore, the sign language, main deaf communication tool, is unlettered, i.e., there is no formal written representation. In consequence, main challenge today is the automatic translation among spatiotemporal sign representation and natural text language. Recent approaches are based on encoder-decoder architectures, where the most relevant strategies integrate attention modules to enhance non-linear correspondences, besides, many of these approximations require complex training and architectural schemes to achieve reasonable predictions, because of the absence of intermediate text projections. However, they are still limited by the redundant background information of the video sequences. This work introduces a multitask transformer architecture that includes a gloss learning representation to achieve a more suitable translation. The proposed approach also includes a dense motion representation that enhances gestures and includes kinematic information, a key component in sign language. From this representation it is possible to avoid background information and exploit the geometry of the signs, in addition, it includes spatiotemporal representations that facilitate the alignment between gestures and glosses as an intermediate textual representation. The proposed approach outperforms the state-of-the-art evaluated on the CoL-SLTD dataset, achieving a BLEU-4 of 72,64% in split 1, and a BLEU-4 of 14,64% in split 2. Additionally, the strategy was validated on the RWTH-PHOENIX-Weather 2014 T dataset, achieving a competitive BLEU-4 of 11,58%.",
        "authors": [
            "Fredy Alejandro Mendoza López",
            "Jefferson Rodriguez",
            "Fabio Martínez"
        ],
        "submitted_date": "2025-03-25T13:53:25+00:00",
        "pdf_url": "https://arxiv.org/pdf/2503.19668v1",
        "similarity_score": 0.8172202110290527
    },
    {
        "arxiv_id": "2504.11942v1",
        "title": "ADAT: Time-Series-Aware Adaptive Transformer Architecture for Sign Language Translation",
        "abstract": "Current sign language machine translation systems rely on recognizing hand movements, facial expressions and body postures, and natural language processing, to convert signs into text. Recent approaches use Transformer architectures to model long-range dependencies via positional encoding. However, they lack accuracy in recognizing fine-grained, short-range temporal dependencies between gestures captured at high frame rates. Moreover, their high computational complexity leads to inefficient training. To mitigate these issues, we propose an Adaptive Transformer (ADAT), which incorporates components for enhanced feature extraction and adaptive feature weighting through a gating mechanism to emphasize contextually relevant features while reducing training overhead and maintaining translation accuracy. To evaluate ADAT, we introduce MedASL, the first public medical American Sign Language dataset. In sign-to-gloss-to-text experiments, ADAT outperforms the encoder-decoder transformer, improving BLEU-4 accuracy by 0.1% while reducing training time by 14.33% on PHOENIX14T and 3.24% on MedASL. In sign-to-text experiments, it improves accuracy by 8.7% and reduces training time by 2.8% on PHOENIX14T and achieves 4.7% higher accuracy and 7.17% faster training on MedASL. Compared to encoder-only and decoder-only baselines in sign-to-text, ADAT is at least 6.8% more accurate despite being up to 12.1% slower due to its dual-stream structure.",
        "authors": [
            "Nada Shahin",
            "Leila Ismail"
        ],
        "submitted_date": "2025-04-16T10:20:11+00:00",
        "pdf_url": "https://arxiv.org/pdf/2504.11942v1",
        "similarity_score": 0.781426191329956
    },
    {
        "arxiv_id": "2407.02854v2",
        "title": "A Spatio-Temporal Representation Learning as an Alternative to Traditional Glosses in Sign Language Translation and Production",
        "abstract": "This work addresses the challenges associated with the use of glosses in both Sign Language Translation (SLT) and Sign Language Production (SLP). While glosses have long been used as a bridge between sign language and spoken language, they come with two major limitations that impede the advancement of sign language systems. First, annotating the glosses is a labor-intensive and time-consuming process, which limits the scalability of datasets. Second, the glosses oversimplify sign language by stripping away its spatio-temporal dynamics, reducing complex signs to basic labels and missing the subtle movements essential for precise interpretation. To address these limitations, we introduce Universal Gloss-level Representation (UniGloR), a framework designed to capture the spatio-temporal features inherent in sign language, providing a more dynamic and detailed alternative to the use of the glosses. The core idea of UniGloR is simple yet effective: We derive dense spatio-temporal representations from sign keypoint sequences using self-supervised learning and seamlessly integrate them into SLT and SLP tasks. Our experiments in a keypoint-based setting demonstrate that UniGloR either outperforms or matches the performance of previous SLT and SLP methods on two widely-used datasets: PHOENIX14T and How2Sign.",
        "authors": [
            "Eui Jun Hwang",
            "Sukmin Cho",
            "Huije Lee",
            "Youngwoo Yoon",
            "Jong C. Park"
        ],
        "submitted_date": "2024-07-03T07:12:36+00:00",
        "pdf_url": "https://arxiv.org/pdf/2407.02854v2",
        "similarity_score": 0.7735357880592346
    },
    {
        "arxiv_id": "2503.16855v2",
        "title": "Stack Transformer Based Spatial-Temporal Attention Model for Dynamic Sign Language and Fingerspelling Recognition",
        "abstract": "Hand gesture-based Sign Language Recognition (SLR) serves as a crucial communication bridge between deaf and non-deaf individuals. While Graph Convolutional Networks (GCNs) are common, they are limited by their reliance on fixed skeletal graphs. To overcome this, we propose the Sequential Spatio-Temporal Attention Network (SSTAN), a novel Transformer-based architecture. Our model employs a hierarchical, stacked design that sequentially integrates Spatial Multi-Head Attention (MHA) to capture intra-frame joint relationships and Temporal MHA to model long-range inter-frame dependencies. This approach allows the model to efficiently learn complex spatio-temporal patterns without predefined graph structures. We validated our model through extensive experiments on diverse, large-scale datasets (WLASL, JSL, and KSL). A key finding is that our model, trained entirely from scratch, achieves state-of-the-art (SOTA) performance in the challenging fingerspelling categories (JSL and KSL). Furthermore, it establishes a new SOTA for skeleton-only methods on WLASL, outperforming several approaches that rely on complex self-supervised pre-training. These results demonstrate our model's high data efficiency and its effectiveness in capturing the intricate dynamics of sign language. The official implementation is available at our GitHub repository: \\href{https://github.com/K-Hirooka-Aizu/skeleton-slr-transformer}{https://github.com/K-Hirooka-Aizu/skeleton-slr-transformer}.",
        "authors": [
            "Koki Hirooka",
            "Abu Saleh Musa Miah",
            "Tatsuya Murakami",
            "Md. Al Mehedi Hasan",
            "Yong Seok Hwang",
            "Jungpil Shin"
        ],
        "submitted_date": "2025-03-21T04:57:18+00:00",
        "pdf_url": "https://arxiv.org/pdf/2503.16855v2",
        "similarity_score": 0.748619556427002
    },
    {
        "arxiv_id": "2505.07890v4",
        "title": "TSLFormer: A Lightweight Transformer Model for Turkish Sign Language Recognition Using Skeletal Landmarks",
        "abstract": "This study presents TSLFormer, a light and robust word-level Turkish Sign Language (TSL) recognition model that treats sign gestures as ordered, string-like language. Instead of using raw RGB or depth videos, our method only works with 3D joint positions - articulation points - extracted using Google's Mediapipe library, which focuses on the hand and torso skeletal locations. This creates efficient input dimensionality reduction while preserving important semantic gesture information. Our approach revisits sign language recognition as sequence-to-sequence translation, inspired by the linguistic nature of sign languages and the success of transformers in natural language processing. Since TSLFormer uses the self-attention mechanism, it effectively captures temporal co-occurrence within gesture sequences and highlights meaningful motion patterns as words unfold. Evaluated on the AUTSL dataset with over 36,000 samples and 227 different words, TSLFormer achieves competitive performance with minimal computational cost. These results show that joint-based input is sufficient for enabling real-time, mobile, and assistive communication systems for hearing-impaired individuals.",
        "authors": [
            "Kutay Ertürk",
            "Furkan Altınışık",
            "İrem Sarıaltın",
            "Ömer Nezih Gerek"
        ],
        "submitted_date": "2025-05-11T14:30:56+00:00",
        "pdf_url": "https://arxiv.org/pdf/2505.07890v4",
        "similarity_score": 0.7464056015014648
    },
    {
        "arxiv_id": "2504.01666v1",
        "title": "CLIP-SLA: Parameter-Efficient CLIP Adaptation for Continuous Sign Language Recognition",
        "abstract": "Continuous sign language recognition (CSLR) focuses on interpreting and transcribing sequences of sign language gestures in videos. In this work, we propose CLIP sign language adaptation (CLIP-SLA), a novel CSLR framework that leverages the powerful pre-trained visual encoder from the CLIP model to sign language tasks through parameter-efficient fine-tuning (PEFT). We introduce two variants, SLA-Adapter and SLA-LoRA, which integrate PEFT modules into the CLIP visual encoder, enabling fine-tuning with minimal trainable parameters. The effectiveness of the proposed frameworks is validated on four datasets: Phoenix2014, Phoenix2014-T, CSL-Daily, and Isharah-500, where both CLIP-SLA variants outperformed several SOTA models with fewer trainable parameters. Extensive ablation studies emphasize the effectiveness and flexibility of the proposed methods with different vision-language models for CSLR. These findings showcase the potential of adapting large-scale pre-trained models for scalable and efficient CSLR, which pave the way for future advancements in sign language understanding.",
        "authors": [
            "Sarah Alyami",
            "Hamzah Luqman"
        ],
        "submitted_date": "2025-04-02T12:15:33+00:00",
        "pdf_url": "https://arxiv.org/pdf/2504.01666v1",
        "similarity_score": 0.7380937337875366
    },
    {
        "arxiv_id": "2509.10266v2",
        "title": "SignMouth: Leveraging Mouthing Cues for Sign Language Translation by Multimodal Contrastive Fusion",
        "abstract": "Sign language translation (SLT) aims to translate natural language from sign language videos, serving as a vital bridge for inclusive communication. While recent advances leverage powerful visual backbones and large language models, most approaches mainly focus on manual signals (hand gestures) and tend to overlook non-manual cues like mouthing. In fact, mouthing conveys essential linguistic information in sign languages and plays a crucial role in disambiguating visually similar signs. In this paper, we propose SignClip, a novel framework to improve the accuracy of sign language translation. It fuses manual and non-manual cues, specifically spatial gesture and lip movement features. Besides, SignClip introduces a hierarchical contrastive learning framework with multi-level alignment objectives, ensuring semantic consistency across sign-lip and visual-text modalities. Extensive experiments on two benchmark datasets, PHOENIX14T and How2Sign, demonstrate the superiority of our approach. For example, on PHOENIX14T, in the Gloss-free setting, SignClip surpasses the previous state-of-the-art model SpaMo, improving BLEU-4 from 24.32 to 24.71, and ROUGE from 46.57 to 48.38.",
        "authors": [
            "Wenfang Wu",
            "Tingting Yuan",
            "Yupeng Li",
            "Daling Wang",
            "Xiaoming Fu"
        ],
        "submitted_date": "2025-09-12T14:08:06+00:00",
        "pdf_url": "https://arxiv.org/pdf/2509.10266v2",
        "similarity_score": 0.7340655326843262
    },
    {
        "arxiv_id": "2504.07792v2",
        "title": "Breaking the Barriers: Video Vision Transformers for Word-Level Sign Language Recognition",
        "abstract": "Sign language is a fundamental means of communication for the deaf and hard-of-hearing (DHH) community, enabling nuanced expression through gestures, facial expressions, and body movements. Despite its critical role in facilitating interaction within the DHH population, significant barriers persist due to the limited fluency in sign language among the hearing population. Overcoming this communication gap through automatic sign language recognition (SLR) remains a challenge, particularly at a dynamic word-level, where temporal and spatial dependencies must be effectively recognized. While Convolutional Neural Networks (CNNs) have shown potential in SLR, they are computationally intensive and have difficulties in capturing global temporal dependencies between video sequences. To address these limitations, we propose a Video Vision Transformer (ViViT) model for word-level American Sign Language (ASL) recognition. Transformer models make use of self-attention mechanisms to effectively capture global relationships across spatial and temporal dimensions, which makes them suitable for complex gesture recognition tasks. The VideoMAE model achieves a Top-1 accuracy of 75.58% on the WLASL100 dataset, highlighting its strong performance compared to traditional CNNs with 65.89%. Our study demonstrates that transformer-based architectures have great potential to advance SLR, overcome communication barriers and promote the inclusion of DHH individuals.",
        "authors": [
            "Alexander Brettmann",
            "Jakob Grävinghoff",
            "Marlene Rüschoff",
            "Marie Westhues"
        ],
        "submitted_date": "2025-04-10T14:27:25+00:00",
        "pdf_url": "https://arxiv.org/pdf/2504.07792v2",
        "similarity_score": 0.7298995852470398
    },
    {
        "arxiv_id": "2503.06484v1",
        "title": "Sign Language Translation using Frame and Event Stream: Benchmark Dataset and Algorithms",
        "abstract": "Accurate sign language understanding serves as a crucial communication channel for individuals with disabilities. Current sign language translation algorithms predominantly rely on RGB frames, which may be limited by fixed frame rates, variable lighting conditions, and motion blur caused by rapid hand movements. Inspired by the recent successful application of event cameras in other fields, we propose to leverage event streams to assist RGB cameras in capturing gesture data, addressing the various challenges mentioned above. Specifically, we first collect a large-scale RGB-Event sign language translation dataset using the DVS346 camera, termed VECSL, which contains 15,676 RGB-Event samples, 15,191 glosses, and covers 2,568 Chinese characters. These samples were gathered across a diverse range of indoor and outdoor environments, capturing multiple viewing angles, varying light intensities, and different camera motions. Due to the absence of benchmark algorithms for comparison in this new task, we retrained and evaluated multiple state-of-the-art SLT algorithms, and believe that this benchmark can effectively support subsequent related research. Additionally, we propose a novel RGB-Event sign language translation framework (i.e., M$^2$-SLT) that incorporates fine-grained micro-sign and coarse-grained macro-sign retrieval, achieving state-of-the-art results on the proposed dataset. Both the source code and dataset will be released on https://github.com/Event-AHU/OpenESL.",
        "authors": [
            "Xiao Wang",
            "Yuehang Li",
            "Fuling Wang",
            "Bo Jiang",
            "Yaowei Wang",
            "Yonghong Tian",
            "Jin Tang",
            "Bin Luo"
        ],
        "submitted_date": "2025-03-09T06:55:46+00:00",
        "pdf_url": "https://arxiv.org/pdf/2503.06484v1",
        "similarity_score": 0.7286813855171204
    },
    {
        "arxiv_id": "2412.11553v2",
        "title": "Training Strategies for Isolated Sign Language Recognition",
        "abstract": "Accurate recognition and interpretation of sign language are crucial for enhancing communication accessibility for deaf and hard of hearing individuals. However, current approaches of Isolated Sign Language Recognition (ISLR) often face challenges such as low data quality and variability in gesturing speed. This paper introduces a comprehensive model training pipeline for ISLR designed to accommodate the distinctive characteristics and constraints of the Sign Language (SL) domain. The constructed pipeline incorporates carefully selected image and video augmentations to tackle the challenges of low data quality and varying sign speeds. Including an additional regression head combined with IoU-balanced classification loss enhances the model's awareness of the gesture and simplifies capturing temporal information. Extensive experiments demonstrate that the developed training pipeline easily adapts to different datasets and architectures. Additionally, the ablation study shows that each proposed component expands the potential to consider ISLR task specifics. The presented strategies enhance recognition performance across various ISLR benchmarks and achieve state-of-the-art results on the WLASL and Slovo datasets.",
        "authors": [
            "Karina Kvanchiani",
            "Roman Kraynov",
            "Elizaveta Petrova",
            "Petr Surovcev",
            "Aleksandr Nagaev",
            "Alexander Kapitanov"
        ],
        "submitted_date": "2024-12-16T08:37:58+00:00",
        "pdf_url": "https://arxiv.org/pdf/2412.11553v2",
        "similarity_score": 0.7245774865150452
    },
    {
        "arxiv_id": "2412.01991v1",
        "title": "Real-Time Multilingual Sign Language Processing",
        "abstract": "Sign Language Processing (SLP) is an interdisciplinary field comprised of Natural Language Processing (NLP) and Computer Vision. It is focused on the computational understanding, translation, and production of signed languages. Traditional approaches have often been constrained by the use of gloss-based systems that are both language-specific and inadequate for capturing the multidimensional nature of sign language. These limitations have hindered the development of technology capable of processing signed languages effectively.   This thesis aims to revolutionize the field of SLP by proposing a simple paradigm that can bridge this existing technological gap. We propose the use of SignWiring, a universal sign language transcription notation system, to serve as an intermediary link between the visual-gestural modality of signed languages and text-based linguistic representations.   We contribute foundational libraries and resources to the SLP community, thereby setting the stage for a more in-depth exploration of the tasks of sign language translation and production. These tasks encompass the translation of sign language from video to spoken language text and vice versa. Through empirical evaluations, we establish the efficacy of our transcription method as a pivot for enabling faster, more targeted research, that can lead to more natural and accurate translations across a range of languages.   The universal nature of our transcription-based paradigm also paves the way for real-time, multilingual applications in SLP, thereby offering a more inclusive and accessible approach to language technology. This is a significant step toward universal accessibility, enabling a wider reach of AI-driven language technologies to include the deaf and hard-of-hearing community.",
        "authors": [
            "Amit Moryossef"
        ],
        "submitted_date": "2024-12-02T21:51:41+00:00",
        "pdf_url": "https://arxiv.org/pdf/2412.01991v1",
        "similarity_score": 0.7223615646362305
    },
    {
        "arxiv_id": "2405.14312v2",
        "title": "Improving Gloss-free Sign Language Translation by Reducing Representation Density",
        "abstract": "Gloss-free sign language translation (SLT) aims to develop well-performing SLT systems with no requirement for the costly gloss annotations, but currently still lags behind gloss-based approaches significantly. In this paper, we identify a representation density problem that could be a bottleneck in restricting the performance of gloss-free SLT. Specifically, the representation density problem describes that the visual representations of semantically distinct sign gestures tend to be closely packed together in feature space, which makes gloss-free methods struggle with distinguishing different sign gestures and suffer from a sharp performance drop. To address the representation density problem, we introduce a simple but effective contrastive learning strategy, namely SignCL, which encourages gloss-free models to learn more discriminative feature representation in a self-supervised manner. Our experiments demonstrate that the proposed SignCL can significantly reduce the representation density and improve performance across various translation frameworks. Specifically, SignCL achieves a significant improvement in BLEU score for the Sign Language Transformer and GFSLT-VLP on the CSL-Daily dataset by 39% and 46%, respectively, without any increase of model parameters. Compared to Sign2GPT, a state-of-the-art method based on large-scale pre-trained vision and language models, SignCL achieves better performance with only 35% of its parameters. Implementation and Checkpoints are available at https://github.com/JinhuiYE/SignCL.",
        "authors": [
            "Jinhui Ye",
            "Xing Wang",
            "Wenxiang Jiao",
            "Junwei Liang",
            "Hui Xiong"
        ],
        "submitted_date": "2024-05-23T08:32:58+00:00",
        "pdf_url": "https://arxiv.org/pdf/2405.14312v2",
        "similarity_score": 0.7175238132476807
    },
    {
        "arxiv_id": "2004.00588v2",
        "title": "Better Sign Language Translation with STMC-Transformer",
        "abstract": "Sign Language Translation (SLT) first uses a Sign Language Recognition (SLR) system to extract sign language glosses from videos. Then, a translation system generates spoken language translations from the sign language glosses. This paper focuses on the translation system and introduces the STMC-Transformer which improves on the current state-of-the-art by over 5 and 7 BLEU respectively on gloss-to-text and video-to-text translation of the PHOENIX-Weather 2014T dataset. On the ASLG-PC12 corpus, we report an increase of over 16 BLEU.   We also demonstrate the problem in current methods that rely on gloss supervision. The video-to-text translation of our STMC-Transformer outperforms translation of GT glosses. This contradicts previous claims that GT gloss translation acts as an upper bound for SLT performance and reveals that glosses are an inefficient representation of sign language. For future SLT research, we therefore suggest an end-to-end training of the recognition and translation models, or using a different sign language annotation scheme.",
        "authors": [
            "Kayo Yin",
            "Jesse Read"
        ],
        "submitted_date": "2020-04-01T17:20:04+00:00",
        "pdf_url": "https://arxiv.org/pdf/2004.00588v2",
        "similarity_score": 0.7151710987091064
    },
    {
        "arxiv_id": "2411.16789v2",
        "title": "Leveraging the Power of MLLMs for Gloss-Free Sign Language Translation",
        "abstract": "Sign language translation (SLT) is a challenging task that involves translating sign language images into spoken language. For SLT models to perform this task successfully, they must bridge the modality gap and identify subtle variations in sign language components to understand their meanings accurately. To address these challenges, we propose a novel gloss-free SLT framework called Multimodal Sign Language Translation (MMSLT), which leverages the representational capabilities of off-the-shelf multimodal large language models (MLLMs). Specifically, we use MLLMs to generate detailed textual descriptions of sign language components. Then, through our proposed multimodal-language pre-training module, we integrate these description features with sign video features to align them within the spoken sentence space. Our approach achieves state-of-the-art performance on benchmark datasets PHOENIX14T and CSL-Daily, highlighting the potential of MLLMs to be utilized effectively in SLT. Code is available at https://github.com/hwjeon98/MMSLT.",
        "authors": [
            "Jungeun Kim",
            "Hyeongwoo Jeon",
            "Jongseong Bae",
            "Ha Young Kim"
        ],
        "submitted_date": "2024-11-25T09:01:41+00:00",
        "pdf_url": "https://arxiv.org/pdf/2411.16789v2",
        "similarity_score": 0.7146763205528259
    },
    {
        "arxiv_id": "2307.05440v1",
        "title": "ISLTranslate: Dataset for Translating Indian Sign Language",
        "abstract": "Sign languages are the primary means of communication for many hard-of-hearing people worldwide. Recently, to bridge the communication gap between the hard-of-hearing community and the rest of the population, several sign language translation datasets have been proposed to enable the development of statistical sign language translation systems. However, there is a dearth of sign language resources for the Indian sign language. This resource paper introduces ISLTranslate, a translation dataset for continuous Indian Sign Language (ISL) consisting of 31k ISL-English sentence/phrase pairs. To the best of our knowledge, it is the largest translation dataset for continuous Indian Sign Language. We provide a detailed analysis of the dataset. To validate the performance of existing end-to-end Sign language to spoken language translation systems, we benchmark the created dataset with a transformer-based model for ISL translation.",
        "authors": [
            "Abhinav Joshi",
            "Susmit Agrawal",
            "Ashutosh Modi"
        ],
        "submitted_date": "2023-07-11T17:06:52+00:00",
        "pdf_url": "https://arxiv.org/pdf/2307.05440v1",
        "similarity_score": 0.7138948440551758
    },
    {
        "arxiv_id": "2507.19840v1",
        "title": "AutoSign: Direct Pose-to-Text Translation for Continuous Sign Language Recognition",
        "abstract": "Continuously recognizing sign gestures and converting them to glosses plays a key role in bridging the gap between the hearing and hearing-impaired communities. This involves recognizing and interpreting the hands, face, and body gestures of the signer, which pose a challenge as it involves a combination of all these features. Continuous Sign Language Recognition (CSLR) methods rely on multi-stage pipelines that first extract visual features, then align variable-length sequences with target glosses using CTC or HMM-based approaches. However, these alignment-based methods suffer from error propagation across stages, overfitting, and struggle with vocabulary scalability due to the intermediate gloss representation bottleneck. To address these limitations, we propose AutoSign, an autoregressive decoder-only transformer that directly translates pose sequences to natural language text, bypassing traditional alignment mechanisms entirely. The use of this decoder-only approach allows the model to directly map between the features and the glosses without the need for CTC loss while also directly learning the textual dependencies in the glosses. Our approach incorporates a temporal compression module using 1D CNNs to efficiently process pose sequences, followed by AraGPT2, a pre-trained Arabic decoder, to generate text (glosses). Through comprehensive ablation studies, we demonstrate that hand and body gestures provide the most discriminative features for signer-independent CSLR. By eliminating the multi-stage pipeline, AutoSign achieves substantial improvements on the Isharah-1000 dataset, achieving an improvement of up to 6.1\\% in WER score compared to the best existing method.",
        "authors": [
            "Samuel Ebimobowei Johnny",
            "Blessed Guda",
            "Andrew Blayama Stephen",
            "Assane Gueye"
        ],
        "submitted_date": "2025-07-26T07:28:33+00:00",
        "pdf_url": "https://arxiv.org/pdf/2507.19840v1",
        "similarity_score": 0.7137995958328247
    },
    {
        "arxiv_id": "2506.11621v1",
        "title": "SignAligner: Harmonizing Complementary Pose Modalities for Coherent Sign Language Generation",
        "abstract": "Sign language generation aims to produce diverse sign representations based on spoken language. However, achieving realistic and naturalistic generation remains a significant challenge due to the complexity of sign language, which encompasses intricate hand gestures, facial expressions, and body movements. In this work, we introduce PHOENIX14T+, an extended version of the widely-used RWTH-PHOENIX-Weather 2014T dataset, featuring three new sign representations: Pose, Hamer and Smplerx. We also propose a novel method, SignAligner, for realistic sign language generation, consisting of three stages: text-driven pose modalities co-generation, online collaborative correction of multimodality, and realistic sign video synthesis. First, by incorporating text semantics, we design a joint sign language generator to simultaneously produce posture coordinates, gesture actions, and body movements. The text encoder, based on a Transformer architecture, extracts semantic features, while a cross-modal attention mechanism integrates these features to generate diverse sign language representations, ensuring accurate mapping and controlling the diversity of modal features. Next, online collaborative correction is introduced to refine the generated pose modalities using a dynamic loss weighting strategy and cross-modal attention, facilitating the complementarity of information across modalities, eliminating spatiotemporal conflicts, and ensuring semantic coherence and action consistency. Finally, the corrected pose modalities are fed into a pre-trained video generation network to produce high-fidelity sign language videos. Extensive experiments demonstrate that SignAligner significantly improves both the accuracy and expressiveness of the generated sign videos.",
        "authors": [
            "Xu Wang",
            "Shengeng Tang",
            "Lechao Cheng",
            "Feng Li",
            "Shuo Wang",
            "Richang Hong"
        ],
        "submitted_date": "2025-06-13T09:44:42+00:00",
        "pdf_url": "https://arxiv.org/pdf/2506.11621v1",
        "similarity_score": 0.711042582988739
    },
    {
        "arxiv_id": "2503.20436v1",
        "title": "Siformer: Feature-isolated Transformer for Efficient Skeleton-based Sign Language Recognition",
        "abstract": "Sign language recognition (SLR) refers to interpreting sign language glosses from given videos automatically. This research area presents a complex challenge in computer vision because of the rapid and intricate movements inherent in sign languages, which encompass hand gestures, body postures, and even facial expressions. Recently, skeleton-based action recognition has attracted increasing attention due to its ability to handle variations in subjects and backgrounds independently. However, current skeleton-based SLR methods exhibit three limitations: 1) they often neglect the importance of realistic hand poses, where most studies train SLR models on non-realistic skeletal representations; 2) they tend to assume complete data availability in both training or inference phases, and capture intricate relationships among different body parts collectively; 3) these methods treat all sign glosses uniformly, failing to account for differences in complexity levels regarding skeletal representations. To enhance the realism of hand skeletal representations, we present a kinematic hand pose rectification method for enforcing constraints. Mitigating the impact of missing data, we propose a feature-isolated mechanism to focus on capturing local spatial-temporal context. This method captures the context concurrently and independently from individual features, thus enhancing the robustness of the SLR model. Additionally, to adapt to varying complexity levels of sign glosses, we develop an input-adaptive inference approach to optimise computational efficiency and accuracy. Experimental results demonstrate the effectiveness of our approach, as evidenced by achieving a new state-of-the-art (SOTA) performance on WLASL100 and LSA64. For WLASL100, we achieve a top-1 accuracy of 86.50\\%, marking a relative improvement of 2.39% over the previous SOTA. For LSA64, we achieve a top-1 accuracy of 99.84%.",
        "authors": [
            "Muxin Pu",
            "Mei Kuan Lim",
            "Chun Yong Chong"
        ],
        "submitted_date": "2025-03-26T11:10:29+00:00",
        "pdf_url": "https://arxiv.org/pdf/2503.20436v1",
        "similarity_score": 0.709775447845459
    },
    {
        "arxiv_id": "2302.07693v2",
        "title": "Fine-tuning of sign language recognition models: a technical report",
        "abstract": "Sign Language Recognition (SLR) is an essential yet challenging task since sign language is performed with the fast and complex movement of hand gestures, body posture, and even facial expressions. %Skeleton Aware Multi-modal Sign Language Recognition In this work, we focused on investigating two questions: how fine-tuning on datasets from other sign languages helps improve sign recognition quality, and whether sign recognition is possible in real-time without using GPU. Three different languages datasets (American sign language WLASL, Turkish - AUTSL, Russian - RSL) have been used to validate the models. The average speed of this system has reached 3 predictions per second, which meets the requirements for the real-time scenario. This model (prototype) will benefit speech or hearing impaired people talk with other trough internet. We also investigated how the additional training of the model in another sign language affects the quality of recognition. The results show that further training of the model on the data of another sign language almost always leads to an improvement in the quality of gesture recognition. We also provide code for reproducing model training experiments, converting models to ONNX format, and inference for real-time gesture recognition.",
        "authors": [
            "Maxim Novopoltsev",
            "Leonid Verkhovtsev",
            "Ruslan Murtazin",
            "Dmitriy Milevich",
            "Iuliia Zemtsova"
        ],
        "submitted_date": "2023-02-15T14:36:18+00:00",
        "pdf_url": "https://arxiv.org/pdf/2302.07693v2",
        "similarity_score": 0.7077116370201111
    },
    {
        "arxiv_id": "2403.10434v3",
        "title": "Spotter+GPT: Turning Sign Spottings into Sentences with LLMs",
        "abstract": "Sign Language Translation (SLT) is a challenging task that aims to generate spoken language sentences from sign language videos. In this paper, we introduce a lightweight, modular SLT framework, Spotter+GPT, that leverages the power of Large Language Models (LLMs) and avoids heavy end-to-end training. Spotter+GPT breaks down the SLT task into two distinct stages. First, a sign spotter identifies individual signs within the input video. The spotted signs are then passed to an LLM, which transforms them into meaningful spoken language sentences. Spotter+GPT eliminates the requirement for SLT-specific training. This significantly reduces computational costs and time requirements. The source code and pretrained weights of the Spotter are available at https://gitlab.surrey.ac.uk/cogvispublic/sign-spotter.",
        "authors": [
            "Ozge Mercanoglu Sincan",
            "Richard Bowden"
        ],
        "submitted_date": "2024-03-15T16:14:34+00:00",
        "pdf_url": "https://arxiv.org/pdf/2403.10434v3",
        "similarity_score": 0.7025765180587769
    },
    {
        "arxiv_id": "2201.01609v2",
        "title": "All You Need In Sign Language Production",
        "abstract": "Sign Language is the dominant form of communication language used in the deaf and hearing-impaired community. To make an easy and mutual communication between the hearing-impaired and the hearing communities, building a robust system capable of translating the spoken language into sign language and vice versa is fundamental. To this end, sign language recognition and production are two necessary parts for making such a two-way system. Sign language recognition and production need to cope with some critical challenges. In this survey, we review recent advances in Sign Language Production (SLP) and related areas using deep learning. To have more realistic perspectives to sign language, we present an introduction to the Deaf culture, Deaf centers, psychological perspective of sign language, the main differences between spoken language and sign language. Furthermore, we present the fundamental components of a bi-directional sign language translation system, discussing the main challenges in this area. Also, the backbone architectures and methods in SLP are briefly introduced and the proposed taxonomy on SLP is presented. Finally, a general framework for SLP and performance evaluation, and also a discussion on the recent developments, advantages, and limitations in SLP, commenting on possible lines for future research are presented.",
        "authors": [
            "Razieh Rastgoo",
            "Kourosh Kiani",
            "Sergio Escalera",
            "Vassilis Athitsos",
            "Mohammad Sabokrou"
        ],
        "submitted_date": "2022-01-05T13:45:09+00:00",
        "pdf_url": "https://arxiv.org/pdf/2201.01609v2",
        "similarity_score": 0.7016377449035645
    },
    {
        "arxiv_id": "2107.12600v1",
        "title": "PiSLTRc: Position-informed Sign Language Transformer with Content-aware Convolution",
        "abstract": "Since the superiority of Transformer in learning long-term dependency, the sign language Transformer model achieves remarkable progress in Sign Language Recognition (SLR) and Translation (SLT). However, there are several issues with the Transformer that prevent it from better sign language understanding. The first issue is that the self-attention mechanism learns sign video representation in a frame-wise manner, neglecting the temporal semantic structure of sign gestures. Secondly, the attention mechanism with absolute position encoding is direction and distance unaware, thus limiting its ability. To address these issues, we propose a new model architecture, namely PiSLTRc, with two distinctive characteristics: (i) content-aware and position-aware convolution layers. Specifically, we explicitly select relevant features using a novel content-aware neighborhood gathering method. Then we aggregate these features with position-informed temporal convolution layers, thus generating robust neighborhood-enhanced sign representation. (ii) injecting the relative position information to the attention mechanism in the encoder, decoder, and even encoder-decoder cross attention. Compared with the vanilla Transformer model, our model performs consistently better on three large-scale sign language benchmarks: PHOENIX-2014, PHOENIX-2014-T and CSL. Furthermore, extensive experiments demonstrate that the proposed method achieves state-of-the-art performance on translation quality with $+1.6$ BLEU improvements.",
        "authors": [
            "Pan Xie",
            "Mengyi Zhao",
            "Xiaohui Hu"
        ],
        "submitted_date": "2021-07-27T05:01:27+00:00",
        "pdf_url": "https://arxiv.org/pdf/2107.12600v1",
        "similarity_score": 0.700109601020813
    },
    {
        "arxiv_id": "2407.01264v2",
        "title": "SignCLIP: Connecting Text and Sign Language by Contrastive Learning",
        "abstract": "We present SignCLIP, which re-purposes CLIP (Contrastive Language-Image Pretraining) to project spoken language text and sign language videos, two classes of natural languages of distinct modalities, into the same space. SignCLIP is an efficient method of learning useful visual representations for sign language processing from large-scale, multilingual video-text pairs, without directly optimizing for a specific task or sign language which is often of limited size.   We pretrain SignCLIP on Spreadthesign, a prominent sign language dictionary consisting of ~500 thousand video clips in up to 44 sign languages, and evaluate it with various downstream datasets. SignCLIP discerns in-domain signing with notable text-to-video/video-to-text retrieval accuracy. It also performs competitively for out-of-domain downstream tasks such as isolated sign language recognition upon essential few-shot prompting or fine-tuning.   We analyze the latent space formed by the spoken language text and sign language poses, which provides additional linguistic insights. Our code and models are openly available.",
        "authors": [
            "Zifan Jiang",
            "Gerard Sant",
            "Amit Moryossef",
            "Mathias Müller",
            "Rico Sennrich",
            "Sarah Ebling"
        ],
        "submitted_date": "2024-07-01T13:17:35+00:00",
        "pdf_url": "https://arxiv.org/pdf/2407.01264v2",
        "similarity_score": 0.6977580785751343
    },
    {
        "arxiv_id": "2501.00765v2",
        "title": "Beyond Words: AuralLLM and SignMST-C for Sign Language Production and Bidirectional Accessibility",
        "abstract": "Sign language is the primary communication mode for 72 million hearing-impaired individuals worldwide, necessitating effective bidirectional Sign Language Production and Sign Language Translation systems. However, functional bidirectional systems require a unified linguistic environment, hindered by the lack of suitable unified datasets, particularly those providing the necessary pose information for accurate Sign Language Production (SLP) evaluation. Concurrently, current SLP evaluation methods like back-translation ignore pose accuracy, and high-quality coordinated generation remains challenging. To create this crucial environment and overcome these challenges, we introduce CNText2Sign and CNSign, which together constitute the first unified dataset aimed at supporting bidirectional accessibility systems for Chinese sign language; CNText2Sign provides 15,000 natural language-to-sign mappings and standardized skeletal keypoints for 8,643 vocabulary items supporting pose assessment. Building upon this foundation, we propose the AuraLLM model, which leverages a decoupled architecture with CNText2Sign's pose data for novel direct gesture accuracy assessment. The model employs retrieval augmentation and Cascading Vocabulary Resolution to handle semantic mapping and out-of-vocabulary words and achieves all-scenario production with controllable coordination of gestures and facial expressions via pose-conditioned video synthesis. Concurrently, our Sign Language Translation model SignMST-C employs targeted self-supervised pretraining for dynamic feature capture, achieving new SOTA results on PHOENIX2014-T with BLEU-4 scores up to 32.08. AuraLLM establishes a strong performance baseline on CNText2Sign with a BLEU-4 score of 50.41 under direct evaluation.",
        "authors": [
            "Yulong Li",
            "Yuxuan Zhang",
            "Feilong Tang",
            "Ming Hu",
            "Zhixiang Lu",
            "Haochen Xue",
            "Jianghao Wu",
            "Mian Zhou",
            "Kang Dang",
            "Chong Li",
            "Yifang Wang",
            "Imran Razzak",
            "Jionglong Su"
        ],
        "submitted_date": "2025-01-01T07:55:15+00:00",
        "pdf_url": "https://arxiv.org/pdf/2501.00765v2",
        "similarity_score": 0.6963809728622437
    },
    {
        "arxiv_id": "2408.08544v1",
        "title": "Scaling up Multimodal Pre-training for Sign Language Understanding",
        "abstract": "Sign language serves as the primary meaning of communication for the deaf-mute community. Different from spoken language, it commonly conveys information by the collaboration of manual features, i.e., hand gestures and body movements, and non-manual features, i.e., facial expressions and mouth cues. To facilitate communication between the deaf-mute and hearing people, a series of sign language understanding (SLU) tasks have been studied in recent years, including isolated/continuous sign language recognition (ISLR/CSLR), gloss-free sign language translation (GF-SLT) and sign language retrieval (SL-RT). Sign language recognition and translation aims to understand the semantic meaning conveyed by sign languages from gloss-level and sentence-level, respectively. In contrast, SL-RT focuses on retrieving sign videos or corresponding texts from a closed-set under the query-by-example search paradigm. These tasks investigate sign language topics from diverse perspectives and raise challenges in learning effective representation of sign language videos. To advance the development of sign language understanding, exploring a generalized model that is applicable across various SLU tasks is a profound research direction.",
        "authors": [
            "Wengang Zhou",
            "Weichao Zhao",
            "Hezhen Hu",
            "Zecheng Li",
            "Houqiang Li"
        ],
        "submitted_date": "2024-08-16T06:04:25+00:00",
        "pdf_url": "https://arxiv.org/pdf/2408.08544v1",
        "similarity_score": 0.6939425468444824
    },
    {
        "arxiv_id": "2507.20414v2",
        "title": "Indian Sign Language Detection for Real-Time Translation using Machine Learning",
        "abstract": "Gestural language is used by deaf & mute communities to communicate through hand gestures & body movements that rely on visual-spatial patterns known as sign languages. Sign languages, which rely on visual-spatial patterns of hand gestures & body movements, are the primary mode of communication for deaf & mute communities worldwide. Effective communication is fundamental to human interaction, yet individuals in these communities often face significant barriers due to a scarcity of skilled interpreters & accessible translation technologies. This research specifically addresses these challenges within the Indian context by focusing on Indian Sign Language (ISL). By leveraging machine learning, this study aims to bridge the critical communication gap for the deaf & hard-of-hearing population in India, where technological solutions for ISL are less developed compared to other global sign languages. We propose a robust, real-time ISL detection & translation system built upon a Convolutional Neural Network (CNN). Our model is trained on a comprehensive ISL dataset & demonstrates exceptional performance, achieving a classification accuracy of 99.95%. This high precision underscores the model's capability to discern the nuanced visual features of different signs. The system's effectiveness is rigorously evaluated using key performance metrics, including accuracy, F1 score, precision & recall, ensuring its reliability for real-world applications. For real-time implementation, the framework integrates MediaPipe for precise hand tracking & motion detection, enabling seamless translation of dynamic gestures. This paper provides a detailed account of the model's architecture, the data preprocessing pipeline & the classification methodology. The research elaborates the model architecture, preprocessing & classification methodologies for enhancing communication in deaf & mute communities.",
        "authors": [
            "Rajat Singhal",
            "Jatin Gupta",
            "Akhil Sharma",
            "Anushka Gupta",
            "Navya Sharma"
        ],
        "submitted_date": "2025-07-27T21:15:46+00:00",
        "pdf_url": "https://arxiv.org/pdf/2507.20414v2",
        "similarity_score": 0.6939199566841125
    },
    {
        "arxiv_id": "2208.06183v1",
        "title": "Non-Autoregressive Sign Language Production via Knowledge Distillation",
        "abstract": "Sign Language Production (SLP) aims to translate expressions in spoken language into corresponding ones in sign language, such as skeleton-based sign poses or videos. Existing SLP models are either AutoRegressive (AR) or Non-Autoregressive (NAR). However, AR-SLP models suffer from regression to the mean and error propagation during decoding. NSLP-G, a NAR-based model, resolves these issues to some extent but engenders other problems. For example, it does not consider target sign lengths and suffers from false decoding initiation. We propose a novel NAR-SLP model via Knowledge Distillation (KD) to address these problems. First, we devise a length regulator to predict the end of the generated sign pose sequence. We then adopt KD, which distills spatial-linguistic features from a pre-trained pose encoder to alleviate false decoding initiation. Extensive experiments show that the proposed approach significantly outperforms existing SLP models in both Frechet Gesture Distance and Back-Translation evaluation.",
        "authors": [
            "Eui Jun Hwang",
            "Jung Ho Kim",
            "Suk Min Cho",
            "Jong C. Park"
        ],
        "submitted_date": "2022-08-12T09:17:11+00:00",
        "pdf_url": "https://arxiv.org/pdf/2208.06183v1",
        "similarity_score": 0.6893380284309387
    },
    {
        "arxiv_id": "2511.08535v1",
        "title": "Large Sign Language Models: Toward 3D American Sign Language Translation",
        "abstract": "We present Large Sign Language Models (LSLM), a novel framework for translating 3D American Sign Language (ASL) by leveraging Large Language Models (LLMs) as the backbone, which can benefit hearing-impaired individuals' virtual communication. Unlike existing sign language recognition methods that rely on 2D video, our approach directly utilizes 3D sign language data to capture rich spatial, gestural, and depth information in 3D scenes. This enables more accurate and resilient translation, enhancing digital communication accessibility for the hearing-impaired community. Beyond the task of ASL translation, our work explores the integration of complex, embodied multimodal languages into the processing capabilities of LLMs, moving beyond purely text-based inputs to broaden their understanding of human communication. We investigate both direct translation from 3D gesture features to text and an instruction-guided setting where translations can be modulated by external prompts, offering greater flexibility. This work provides a foundational step toward inclusive, multimodal intelligent systems capable of understanding diverse forms of language.",
        "authors": [
            "Sen Zhang",
            "Xiaoxiao He",
            "Di Liu",
            "Zhaoyang Xia",
            "Mingyu Zhao",
            "Chaowei Tan",
            "Vivian Li",
            "Bo Liu",
            "Dimitris N. Metaxas",
            "Mubbasir Kapadia"
        ],
        "submitted_date": "2025-11-11T18:16:43+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.08535v1",
        "similarity_score": 0.6888474225997925
    },
    {
        "arxiv_id": "2405.04164v1",
        "title": "Sign2GPT: Leveraging Large Language Models for Gloss-Free Sign Language Translation",
        "abstract": "Automatic Sign Language Translation requires the integration of both computer vision and natural language processing to effectively bridge the communication gap between sign and spoken languages. However, the deficiency in large-scale training data to support sign language translation means we need to leverage resources from spoken language. We introduce, Sign2GPT, a novel framework for sign language translation that utilizes large-scale pretrained vision and language models via lightweight adapters for gloss-free sign language translation. The lightweight adapters are crucial for sign language translation, due to the constraints imposed by limited dataset sizes and the computational requirements when training with long sign videos. We also propose a novel pretraining strategy that directs our encoder to learn sign representations from automatically extracted pseudo-glosses without requiring gloss order information or annotations. We evaluate our approach on two public benchmark sign language translation datasets, namely RWTH-PHOENIX-Weather 2014T and CSL-Daily, and improve on state-of-the-art gloss-free translation performance with a significant margin.",
        "authors": [
            "Ryan Wong",
            "Necati Cihan Camgoz",
            "Richard Bowden"
        ],
        "submitted_date": "2024-05-07T10:00:38+00:00",
        "pdf_url": "https://arxiv.org/pdf/2405.04164v1",
        "similarity_score": 0.6876469850540161
    },
    {
        "arxiv_id": "2506.04367v1",
        "title": "Fine-Tuning Video Transformers for Word-Level Bangla Sign Language: A Comparative Analysis for Classification Tasks",
        "abstract": "Sign Language Recognition (SLR) involves the automatic identification and classification of sign gestures from images or video, converting them into text or speech to improve accessibility for the hearing-impaired community. In Bangladesh, Bangla Sign Language (BdSL) serves as the primary mode of communication for many individuals with hearing impairments. This study fine-tunes state-of-the-art video transformer architectures -- VideoMAE, ViViT, and TimeSformer -- on BdSLW60 (arXiv:2402.08635), a small-scale BdSL dataset with 60 frequent signs. We standardized the videos to 30 FPS, resulting in 9,307 user trial clips. To evaluate scalability and robustness, the models were also fine-tuned on BdSLW401 (arXiv:2503.02360), a large-scale dataset with 401 sign classes. Additionally, we benchmark performance against public datasets, including LSA64 and WLASL. Data augmentation techniques such as random cropping, horizontal flipping, and short-side scaling were applied to improve model robustness. To ensure balanced evaluation across folds during model selection, we employed 10-fold stratified cross-validation on the training set, while signer-independent evaluation was carried out using held-out test data from unseen users U4 and U8. Results show that video transformer models significantly outperform traditional machine learning and deep learning approaches. Performance is influenced by factors such as dataset size, video quality, frame distribution, frame rate, and model architecture. Among the models, the VideoMAE variant (MCG-NJU/videomae-base-finetuned-kinetics) achieved the highest accuracies of 95.5% on the frame rate corrected BdSLW60 dataset and 81.04% on the front-facing signs of BdSLW401 -- demonstrating strong potential for scalable and accurate BdSL recognition.",
        "authors": [
            "Jubayer Ahmed Bhuiyan Shawon",
            "Hasan Mahmud",
            "Kamrul Hasan"
        ],
        "submitted_date": "2025-06-04T18:29:32+00:00",
        "pdf_url": "https://arxiv.org/pdf/2506.04367v1",
        "similarity_score": 0.6873548626899719
    },
    {
        "arxiv_id": "1911.00071v1",
        "title": "SignCol: Open-Source Software for Collecting Sign Language Gestures",
        "abstract": "Sign(ed) languages use gestures, such as hand or head movements, for communication. Sign language recognition is an assistive technology for individuals with hearing disability and its goal is to improve such individuals' life quality by facilitating their social involvement. Since sign languages are vastly varied in alphabets, as known as signs, a sign recognition software should be capable of handling eight different types of sign combinations, e.g. numbers, letters, words and sentences. Due to the intrinsic complexity and diversity of symbolic gestures, recognition algorithms need a comprehensive visual dataset to learn by. In this paper, we describe the design and implementation of a Microsoft Kinect-based open source software, called SignCol, for capturing and saving the gestures used in sign languages. Our work supports a multi-language database and reports the recorded items statistics. SignCol can capture and store colored(RGB) frames, depth frames, infrared frames, body index frames, coordinate mapped color-body frames, skeleton information of each frame and camera parameters simultaneously.",
        "authors": [
            "Mohammad Eslami",
            "Mahdi Karami",
            "Sedigheh Eslami",
            "Solale Tabarestani",
            "Farah Torkamani-Azar",
            "Christoph Meinel"
        ],
        "submitted_date": "2019-10-31T19:36:18+00:00",
        "pdf_url": "https://arxiv.org/pdf/1911.00071v1",
        "similarity_score": 0.68567955493927
    },
    {
        "arxiv_id": "2408.14825v1",
        "title": "From Rule-Based Models to Deep Learning Transformers Architectures for Natural Language Processing and Sign Language Translation Systems: Survey, Taxonomy and Performance Evaluation",
        "abstract": "With the growing Deaf and Hard of Hearing population worldwide and the persistent shortage of certified sign language interpreters, there is a pressing need for an efficient, signs-driven, integrated end-to-end translation system, from sign to gloss to text and vice-versa. There has been a wealth of research on machine translations and related reviews. However, there are few works on sign language machine translation considering the particularity of the language being continuous and dynamic. This paper aims to address this void, providing a retrospective analysis of the temporal evolution of sign language machine translation algorithms and a taxonomy of the Transformers architectures, the most used approach in language translation. We also present the requirements of a real-time Quality-of-Service sign language ma-chine translation system underpinned by accurate deep learning algorithms. We propose future research directions for sign language translation systems.",
        "authors": [
            "Nada Shahin",
            "Leila Ismail"
        ],
        "submitted_date": "2024-08-27T07:11:45+00:00",
        "pdf_url": "https://arxiv.org/pdf/2408.14825v1",
        "similarity_score": 0.6849029064178467
    },
    {
        "arxiv_id": "2105.07625v2",
        "title": "A Fine-Grained Visual Attention Approach for Fingerspelling Recognition in the Wild",
        "abstract": "Fingerspelling in sign language has been the means of communicating technical terms and proper nouns when they do not have dedicated sign language gestures. Automatic recognition of fingerspelling can help resolve communication barriers when interacting with deaf people. The main challenges prevalent in fingerspelling recognition are the ambiguity in the gestures and strong articulation of the hands. The automatic recognition model should address high inter-class visual similarity and high intra-class variation in the gestures. Most of the existing research in fingerspelling recognition has focused on the dataset collected in a controlled environment. The recent collection of a large-scale annotated fingerspelling dataset in the wild, from social media and online platforms, captures the challenges in a real-world scenario. In this work, we propose a fine-grained visual attention mechanism using the Transformer model for the sequence-to-sequence prediction task in the wild dataset. The fine-grained attention is achieved by utilizing the change in motion of the video frames (optical flow) in sequential context-based attention along with a Transformer encoder model. The unsegmented continuous video dataset is jointly trained by balancing the Connectionist Temporal Classification (CTC) loss and the maximum-entropy loss. The proposed approach can capture better fine-grained attention in a single iteration. Experiment evaluations show that it outperforms the state-of-the-art approaches.",
        "authors": [
            "Kamala Gajurel",
            "Cuncong Zhong",
            "Guanghui Wang"
        ],
        "submitted_date": "2021-05-17T06:15:35+00:00",
        "pdf_url": "https://arxiv.org/pdf/2105.07625v2",
        "similarity_score": 0.6845158934593201
    },
    {
        "arxiv_id": "1608.02059v1",
        "title": "Signs in time: Encoding human motion as a temporal image",
        "abstract": "The goal of this work is to recognise and localise short temporal signals in image time series, where strong supervision is not available for training.   To this end we propose an image encoding that concisely represents human motion in a video sequence in a form that is suitable for learning with a ConvNet. The encoding reduces the pose information from an image to a single column, dramatically diminishing the input requirements for the network, but retaining the essential information for recognition.   The encoding is applied to the task of recognizing and localizing signed gestures in British Sign Language (BSL) videos. We demonstrate that using the proposed encoding, signs as short as 10 frames duration can be learnt from clips lasting hundreds of frames using only weak (clip level) supervision and with considerable label noise.",
        "authors": [
            "Joon Son Chung",
            "Andrew Zisserman"
        ],
        "submitted_date": "2016-08-06T03:37:28+00:00",
        "pdf_url": "https://arxiv.org/pdf/1608.02059v1",
        "similarity_score": 0.6838173270225525
    },
    {
        "arxiv_id": "2405.05672v1",
        "title": "Multi-Stream Keypoint Attention Network for Sign Language Recognition and Translation",
        "abstract": "Sign language serves as a non-vocal means of communication, transmitting information and significance through gestures, facial expressions, and bodily movements. The majority of current approaches for sign language recognition (SLR) and translation rely on RGB video inputs, which are vulnerable to fluctuations in the background. Employing a keypoint-based strategy not only mitigates the effects of background alterations but also substantially diminishes the computational demands of the model. Nevertheless, contemporary keypoint-based methodologies fail to fully harness the implicit knowledge embedded in keypoint sequences. To tackle this challenge, our inspiration is derived from the human cognition mechanism, which discerns sign language by analyzing the interplay between gesture configurations and supplementary elements. We propose a multi-stream keypoint attention network to depict a sequence of keypoints produced by a readily available keypoint estimator. In order to facilitate interaction across multiple streams, we investigate diverse methodologies such as keypoint fusion strategies, head fusion, and self-distillation. The resulting framework is denoted as MSKA-SLR, which is expanded into a sign language translation (SLT) model through the straightforward addition of an extra translation network. We carry out comprehensive experiments on well-known benchmarks like Phoenix-2014, Phoenix-2014T, and CSL-Daily to showcase the efficacy of our methodology. Notably, we have attained a novel state-of-the-art performance in the sign language translation task of Phoenix-2014T. The code and models can be accessed at: https://github.com/sutwangyan/MSKA.",
        "authors": [
            "Mo Guan",
            "Yan Wang",
            "Guangkun Ma",
            "Jiarui Liu",
            "Mingzu Sun"
        ],
        "submitted_date": "2024-05-09T10:58:37+00:00",
        "pdf_url": "https://arxiv.org/pdf/2405.05672v1",
        "similarity_score": 0.6833727359771729
    },
    {
        "arxiv_id": "2506.09643v1",
        "title": "Using Sign Language Production as Data Augmentation to enhance Sign Language Translation",
        "abstract": "Machine learning models fundamentally rely on large quantities of high-quality data. Collecting the necessary data for these models can be challenging due to cost, scarcity, and privacy restrictions. Signed languages are visual languages used by the deaf community and are considered low-resource languages. Sign language datasets are often orders of magnitude smaller than their spoken language counterparts. Sign Language Production is the task of generating sign language videos from spoken language sentences, while Sign Language Translation is the reverse translation task. Here, we propose leveraging recent advancements in Sign Language Production to augment existing sign language datasets and enhance the performance of Sign Language Translation models. For this, we utilize three techniques: a skeleton-based approach to production, sign stitching, and two photo-realistic generative models, SignGAN and SignSplat. We evaluate the effectiveness of these techniques in enhancing the performance of Sign Language Translation models by generating variation in the signer's appearance and the motion of the skeletal data. Our results demonstrate that the proposed methods can effectively augment existing datasets and enhance the performance of Sign Language Translation models by up to 19%, paving the way for more robust and accurate Sign Language Translation systems, even in resource-constrained environments.",
        "authors": [
            "Harry Walsh",
            "Maksym Ivashechkin",
            "Richard Bowden"
        ],
        "submitted_date": "2025-06-11T11:56:51+00:00",
        "pdf_url": "https://arxiv.org/pdf/2506.09643v1",
        "similarity_score": 0.6832224130630493
    },
    {
        "arxiv_id": "2103.16481v1",
        "title": "Read and Attend: Temporal Localisation in Sign Language Videos",
        "abstract": "The objective of this work is to annotate sign instances across a broad vocabulary in continuous sign language. We train a Transformer model to ingest a continuous signing stream and output a sequence of written tokens on a large-scale collection of signing footage with weakly-aligned subtitles. We show that through this training it acquires the ability to attend to a large vocabulary of sign instances in the input sequence, enabling their localisation. Our contributions are as follows: (1) we demonstrate the ability to leverage large quantities of continuous signing videos with weakly-aligned subtitles to localise signs in continuous sign language; (2) we employ the learned attention to automatically generate hundreds of thousands of annotations for a large sign vocabulary; (3) we collect a set of 37K manually verified sign instances across a vocabulary of 950 sign classes to support our study of sign language recognition; (4) by training on the newly annotated data from our method, we outperform the prior state of the art on the BSL-1K sign language recognition benchmark.",
        "authors": [
            "Gül Varol",
            "Liliane Momeni",
            "Samuel Albanie",
            "Triantafyllos Afouras",
            "Andrew Zisserman"
        ],
        "submitted_date": "2021-03-30T16:39:53+00:00",
        "pdf_url": "https://arxiv.org/pdf/2103.16481v1",
        "similarity_score": 0.6820277571678162
    },
    {
        "arxiv_id": "2103.08833v5",
        "title": "Skeleton Aware Multi-modal Sign Language Recognition",
        "abstract": "Sign language is commonly used by deaf or speech impaired people to communicate but requires significant effort to master. Sign Language Recognition (SLR) aims to bridge the gap between sign language users and others by recognizing signs from given videos. It is an essential yet challenging task since sign language is performed with the fast and complex movement of hand gestures, body posture, and even facial expressions. Recently, skeleton-based action recognition attracts increasing attention due to the independence between the subject and background variation. However, skeleton-based SLR is still under exploration due to the lack of annotations on hand keypoints. Some efforts have been made to use hand detectors with pose estimators to extract hand key points and learn to recognize sign language via Neural Networks, but none of them outperforms RGB-based methods. To this end, we propose a novel Skeleton Aware Multi-modal SLR framework (SAM-SLR) to take advantage of multi-modal information towards a higher recognition rate. Specifically, we propose a Sign Language Graph Convolution Network (SL-GCN) to model the embedded dynamics and a novel Separable Spatial-Temporal Convolution Network (SSTCN) to exploit skeleton features. RGB and depth modalities are also incorporated and assembled into our framework to provide global information that is complementary to the skeleton-based methods SL-GCN and SSTCN. As a result, SAM-SLR achieves the highest performance in both RGB (98.42\\%) and RGB-D (98.53\\%) tracks in 2021 Looking at People Large Scale Signer Independent Isolated SLR Challenge. Our code is available at https://github.com/jackyjsy/CVPR21Chal-SLR",
        "authors": [
            "Songyao Jiang",
            "Bin Sun",
            "Lichen Wang",
            "Yue Bai",
            "Kunpeng Li",
            "Yun Fu"
        ],
        "submitted_date": "2021-03-16T03:38:17+00:00",
        "pdf_url": "https://arxiv.org/pdf/2103.08833v5",
        "similarity_score": 0.6816153526306152
    },
    {
        "arxiv_id": "2509.08661v2",
        "title": "Skeleton-based sign language recognition using a dual-stream spatio-temporal dynamic graph convolutional network",
        "abstract": "Isolated Sign Language Recognition (ISLR) is challenged by gestures that are morphologically similar yet semantically distinct, a problem rooted in the complex interplay between hand shape and motion trajectory. Existing methods, often relying on a single reference frame, struggle to resolve this geometric ambiguity. This paper introduces Dual-SignLanguageNet (DSLNet), a dual-reference, dual-stream architecture that decouples and models gesture morphology and trajectory in separate, complementary coordinate systems. The architecture processes these streams through specialized networks: a topology-aware graph convolution models the view-invariant shape from a wrist-centric frame, while a Finsler geometry-based encoder captures the context-aware trajectory from a facial-centric frame. These features are then integrated via a geometry-driven optimal transport fusion mechanism. DSLNet sets a new state-of-the-art, achieving 93.70%, 89.97%, and 99.79% accuracy on the challenging WLASL-100, WLASL-300, and LSA64 datasets, respectively, with significantly fewer parameters than competing models.",
        "authors": [
            "Liangjin Liu",
            "Haoyang Zheng",
            "Zhengzhong Zhu",
            "Pei Zhou"
        ],
        "submitted_date": "2025-09-10T14:58:21+00:00",
        "pdf_url": "https://arxiv.org/pdf/2509.08661v2",
        "similarity_score": 0.6772356033325195
    },
    {
        "arxiv_id": "2404.00925v1",
        "title": "LLMs are Good Sign Language Translators",
        "abstract": "Sign Language Translation (SLT) is a challenging task that aims to translate sign videos into spoken language. Inspired by the strong translation capabilities of large language models (LLMs) that are trained on extensive multilingual text corpora, we aim to harness off-the-shelf LLMs to handle SLT. In this paper, we regularize the sign videos to embody linguistic characteristics of spoken language, and propose a novel SignLLM framework to transform sign videos into a language-like representation for improved readability by off-the-shelf LLMs. SignLLM comprises two key modules: (1) The Vector-Quantized Visual Sign module converts sign videos into a sequence of discrete character-level sign tokens, and (2) the Codebook Reconstruction and Alignment module converts these character-level tokens into word-level sign representations using an optimal transport formulation. A sign-text alignment loss further bridges the gap between sign and text tokens, enhancing semantic compatibility. We achieve state-of-the-art gloss-free results on two widely-used SLT benchmarks.",
        "authors": [
            "Jia Gong",
            "Lin Geng Foo",
            "Yixuan He",
            "Hossein Rahmani",
            "Jun Liu"
        ],
        "submitted_date": "2024-04-01T05:07:13+00:00",
        "pdf_url": "https://arxiv.org/pdf/2404.00925v1",
        "similarity_score": 0.6758600473403931
    },
    {
        "arxiv_id": "2406.12369v1",
        "title": "A Comparative Study of Continuous Sign Language Recognition Techniques",
        "abstract": "Continuous Sign Language Recognition (CSLR) focuses on the interpretation of a sequence of sign language gestures performed continually without pauses. In this study, we conduct an empirical evaluation of recent deep learning CSLR techniques and assess their performance across various datasets and sign languages. The models selected for analysis implement a range of approaches for extracting meaningful features and employ distinct training strategies. To determine their efficacy in modeling different sign languages, these models were evaluated using multiple datasets, specifically RWTH-PHOENIX-Weather-2014, ArabSign, and GrSL, each representing a unique sign language. The performance of the models was further tested with unseen signers and sentences. The conducted experiments establish new benchmarks on the selected datasets and provide valuable insights into the robustness and generalization of the evaluated techniques under challenging scenarios.",
        "authors": [
            "Sarah Alyami",
            "Hamzah Luqman"
        ],
        "submitted_date": "2024-06-18T07:51:44+00:00",
        "pdf_url": "https://arxiv.org/pdf/2406.12369v1",
        "similarity_score": 0.6747916340827942
    },
    {
        "arxiv_id": "2310.17429v1",
        "title": "LSA64: An Argentinian Sign Language Dataset",
        "abstract": "Automatic sign language recognition is a research area that encompasses human-computer interaction, computer vision and machine learning. Robust automatic recognition of sign language could assist in the translation process and the integration of hearing-impaired people, as well as the teaching of sign language to the hearing population. Sign languages differ significantly in different countries and even regions, and their syntax and semantics are different as well from those of written languages. While the techniques for automatic sign language recognition are mostly the same for different languages, training a recognition system for a new language requires having an entire dataset for that language. This paper presents a dataset of 64 signs from the Argentinian Sign Language (LSA). The dataset, called LSA64, contains 3200 videos of 64 different LSA signs recorded by 10 subjects, and is a first step towards building a comprehensive research-level dataset of Argentinian signs, specifically tailored to sign language recognition or other machine learning tasks. The subjects that performed the signs wore colored gloves to ease the hand tracking and segmentation steps, allowing experiments on the dataset to focus specifically on the recognition of signs. We also present a pre-processed version of the dataset, from which we computed statistics of movement, position and handshape of the signs.",
        "authors": [
            "Franco Ronchetti",
            "Facundo Manuel Quiroga",
            "César Estrebou",
            "Laura Lanzarini",
            "Alejandro Rosete"
        ],
        "submitted_date": "2023-10-26T14:37:01+00:00",
        "pdf_url": "https://arxiv.org/pdf/2310.17429v1",
        "similarity_score": 0.6747362613677979
    },
    {
        "arxiv_id": "2512.04048v1",
        "title": "Stable Signer: Hierarchical Sign Language Generative Model",
        "abstract": "Sign Language Production (SLP) is the process of converting the complex input text into a real video. Most previous works focused on the Text2Gloss, Gloss2Pose, Pose2Vid stages, and some concentrated on Prompt2Gloss and Text2Avatar stages. However, this field has made slow progress due to the inaccuracy of text conversion, pose generation, and the rendering of poses into real human videos in these stages, resulting in gradually accumulating errors. Therefore, in this paper, we streamline the traditional redundant structure, simplify and optimize the task objective, and design a new sign language generative model called Stable Signer. It redefines the SLP task as a hierarchical generation end-to-end task that only includes text understanding (Prompt2Gloss, Text2Gloss) and Pose2Vid, and executes text understanding through our proposed new Sign Language Understanding Linker called SLUL, and generates hand gestures through the named SLP-MoE hand gesture rendering expert block to end-to-end generate high-quality and multi-style sign language videos. SLUL is trained using the newly developed Semantic-Aware Gloss Masking Loss (SAGM Loss). Its performance has improved by 48.6% compared to the current SOTA generation methods.",
        "authors": [
            "Sen Fang",
            "Yalin Feng",
            "Hongbin Zhong",
            "Yanxin Zhang",
            "Dimitris N. Metaxas"
        ],
        "submitted_date": "2025-12-03T18:33:40+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.04048v1",
        "similarity_score": 0.6741878986358643
    },
    {
        "arxiv_id": "2212.00305v1",
        "title": "Multilingual Communication System with Deaf Individuals Utilizing Natural and Visual Languages",
        "abstract": "According to the World Federation of the Deaf, more than two hundred sign languages exist. Therefore, it is challenging to understand deaf individuals, even proficient sign language users, resulting in a barrier between the deaf community and the rest of society. To bridge this language barrier, we propose a novel multilingual communication system, namely MUGCAT, to improve the communication efficiency of sign language users. By converting recognized specific hand gestures into expressive pictures, which is universal usage and language independence, our MUGCAT system significantly helps deaf people convey their thoughts. To overcome the limitation of sign language usage, which is mostly impossible to translate into complete sentences for ordinary people, we propose to reconstruct meaningful sentences from the incomplete translation of sign language. We also measure the semantic similarity of generated sentences with fragmented recognized hand gestures to keep the original meaning. Experimental results show that the proposed system can work in a real-time manner and synthesize exquisite stunning illustrations and meaningful sentences from a few hand gestures of sign language. This proves that our MUGCAT has promising potential in assisting deaf communication.",
        "authors": [
            "Tuan-Luc Huynh",
            "Khoi-Nguyen Nguyen-Ngoc",
            "Chi-Bien Chu",
            "Minh-Triet Tran",
            "Trung-Nghia Le"
        ],
        "submitted_date": "2022-12-01T06:43:44+00:00",
        "pdf_url": "https://arxiv.org/pdf/2212.00305v1",
        "similarity_score": 0.6739065647125244
    },
    {
        "arxiv_id": "2302.11559v2",
        "title": "Word level Bangla Sign Language Dataset for Continuous BSL Recognition",
        "abstract": "An robust sign language recognition system can greatly alleviate communication barriers, particularly for people who struggle with verbal communication. This is crucial for human growth and progress as it enables the expression of thoughts, feelings, and ideas. However, sign recognition is a complex task that faces numerous challenges such as same gesture patterns for multiple signs, lighting, clothing, carrying conditions, and the presence of large poses, as well as illumination discrepancies across different views. Additionally, the absence of an extensive Bangla sign language video dataset makes it even more challenging to operate recognition systems, particularly when utilizing deep learning techniques. In order to address this issue, firstly, we created a large-scale dataset called the MVBSL-W50, which comprises 50 isolated words across 13 categories. Secondly, we developed an attention-based Bi-GRU model that captures the temporal dynamics of pose information for individuals communicating through sign language. The proposed model utilizes human pose information, which has shown to be successful in analyzing sign language patterns. By focusing solely on movement information and disregarding body appearance and environmental factors, the model is simplified and can achieve a speedier performance. The accuracy of the model is reported to be 85.64%.",
        "authors": [
            "Md Shamimul Islam",
            "A. J. M. Akhtarujjaman Joha",
            "Md Nur Hossain",
            "Sohaib Abdullah",
            "Ibrahim Elwarfalli",
            "Md Mahedi Hasan"
        ],
        "submitted_date": "2023-02-22T18:55:54+00:00",
        "pdf_url": "https://arxiv.org/pdf/2302.11559v2",
        "similarity_score": 0.6735569834709167
    },
    {
        "arxiv_id": "2411.13597v1",
        "title": "Enhancing Bidirectional Sign Language Communication: Integrating YOLOv8 and NLP for Real-Time Gesture Recognition & Translation",
        "abstract": "The primary concern of this research is to take American Sign Language (ASL) data through real time camera footage and be able to convert the data and information into text. Adding to that, we are also putting focus on creating a framework that can also convert text into sign language in real time which can help us break the language barrier for the people who are in need. In this work, for recognising American Sign Language (ASL), we have used the You Only Look Once(YOLO) model and Convolutional Neural Network (CNN) model. YOLO model is run in real time and automatically extracts discriminative spatial-temporal characteristics from the raw video stream without the need for any prior knowledge, eliminating design flaws. The CNN model here is also run in real time for sign language detection. We have introduced a novel method for converting text based input to sign language by making a framework that will take a sentence as input, identify keywords from that sentence and then show a video where sign language is performed with respect to the sentence given as input in real time. To the best of our knowledge, this is a rare study to demonstrate bidirectional sign language communication in real time in the American Sign Language (ASL).",
        "authors": [
            "Hasnat Jamil Bhuiyan",
            "Mubtasim Fuad Mozumder",
            "Md. Rabiul Islam Khan",
            "Md. Sabbir Ahmed",
            "Nabuat Zaman Nahim"
        ],
        "submitted_date": "2024-11-18T19:55:11+00:00",
        "pdf_url": "https://arxiv.org/pdf/2411.13597v1",
        "similarity_score": 0.6731187105178833
    },
    {
        "arxiv_id": "2411.12685v1",
        "title": "Enhanced Sign Language Translation between American Sign Language (ASL) and Indian Sign Language (ISL) Using LLMs",
        "abstract": "We have come up with a research that hopes to provide a bridge between the users of American Sign Language and the users of spoken language and Indian Sign Language (ISL). The research enabled us to create a novel framework that we have developed for Learner Systems. Leveraging art of Large models to create key features including: - Real-time translation between these two sign languages in an efficient manner. Making LLM's capability available for seamless translations to ISL. Here is the full study showing its implementation in this paper. The core of the system is a sophisticated pipeline that begins with reclassification and recognition of ASL gestures based on a strong Random Forest Classifier. By recognizing the ASL, it is translated into text which can be more easily processed. Highly evolved natural language NLP (Natural Language Processing) techniques come in handy as they play a role in our LLM integration where you then use LLMs to be able to convert the ASL text to ISL which provides you with the intent of sentence or phrase. The final step is to synthesize the translated text back into ISL gestures, creating an end-to-end translation experience using RIFE-Net. This framework is tasked with key challenges such as automatically dealing with gesture variability and overcoming the linguistic differences between ASL and ISL. By automating the translation process, we hope to vastly improve accessibility for sign language users. No longer will the communication gap between ASL and ISL create barriers; this totally cool innovation aims to bring our communities closer together. And we believe, with full confidence in our framework, that we're able to apply the same principles across a wide variety of sign language dialects.",
        "authors": [
            "Malay Kumar",
            "S. Sarvajit Visagan",
            "Tanish Sarang Mahajan",
            "Anisha Natarajan"
        ],
        "submitted_date": "2024-11-19T17:45:12+00:00",
        "pdf_url": "https://arxiv.org/pdf/2411.12685v1",
        "similarity_score": 0.6725030541419983
    },
    {
        "arxiv_id": "2010.05468v1",
        "title": "TSPNet: Hierarchical Feature Learning via Temporal Semantic Pyramid for Sign Language Translation",
        "abstract": "Sign language translation (SLT) aims to interpret sign video sequences into text-based natural language sentences. Sign videos consist of continuous sequences of sign gestures with no clear boundaries in between. Existing SLT models usually represent sign visual features in a frame-wise manner so as to avoid needing to explicitly segmenting the videos into isolated signs. However, these methods neglect the temporal information of signs and lead to substantial ambiguity in translation. In this paper, we explore the temporal semantic structures of signvideos to learn more discriminative features. To this end, we first present a novel sign video segment representation which takes into account multiple temporal granularities, thus alleviating the need for accurate video segmentation. Taking advantage of the proposed segment representation, we develop a novel hierarchical sign video feature learning method via a temporal semantic pyramid network, called TSPNet. Specifically, TSPNet introduces an inter-scale attention to evaluate and enhance local semantic consistency of sign segments and an intra-scale attention to resolve semantic ambiguity by using non-local video context. Experiments show that our TSPNet outperforms the state-of-the-art with significant improvements on the BLEU score (from 9.58 to 13.41) and ROUGE score (from 31.80 to 34.96)on the largest commonly-used SLT dataset. Our implementation is available at https://github.com/verashira/TSPNet.",
        "authors": [
            "Dongxu Li",
            "Chenchen Xu",
            "Xin Yu",
            "Kaihao Zhang",
            "Ben Swift",
            "Hanna Suominen",
            "Hongdong Li"
        ],
        "submitted_date": "2020-10-12T05:58:09+00:00",
        "pdf_url": "https://arxiv.org/pdf/2010.05468v1",
        "similarity_score": 0.6674916744232178
    },
    {
        "arxiv_id": "2406.06907v2",
        "title": "SignMusketeers: An Efficient Multi-Stream Approach for Sign Language Translation at Scale",
        "abstract": "A persistent challenge in sign language video processing, including the task of sign to written language translation, is how we learn representations of sign language in an effective and efficient way that preserves the important attributes of these languages, while remaining invariant to irrelevant visual differences. Informed by the nature and linguistics of signed languages, our proposed method focuses on just the most relevant parts in a signing video: the face, hands and body pose of the signer. However, instead of fully relying on pose estimation from off-the-shelf pose tracking models, which have inconsistent performance for hands and faces, we propose to learn a representation of the complex handshapes and facial expressions of sign languages in a self-supervised fashion. Our approach is based on learning from individual frames (rather than video sequences) and is therefore much more efficient than prior work on sign language pre-training. Compared to a recent model that established a new state of the art in sign language translation on the How2Sign dataset, our approach yields similar translation performance, using less than 3\\% of the compute.",
        "authors": [
            "Shester Gueuwou",
            "Xiaodan Du",
            "Greg Shakhnarovich",
            "Karen Livescu"
        ],
        "submitted_date": "2024-06-11T03:00:41+00:00",
        "pdf_url": "https://arxiv.org/pdf/2406.06907v2",
        "similarity_score": 0.6649922132492065
    },
    {
        "arxiv_id": "2010.12669v4",
        "title": "Position and Rotation Invariant Sign Language Recognition from 3D Kinect Data with Recurrent Neural Networks",
        "abstract": "Sign language is a gesture-based symbolic communication medium among speech and hearing impaired people. It also serves as a communication bridge between non-impaired and impaired populations. Unfortunately, in most situations, a non-impaired person is not well conversant in such symbolic languages restricting the natural information flow between these two categories. Therefore, an automated translation mechanism that seamlessly translates sign language into natural language can be highly advantageous. In this paper, we attempt to perform recognition of 30 basic Indian sign gestures. Gestures are represented as temporal sequences of 3D maps (RGB + depth), each consisting of 3D coordinates of 20 body joints captured by the Kinect sensor. A recurrent neural network (RNN) is employed as the classifier. To improve the classifier's performance, we use geometric transformation for the alignment correction of depth frames. In our experiments, the model achieves 84.81% accuracy.",
        "authors": [
            "Prasun Roy",
            "Saumik Bhattacharya",
            "Partha Pratim Roy",
            "Umapada Pal"
        ],
        "submitted_date": "2020-10-23T21:07:40+00:00",
        "pdf_url": "https://arxiv.org/pdf/2010.12669v4",
        "similarity_score": 0.6649001836776733
    },
    {
        "arxiv_id": "2510.11243v1",
        "title": "Nepali Sign Language Characters Recognition: Dataset Development and Deep Learning Approaches",
        "abstract": "Sign languages serve as essential communication systems for individuals with hearing and speech impairments. However, digital linguistic dataset resources for underrepresented sign languages, such as Nepali Sign Language (NSL), remain scarce. This study introduces the first benchmark dataset for NSL, consisting of 36 gesture classes with 1,500 samples per class, designed to capture the structural and visual features of the language. To evaluate recognition performance, we fine-tuned MobileNetV2 and ResNet50 architectures on the dataset, achieving classification accuracies of 90.45% and 88.78%, respectively. These findings demonstrate the effectiveness of convolutional neural networks in sign recognition tasks, particularly within low-resource settings. To the best of our knowledge, this work represents the first systematic effort to construct a benchmark dataset and assess deep learning approaches for NSL recognition, highlighting the potential of transfer learning and fine-tuning for advancing research in underexplored sign languages.",
        "authors": [
            "Birat Poudel",
            "Satyam Ghimire",
            "Sijan Bhattarai",
            "Saurav Bhandari",
            "Suramya Sharma Dahal"
        ],
        "submitted_date": "2025-10-13T10:29:08+00:00",
        "pdf_url": "https://arxiv.org/pdf/2510.11243v1",
        "similarity_score": 0.6648457050323486
    },
    {
        "arxiv_id": "2305.17714v1",
        "title": "An Open-Source Gloss-Based Baseline for Spoken to Signed Language Translation",
        "abstract": "Sign language translation systems are complex and require many components. As a result, it is very hard to compare methods across publications. We present an open-source implementation of a text-to-gloss-to-pose-to-video pipeline approach, demonstrating conversion from German to Swiss German Sign Language, French to French Sign Language of Switzerland, and Italian to Italian Sign Language of Switzerland. We propose three different components for the text-to-gloss translation: a lemmatizer, a rule-based word reordering and dropping component, and a neural machine translation system. Gloss-to-pose conversion occurs using data from a lexicon for three different signed languages, with skeletal poses extracted from videos. To generate a sentence, the text-to-gloss system is first run, and the pose representations of the resulting signs are stitched together.",
        "authors": [
            "Amit Moryossef",
            "Mathias Müller",
            "Anne Göhring",
            "Zifan Jiang",
            "Yoav Goldberg",
            "Sarah Ebling"
        ],
        "submitted_date": "2023-05-28T12:57:20+00:00",
        "pdf_url": "https://arxiv.org/pdf/2305.17714v1",
        "similarity_score": 0.6645025610923767
    },
    {
        "arxiv_id": "2506.20947v1",
        "title": "Hierarchical Sub-action Tree for Continuous Sign Language Recognition",
        "abstract": "Continuous sign language recognition (CSLR) aims to transcribe untrimmed videos into glosses, which are typically textual words. Recent studies indicate that the lack of large datasets and precise annotations has become a bottleneck for CSLR due to insufficient training data. To address this, some works have developed cross-modal solutions to align visual and textual modalities. However, they typically extract textual features from glosses without fully utilizing their knowledge. In this paper, we propose the Hierarchical Sub-action Tree (HST), termed HST-CSLR, to efficiently combine gloss knowledge with visual representation learning. By incorporating gloss-specific knowledge from large language models, our approach leverages textual information more effectively. Specifically, we construct an HST for textual information representation, aligning visual and textual modalities step-by-step and benefiting from the tree structure to reduce computational complexity. Additionally, we impose a contrastive alignment enhancement to bridge the gap between the two modalities. Experiments on four datasets (PHOENIX-2014, PHOENIX-2014T, CSL-Daily, and Sign Language Gesture) demonstrate the effectiveness of our HST-CSLR.",
        "authors": [
            "Dejie Yang",
            "Zhu Xu",
            "Xinjie Gao",
            "Yang Liu"
        ],
        "submitted_date": "2025-06-26T02:27:50+00:00",
        "pdf_url": "https://arxiv.org/pdf/2506.20947v1",
        "similarity_score": 0.6625994443893433
    },
    {
        "arxiv_id": "2004.01225v1",
        "title": "Temporal Accumulative Features for Sign Language Recognition",
        "abstract": "In this paper, we propose a set of features called temporal accumulative features (TAF) for representing and recognizing isolated sign language gestures. By incorporating sign language specific constructs to better represent the unique linguistic characteristic of sign language videos, we have devised an efficient and fast SLR method for recognizing isolated sign language gestures. The proposed method is an HSV based accumulative video representation where keyframes based on the linguistic movement-hold model are represented by different colors. We also incorporate hand shape information and using a small scale convolutional neural network, demonstrate that sequential modeling of accumulative features for linguistic subunits improves upon baseline classification results.",
        "authors": [
            "Ahmet Alp Kındıroğlu",
            "Oğulcan Özdemir",
            "Lale Akarun"
        ],
        "submitted_date": "2020-04-02T19:03:40+00:00",
        "pdf_url": "https://arxiv.org/pdf/2004.01225v1",
        "similarity_score": 0.6619446873664856
    },
    {
        "arxiv_id": "2508.12713v1",
        "title": "Real-Time Sign Language Gestures to Speech Transcription using Deep Learning",
        "abstract": "Communication barriers pose significant challenges for individuals with hearing and speech impairments, often limiting their ability to effectively interact in everyday environments. This project introduces a real-time assistive technology solution that leverages advanced deep learning techniques to translate sign language gestures into textual and audible speech. By employing convolution neural networks (CNN) trained on the Sign Language MNIST dataset, the system accurately classifies hand gestures captured live via webcam. Detected gestures are instantaneously translated into their corresponding meanings and transcribed into spoken language using text-to-speech synthesis, thus facilitating seamless communication. Comprehensive experiments demonstrate high model accuracy and robust real-time performance with some latency, highlighting the system's practical applicability as an accessible, reliable, and user-friendly tool for enhancing the autonomy and integration of sign language users in diverse social settings.",
        "authors": [
            "Brandone Fonya"
        ],
        "submitted_date": "2025-08-18T08:25:18+00:00",
        "pdf_url": "https://arxiv.org/pdf/2508.12713v1",
        "similarity_score": 0.6615870594978333
    },
    {
        "arxiv_id": "2308.12419v1",
        "title": "Toward American Sign Language Processing in the Real World: Data, Tasks, and Methods",
        "abstract": "Sign language, which conveys meaning through gestures, is the chief means of communication among deaf people. Recognizing sign language in natural settings presents significant challenges due to factors such as lighting, background clutter, and variations in signer characteristics. In this thesis, I study automatic sign language processing in the wild, using signing videos collected from the Internet. This thesis contributes new datasets, tasks, and methods. Most chapters of this thesis address tasks related to fingerspelling, an important component of sign language and yet has not been studied widely by prior work. I present three new large-scale ASL datasets in the wild: ChicagoFSWild, ChicagoFSWild+, and OpenASL. Using ChicagoFSWild and ChicagoFSWild+, I address fingerspelling recognition, which consists of transcribing fingerspelling sequences into text. I propose an end-to-end approach based on iterative attention that allows recognition from a raw video without explicit hand detection. I further show that using a Conformer-based network jointly modeling handshape and mouthing can bring performance close to that of humans. Next, I propose two tasks for building real-world fingerspelling-based applications: fingerspelling detection and search. For fingerspelling detection, I introduce a suite of evaluation metrics and a new detection model via multi-task training. To address the problem of searching for fingerspelled keywords in raw sign language videos, we propose a novel method that jointly localizes and matches fingerspelling segments to text. Finally, I will describe a benchmark for large-vocabulary open-domain sign language translation based on OpenASL. To address the challenges of sign language translation in realistic settings, we propose a set of techniques including sign search as a pretext task for pre-training and fusion of mouthing and handshape features.",
        "authors": [
            "Bowen Shi"
        ],
        "submitted_date": "2023-08-23T20:38:19+00:00",
        "pdf_url": "https://arxiv.org/pdf/2308.12419v1",
        "similarity_score": 0.6615540981292725
    },
    {
        "arxiv_id": "2408.15159v1",
        "title": "Empowering Sign Language Communication: Integrating Sentiment and Semantics for Facial Expression Synthesis",
        "abstract": "Translating written sentences from oral languages to a sequence of manual and non-manual gestures plays a crucial role in building a more inclusive society for deaf and hard-of-hearing people. Facial expressions (non-manual), in particular, are responsible for encoding the grammar of the sentence to be spoken, applying punctuation, pronouns, or emphasizing signs. These non-manual gestures are closely related to the semantics of the sentence being spoken and also to the utterance of the speaker's emotions. However, most Sign Language Production (SLP) approaches are centered on synthesizing manual gestures and do not focus on modeling the speakers expression. This paper introduces a new method focused in synthesizing facial expressions for sign language. Our goal is to improve sign language production by integrating sentiment information in facial expression generation. The approach leverages a sentence sentiment and semantic features to sample from a meaningful representation space, integrating the bias of the non-manual components into the sign language production process. To evaluate our method, we extend the Frechet Gesture Distance (FGD) and propose a new metric called Frechet Expression Distance (FED) and apply an extensive set of metrics to assess the quality of specific regions of the face. The experimental results showed that our method achieved state of the art, being superior to the competitors on How2Sign and PHOENIX14T datasets. Moreover, our architecture is based on a carefully designed graph pyramid that makes it simpler, easier to train, and capable of leveraging emotions to produce facial expressions.",
        "authors": [
            "Rafael Azevedo",
            "Thiago Coutinho",
            "João Ferreira",
            "Thiago Gomes",
            "Erickson Nascimento"
        ],
        "submitted_date": "2024-08-27T15:55:18+00:00",
        "pdf_url": "https://arxiv.org/pdf/2408.15159v1",
        "similarity_score": 0.6611732244491577
    },
    {
        "arxiv_id": "2304.14710v1",
        "title": "Image-based Indian Sign Language Recognition: A Practical Review using Deep Neural Networks",
        "abstract": "People with vocal and hearing disabilities use sign language to express themselves using visual gestures and signs. Although sign language is a solution for communication difficulties faced by deaf people, there are still problems as most of the general population cannot understand this language, creating a communication barrier, especially in places such as banks, airports, supermarkets, etc. [1]. A sign language recognition(SLR) system is a must to solve this problem. The main focus of this model is to develop a real-time word-level sign language recognition system that would translate sign language to text. Much research has been done on ASL(American sign language). Thus, we have worked on ISL(Indian sign language) to cater to the needs of the deaf and hard-of-hearing community of India[2]. In this research, we provide an Indian Sign Language-based Sign Language recognition system. For this analysis, the user must be able to take pictures of hand movements using a web camera, and the system must anticipate and display the name of the taken picture. The acquired image goes through several processing phases, some of which use computer vision techniques, including grayscale conversion, dilatation, and masking. Our model is trained using a convolutional neural network (CNN), which is then utilized to recognize the images. Our best model has a 99% accuracy rate[3].",
        "authors": [
            "Mallikharjuna Rao K",
            "Harleen Kaur",
            "Sanjam Kaur Bedi",
            "M A Lekhana"
        ],
        "submitted_date": "2023-04-28T09:27:04+00:00",
        "pdf_url": "https://arxiv.org/pdf/2304.14710v1",
        "similarity_score": 0.6602321863174438
    },
    {
        "arxiv_id": "2502.02196v1",
        "title": "Exploiting Ensemble Learning for Cross-View Isolated Sign Language Recognition",
        "abstract": "In this paper, we present our solution to the Cross-View Isolated Sign Language Recognition (CV-ISLR) challenge held at WWW 2025. CV-ISLR addresses a critical issue in traditional Isolated Sign Language Recognition (ISLR), where existing datasets predominantly capture sign language videos from a frontal perspective, while real-world camera angles often vary. To accurately recognize sign language from different viewpoints, models must be capable of understanding gestures from multiple angles, making cross-view recognition challenging. To address this, we explore the advantages of ensemble learning, which enhances model robustness and generalization across diverse views. Our approach, built on a multi-dimensional Video Swin Transformer model, leverages this ensemble strategy to achieve competitive performance. Finally, our solution ranked 3rd in both the RGB-based ISLR and RGB-D-based ISLR tracks, demonstrating the effectiveness in handling the challenges of cross-view recognition. The code is available at: https://github.com/Jiafei127/CV_ISLR_WWW2025.",
        "authors": [
            "Fei Wang",
            "Kun Li",
            "Yiqi Nie",
            "Zhangling Duan",
            "Peng Zou",
            "Zhiliang Wu",
            "Yuwei Wang",
            "Yanyan Wei"
        ],
        "submitted_date": "2025-02-04T10:21:28+00:00",
        "pdf_url": "https://arxiv.org/pdf/2502.02196v1",
        "similarity_score": 0.6595345735549927
    },
    {
        "arxiv_id": "2308.10809v1",
        "title": "Improving Continuous Sign Language Recognition with Cross-Lingual Signs",
        "abstract": "This work dedicates to continuous sign language recognition (CSLR), which is a weakly supervised task dealing with the recognition of continuous signs from videos, without any prior knowledge about the temporal boundaries between consecutive signs. Data scarcity heavily impedes the progress of CSLR. Existing approaches typically train CSLR models on a monolingual corpus, which is orders of magnitude smaller than that of speech recognition. In this work, we explore the feasibility of utilizing multilingual sign language corpora to facilitate monolingual CSLR. Our work is built upon the observation of cross-lingual signs, which originate from different sign languages but have similar visual signals (e.g., hand shape and motion). The underlying idea of our approach is to identify the cross-lingual signs in one sign language and properly leverage them as auxiliary training data to improve the recognition capability of another. To achieve the goal, we first build two sign language dictionaries containing isolated signs that appear in two datasets. Then we identify the sign-to-sign mappings between two sign languages via a well-optimized isolated sign language recognition model. At last, we train a CSLR model on the combination of the target data with original labels and the auxiliary data with mapped labels. Experimentally, our approach achieves state-of-the-art performance on two widely-used CSLR datasets: Phoenix-2014 and Phoenix-2014T.",
        "authors": [
            "Fangyun Wei",
            "Yutong Chen"
        ],
        "submitted_date": "2023-08-21T15:58:47+00:00",
        "pdf_url": "https://arxiv.org/pdf/2308.10809v1",
        "similarity_score": 0.6581591367721558
    },
    {
        "arxiv_id": "2309.12179v2",
        "title": "Autoregressive Sign Language Production: A Gloss-Free Approach with Discrete Representations",
        "abstract": "Gloss-free Sign Language Production (SLP) offers a direct translation of spoken language sentences into sign language, bypassing the need for gloss intermediaries. This paper presents the Sign language Vector Quantization Network, a novel approach to SLP that leverages Vector Quantization to derive discrete representations from sign pose sequences. Our method, rooted in both manual and non-manual elements of signing, supports advanced decoding methods and integrates latent-level alignment for enhanced linguistic coherence. Through comprehensive evaluations, we demonstrate superior performance of our method over prior SLP methods and highlight the reliability of Back-Translation and Fréchet Gesture Distance as evaluation metrics.",
        "authors": [
            "Eui Jun Hwang",
            "Huije Lee",
            "Jong C. Park"
        ],
        "submitted_date": "2023-09-21T15:46:01+00:00",
        "pdf_url": "https://arxiv.org/pdf/2309.12179v2",
        "similarity_score": 0.6570335626602173
    },
    {
        "arxiv_id": "2408.07244v1",
        "title": "Sign language recognition based on deep learning and low-cost handcrafted descriptors",
        "abstract": "In recent years, deep learning techniques have been used to develop sign language recognition systems, potentially serving as a communication tool for millions of hearing-impaired individuals worldwide. However, there are inherent challenges in creating such systems. Firstly, it is important to consider as many linguistic parameters as possible in gesture execution to avoid ambiguity between words. Moreover, to facilitate the real-world adoption of the created solution, it is essential to ensure that the chosen technology is realistic, avoiding expensive, intrusive, or low-mobility sensors, as well as very complex deep learning architectures that impose high computational requirements. Based on this, our work aims to propose an efficient sign language recognition system that utilizes low-cost sensors and techniques. To this end, an object detection model was trained specifically for detecting the interpreter's face and hands, ensuring focus on the most relevant regions of the image and generating inputs with higher semantic value for the classifier. Additionally, we introduced a novel approach to obtain features representing hand location and movement by leveraging spatial information derived from centroid positions of bounding boxes, thereby enhancing sign discrimination. The results demonstrate the efficiency of our handcrafted features, increasing accuracy by 7.96% on the AUTSL dataset, while adding fewer than 700 thousand parameters and incurring less than 10 milliseconds of additional inference time. These findings highlight the potential of our technique to strike a favorable balance between computational cost and accuracy, making it a promising approach for practical sign language recognition applications.",
        "authors": [
            "Alvaro Leandro Cavalcante Carneiro",
            "Denis Henrique Pinheiro Salvadeo",
            "Lucas de Brito Silva"
        ],
        "submitted_date": "2024-08-14T00:56:51+00:00",
        "pdf_url": "https://arxiv.org/pdf/2408.07244v1",
        "similarity_score": 0.6560136675834656
    },
    {
        "arxiv_id": "2411.17799v3",
        "title": "Signs as Tokens: A Retrieval-Enhanced Multilingual Sign Language Generator",
        "abstract": "Sign language is a visual language that encompasses all linguistic features of natural languages and serves as the primary communication method for the deaf and hard-of-hearing communities. Although many studies have successfully adapted pretrained language models (LMs) for sign language translation (sign-to-text), the reverse task-sign language generation (text-to-sign)-remains largely unexplored. In this work, we introduce a multilingual sign language model, Signs as Tokens (SOKE), which can generate 3D sign avatars autoregressively from text inputs using a pretrained LM. To align sign language with the LM, we leverage a decoupled tokenizer that discretizes continuous signs into token sequences representing various body parts. During decoding, unlike existing approaches that flatten all part-wise tokens into a single sequence and predict one token at a time, we propose a multi-head decoding method capable of predicting multiple tokens simultaneously. This approach improves inference efficiency while maintaining effective information fusion across different body parts. To further ease the generation process, we propose a retrieval-enhanced SLG approach, which incorporates external sign dictionaries to provide accurate word-level signs as auxiliary conditions, significantly improving the precision of generated signs. Extensive qualitative and quantitative evaluations demonstrate the effectiveness of SOKE.",
        "authors": [
            "Ronglai Zuo",
            "Rolandos Alexandros Potamias",
            "Evangelos Ververas",
            "Jiankang Deng",
            "Stefanos Zafeiriou"
        ],
        "submitted_date": "2024-11-26T18:28:09+00:00",
        "pdf_url": "https://arxiv.org/pdf/2411.17799v3",
        "similarity_score": 0.6551994681358337
    },
    {
        "arxiv_id": "2508.09362v1",
        "title": "FusionEnsemble-Net: An Attention-Based Ensemble of Spatiotemporal Networks for Multimodal Sign Language Recognition",
        "abstract": "Accurate recognition of sign language in healthcare communication poses a significant challenge, requiring frameworks that can accurately interpret complex multimodal gestures. To deal with this, we propose FusionEnsemble-Net, a novel attention-based ensemble of spatiotemporal networks that dynamically fuses visual and motion data to enhance recognition accuracy. The proposed approach processes RGB video and range Doppler map radar modalities synchronously through four different spatiotemporal networks. For each network, features from both modalities are continuously fused using an attention-based fusion module before being fed into an ensemble of classifiers. Finally, the outputs of these four different fused channels are combined in an ensemble classification head, thereby enhancing the model's robustness. Experiments demonstrate that FusionEnsemble-Net outperforms state-of-the-art approaches with a test accuracy of 99.44% on the large-scale MultiMeDaLIS dataset for Italian Sign Language. Our findings indicate that an ensemble of diverse spatiotemporal networks, unified by attention-based fusion, yields a robust and accurate framework for complex, multimodal isolated gesture recognition tasks. The source code is available at: https://github.com/rezwanh001/Multimodal-Isolated-Italian-Sign-Language-Recognition.",
        "authors": [
            "Md. Milon Islam",
            "Md Rezwanul Haque",
            "S M Taslim Uddin Raju",
            "Fakhri Karray"
        ],
        "submitted_date": "2025-08-12T21:44:23+00:00",
        "pdf_url": "https://arxiv.org/pdf/2508.09362v1",
        "similarity_score": 0.654157817363739
    },
    {
        "arxiv_id": "2408.14111v1",
        "title": "Bengali Sign Language Recognition through Hand Pose Estimation using Multi-Branch Spatial-Temporal Attention Model",
        "abstract": "Hand gesture-based sign language recognition (SLR) is one of the most advanced applications of machine learning, and computer vision uses hand gestures. Although, in the past few years, many researchers have widely explored and studied how to address BSL problems, specific unaddressed issues remain, such as skeleton and transformer-based BSL recognition. In addition, the lack of evaluation of the BSL model in various concealed environmental conditions can prove the generalized property of the existing model by facing daily life signs. As a consequence, existing BSL recognition systems provide a limited perspective of their generalisation ability as they are tested on datasets containing few BSL alphabets that have a wide disparity in gestures and are easy to differentiate. To overcome these limitations, we propose a spatial-temporal attention-based BSL recognition model considering hand joint skeletons extracted from the sequence of images. The main aim of utilising hand skeleton-based BSL data is to ensure the privacy and low-resolution sequence of images, which need minimum computational cost and low hardware configurations. Our model captures discriminative structural displacements and short-range dependency based on unified joint features projected onto high-dimensional feature space. Specifically, the use of Separable TCN combined with a powerful multi-head spatial-temporal attention architecture generated high-performance accuracy. The extensive experiments with a proposed dataset and two benchmark BSL datasets with a wide range of evaluations, such as intra- and inter-dataset evaluation settings, demonstrated that our proposed models achieve competitive performance with extremely low computational complexity and run faster than existing models.",
        "authors": [
            "Abu Saleh Musa Miah",
            "Md. Al Mehedi Hasan",
            "Md Hadiuzzaman",
            "Muhammad Nazrul Islam",
            "Jungpil Shin"
        ],
        "submitted_date": "2024-08-26T08:55:16+00:00",
        "pdf_url": "https://arxiv.org/pdf/2408.14111v1",
        "similarity_score": 0.6540279388427734
    },
    {
        "arxiv_id": "2307.14768v1",
        "title": "Gloss-free Sign Language Translation: Improving from Visual-Language Pretraining",
        "abstract": "Sign Language Translation (SLT) is a challenging task due to its cross-domain nature, involving the translation of visual-gestural language to text. Many previous methods employ an intermediate representation, i.e., gloss sequences, to facilitate SLT, thus transforming it into a two-stage task of sign language recognition (SLR) followed by sign language translation (SLT). However, the scarcity of gloss-annotated sign language data, combined with the information bottleneck in the mid-level gloss representation, has hindered the further development of the SLT task. To address this challenge, we propose a novel Gloss-Free SLT based on Visual-Language Pretraining (GFSLT-VLP), which improves SLT by inheriting language-oriented prior knowledge from pre-trained models, without any gloss annotation assistance. Our approach involves two stages: (i) integrating Contrastive Language-Image Pre-training (CLIP) with masked self-supervised learning to create pre-tasks that bridge the semantic gap between visual and textual representations and restore masked sentences, and (ii) constructing an end-to-end architecture with an encoder-decoder-like structure that inherits the parameters of the pre-trained Visual Encoder and Text Decoder from the first stage. The seamless combination of these novel designs forms a robust sign language representation and significantly improves gloss-free sign language translation. In particular, we have achieved unprecedented improvements in terms of BLEU-4 score on the PHOENIX14T dataset (>+5) and the CSL-Daily dataset (>+3) compared to state-of-the-art gloss-free SLT methods. Furthermore, our approach also achieves competitive results on the PHOENIX14T dataset when compared with most of the gloss-based methods. Our code is available at https://github.com/zhoubenjia/GFSLT-VLP.",
        "authors": [
            "Benjia Zhou",
            "Zhigang Chen",
            "Albert Clapés",
            "Jun Wan",
            "Yanyan Liang",
            "Sergio Escalera",
            "Zhen Lei",
            "Du Zhang"
        ],
        "submitted_date": "2023-07-27T10:59:18+00:00",
        "pdf_url": "https://arxiv.org/pdf/2307.14768v1",
        "similarity_score": 0.6533757448196411
    },
    {
        "arxiv_id": "2406.07119v1",
        "title": "T2S-GPT: Dynamic Vector Quantization for Autoregressive Sign Language Production from Text",
        "abstract": "In this work, we propose a two-stage sign language production (SLP) paradigm that first encodes sign language sequences into discrete codes and then autoregressively generates sign language from text based on the learned codebook. However, existing vector quantization (VQ) methods are fixed-length encodings, overlooking the uneven information density in sign language, which leads to under-encoding of important regions and over-encoding of unimportant regions. To address this issue, we propose a novel dynamic vector quantization (DVA-VAE) model that can dynamically adjust the encoding length based on the information density in sign language to achieve accurate and compact encoding. Then, a GPT-like model learns to generate code sequences and their corresponding durations from spoken language text. Extensive experiments conducted on the PHOENIX14T dataset demonstrate the effectiveness of our proposed method. To promote sign language research, we propose a new large German sign language dataset, PHOENIX-News, which contains 486 hours of sign language videos, audio, and transcription texts.Experimental analysis on PHOENIX-News shows that the performance of our model can be further improved by increasing the size of the training data. Our project homepage is https://t2sgpt-demo.yinaoxiong.cn.",
        "authors": [
            "Aoxiong Yin",
            "Haoyuan Li",
            "Kai Shen",
            "Siliang Tang",
            "Yueting Zhuang"
        ],
        "submitted_date": "2024-06-11T10:06:53+00:00",
        "pdf_url": "https://arxiv.org/pdf/2406.07119v1",
        "similarity_score": 0.6530122756958008
    },
    {
        "arxiv_id": "2207.05261v1",
        "title": "Building Korean Sign Language Augmentation (KoSLA) Corpus with Data Augmentation Technique",
        "abstract": "We present an efficient framework of corpus for sign language translation. Aided with a simple but dramatic data augmentation technique, our method converts text into annotated forms with minimum information loss. Sign languages are composed of manual signals, non-manual signals, and iconic features. According to professional sign language interpreters, non-manual signals such as facial expressions and gestures play an important role in conveying exact meaning. By considering the linguistic features of sign language, our proposed framework is a first and unique attempt to build a multimodal sign language augmentation corpus (hereinafter referred to as the KoSLA corpus) containing both manual and non-manual modalities. The corpus we built demonstrates confident results in the hospital context, showing improved performance with augmented datasets. To overcome data scarcity, we resorted to data augmentation techniques such as synonym replacement to boost the efficiency of our translation model and available data, while maintaining grammatical and semantic structures of sign language. For the experimental support, we verify the effectiveness of data augmentation technique and usefulness of our corpus by performing a translation task between normal sentences and sign language annotations on two tokenizers. The result was convincing, proving that the BLEU scores with the KoSLA corpus were significant.",
        "authors": [
            "Changnam An",
            "Eunkyung Han",
            "Dongmyeong Noh",
            "Ohkyoon Kwon",
            "Sumi Lee",
            "Hyunshim Han"
        ],
        "submitted_date": "2022-07-12T02:12:36+00:00",
        "pdf_url": "https://arxiv.org/pdf/2207.05261v1",
        "similarity_score": 0.6517284512519836
    },
    {
        "arxiv_id": "2411.04517v1",
        "title": "Continuous Sign Language Recognition System using Deep Learning with MediaPipe Holistic",
        "abstract": "Sign languages are the language of hearing-impaired people who use visuals like the hand, facial, and body movements for communication. There are different signs and gestures representing alphabets, words, and phrases. Nowadays approximately 300 sign languages are being practiced worldwide such as American Sign Language (ASL), Chinese Sign Language (CSL), Indian Sign Language (ISL), and many more. Sign languages are dependent on the vocal language of a place. Unlike vocal or spoken languages, there are no helping words in sign language like is, am, are, was, were, will, be, etc. As only a limited population is well-versed in sign language, this lack of familiarity of sign language hinders hearing-impaired people from communicating freely and easily with everyone. This issue can be addressed by a sign language recognition (SLR) system which has the capability to translate the sign language into vocal language. In this paper, a continuous SLR system is proposed using a deep learning model employing Long Short-Term Memory (LSTM), trained and tested on an ISL primary dataset. This dataset is created using MediaPipe Holistic pipeline for tracking face, hand, and body movements and collecting landmarks. The system recognizes the signs and gestures in real-time with 88.23% accuracy.",
        "authors": [
            "Sharvani Srivastava",
            "Sudhakar Singh",
            "Pooja",
            "Shiv Prakash"
        ],
        "submitted_date": "2024-11-07T08:19:39+00:00",
        "pdf_url": "https://arxiv.org/pdf/2411.04517v1",
        "similarity_score": 0.6515116691589355
    },
    {
        "arxiv_id": "2103.15910v1",
        "title": "Sign Language Production: A Review",
        "abstract": "Sign Language is the dominant yet non-primary form of communication language used in the deaf and hearing-impaired community. To make an easy and mutual communication between the hearing-impaired and the hearing communities, building a robust system capable of translating the spoken language into sign language and vice versa is fundamental. To this end, sign language recognition and production are two necessary parts for making such a two-way system. Sign language recognition and production need to cope with some critical challenges. In this survey, we review recent advances in Sign Language Production (SLP) and related areas using deep learning. This survey aims to briefly summarize recent achievements in SLP, discussing their advantages, limitations, and future directions of research.",
        "authors": [
            "Razieh Rastgoo",
            "Kourosh Kiani",
            "Sergio Escalera",
            "Mohammad Sabokrou"
        ],
        "submitted_date": "2021-03-29T19:38:22+00:00",
        "pdf_url": "https://arxiv.org/pdf/2103.15910v1",
        "similarity_score": 0.6488083004951477
    },
    {
        "arxiv_id": "1908.10546v1",
        "title": "Fingerspelling recognition in the wild with iterative visual attention",
        "abstract": "Sign language recognition is a challenging gesture sequence recognition problem, characterized by quick and highly coarticulated motion. In this paper we focus on recognition of fingerspelling sequences in American Sign Language (ASL) videos collected in the wild, mainly from YouTube and Deaf social media. Most previous work on sign language recognition has focused on controlled settings where the data is recorded in a studio environment and the number of signers is limited. Our work aims to address the challenges of real-life data, reducing the need for detection or segmentation modules commonly used in this domain. We propose an end-to-end model based on an iterative attention mechanism, without explicit hand detection or segmentation. Our approach dynamically focuses on increasingly high-resolution regions of interest. It outperforms prior work by a large margin. We also introduce a newly collected data set of crowdsourced annotations of fingerspelling in the wild, and show that performance can be further improved with this additional data set.",
        "authors": [
            "Bowen Shi",
            "Aurora Martinez Del Rio",
            "Jonathan Keane",
            "Diane Brentari",
            "Greg Shakhnarovich",
            "Karen Livescu"
        ],
        "submitted_date": "2019-08-28T04:52:32+00:00",
        "pdf_url": "https://arxiv.org/pdf/1908.10546v1",
        "similarity_score": 0.6479707956314087
    },
    {
        "arxiv_id": "2305.14527v3",
        "title": "Slovo: Russian Sign Language Dataset",
        "abstract": "One of the main challenges of the sign language recognition task is the difficulty of collecting a suitable dataset due to the gap between hard-of-hearing and hearing societies. In addition, the sign language in each country differs significantly, which obliges the creation of new data for each of them. This paper presents the Russian Sign Language (RSL) video dataset Slovo, produced using crowdsourcing platforms. The dataset contains 20,000 FullHD recordings, divided into 1,000 classes of isolated RSL gestures received by 194 signers. We also provide the entire dataset creation pipeline, from data collection to video annotation, with the following demo application. Several neural networks are trained and evaluated on the Slovo to demonstrate its teaching ability. Proposed data and pre-trained models are publicly available.",
        "authors": [
            "Alexander Kapitanov",
            "Karina Kvanchiani",
            "Alexander Nagaev",
            "Elizaveta Petrova"
        ],
        "submitted_date": "2023-05-23T21:00:42+00:00",
        "pdf_url": "https://arxiv.org/pdf/2305.14527v3",
        "similarity_score": 0.6473305225372314
    },
    {
        "arxiv_id": "2304.10256v1",
        "title": "Indian Sign Language Recognition Using Mediapipe Holistic",
        "abstract": "Deaf individuals confront significant communication obstacles on a daily basis. Their inability to hear makes it difficult for them to communicate with those who do not understand sign language. Moreover, it presents difficulties in educational, occupational, and social contexts. By providing alternative communication channels, technology can play a crucial role in overcoming these obstacles. One such technology that can facilitate communication between deaf and hearing individuals is sign language recognition. We will create a robust system for sign language recognition in order to convert Indian Sign Language to text or speech. We will evaluate the proposed system and compare CNN and LSTM models. Since there are both static and gesture sign languages, a robust model is required to distinguish between them. In this study, we discovered that a CNN model captures letters and characters for recognition of static sign language better than an LSTM model, but it outperforms CNN by monitoring hands, faces, and pose in gesture sign language phrases and sentences. The creation of a text-to-sign language paradigm is essential since it will enhance the sign language-dependent deaf and hard-of-hearing population's communication skills. Even though the sign-to-text translation is just one side of communication, not all deaf or hard-of-hearing people are proficient in reading or writing text. Some may have difficulty comprehending written language due to educational or literacy issues. Therefore, a text-to-sign language paradigm would allow them to comprehend text-based information and participate in a variety of social, educational, and professional settings.   Keywords: deaf and hard-of-hearing, DHH, Indian sign language, CNN, LSTM, static and gesture sign languages, text-to-sign language model, MediaPipe Holistic, sign language recognition, SLR, SLT",
        "authors": [
            "Velmathi G",
            "Kaushal Goyal"
        ],
        "submitted_date": "2023-04-20T12:25:47+00:00",
        "pdf_url": "https://arxiv.org/pdf/2304.10256v1",
        "similarity_score": 0.6465283632278442
    },
    {
        "arxiv_id": "2402.08635v1",
        "title": "BdSLW60: A Word-Level Bangla Sign Language Dataset",
        "abstract": "Sign language discourse is an essential mode of daily communication for the deaf and hard-of-hearing people. However, research on Bangla Sign Language (BdSL) faces notable limitations, primarily due to the lack of datasets. Recognizing wordlevel signs in BdSL (WL-BdSL) presents a multitude of challenges, including the need for well-annotated datasets, capturing the dynamic nature of sign gestures from facial or hand landmarks, developing suitable machine learning or deep learning-based models with substantial video samples, and so on. In this paper, we address these challenges by creating a comprehensive BdSL word-level dataset named BdSLW60 in an unconstrained and natural setting, allowing positional and temporal variations and allowing sign users to change hand dominance freely. The dataset encompasses 60 Bangla sign words, with a significant scale of 9307 video trials provided by 18 signers under the supervision of a sign language professional. The dataset was rigorously annotated and cross-checked by 60 annotators. We also introduced a unique approach of a relative quantization-based key frame encoding technique for landmark based sign gesture recognition. We report the benchmarking of our BdSLW60 dataset using the Support Vector Machine (SVM) with testing accuracy up to 67.6% and an attention-based bi-LSTM with testing accuracy up to 75.1%. The dataset is available at https://www.kaggle.com/datasets/hasaniut/bdslw60 and the code base is accessible from https://github.com/hasanssl/BdSLW60_Code.",
        "authors": [
            "Husne Ara Rubaiyeat",
            "Hasan Mahmud",
            "Ahsan Habib",
            "Md. Kamrul Hasan"
        ],
        "submitted_date": "2024-02-13T18:02:58+00:00",
        "pdf_url": "https://arxiv.org/pdf/2402.08635v1",
        "similarity_score": 0.6452305316925049
    },
    {
        "arxiv_id": "2007.12131v2",
        "title": "BSL-1K: Scaling up co-articulated sign language recognition using mouthing cues",
        "abstract": "Recent progress in fine-grained gesture and action classification, and machine translation, point to the possibility of automated sign language recognition becoming a reality. A key stumbling block in making progress towards this goal is a lack of appropriate training data, stemming from the high complexity of sign annotation and a limited supply of qualified annotators. In this work, we introduce a new scalable approach to data collection for sign recognition in continuous videos. We make use of weakly-aligned subtitles for broadcast footage together with a keyword spotting method to automatically localise sign-instances for a vocabulary of 1,000 signs in 1,000 hours of video. We make the following contributions: (1) We show how to use mouthing cues from signers to obtain high-quality annotations from video data - the result is the BSL-1K dataset, a collection of British Sign Language (BSL) signs of unprecedented scale; (2) We show that we can use BSL-1K to train strong sign recognition models for co-articulated signs in BSL and that these models additionally form excellent pretraining for other sign languages and benchmarks - we exceed the state of the art on both the MSASL and WLASL benchmarks. Finally, (3) we propose new large-scale evaluation sets for the tasks of sign recognition and sign spotting and provide baselines which we hope will serve to stimulate research in this area.",
        "authors": [
            "Samuel Albanie",
            "Gül Varol",
            "Liliane Momeni",
            "Triantafyllos Afouras",
            "Joon Son Chung",
            "Neil Fox",
            "Andrew Zisserman"
        ],
        "submitted_date": "2020-07-23T16:59:01+00:00",
        "pdf_url": "https://arxiv.org/pdf/2007.12131v2",
        "similarity_score": 0.6441970467567444
    },
    {
        "arxiv_id": "2409.10874v1",
        "title": "American Sign Language to Text Translation using Transformer and Seq2Seq with LSTM",
        "abstract": "Sign language translation is one of the important issues in communication between deaf and hearing people, as it expresses words through hand, body, and mouth movements. American Sign Language is one of the sign languages used, one of which is the alphabetic sign. The development of neural machine translation technology is moving towards sign language translation. Transformer became the state-of-the-art in natural language processing. This study compares the Transformer with the Sequence-to-Sequence (Seq2Seq) model in translating sign language to text. In addition, an experiment was conducted by adding Residual Long Short-Term Memory (ResidualLSTM) in the Transformer. The addition of ResidualLSTM to the Transformer reduces the performance of the Transformer model by 23.37% based on the BLEU Score value. In comparison, the Transformer itself increases the BLEU Score value by 28.14 compared to the Seq2Seq model.",
        "authors": [
            "Gregorius Guntur Sunardi Putra",
            "Adifa Widyadhani Chanda D'Layla",
            "Dimas Wahono",
            "Riyanarto Sarno",
            "Agus Tri Haryono"
        ],
        "submitted_date": "2024-09-17T04:00:33+00:00",
        "pdf_url": "https://arxiv.org/pdf/2409.10874v1",
        "similarity_score": 0.6439914703369141
    },
    {
        "arxiv_id": "2505.13784v2",
        "title": "Transfer Learning from Visual Speech Recognition to Mouthing Recognition in German Sign Language",
        "abstract": "Sign Language Recognition (SLR) systems primarily focus on manual gestures, but non-manual features such as mouth movements, specifically mouthing, provide valuable linguistic information. This work directly classifies mouthing instances to their corresponding words in the spoken language while exploring the potential of transfer learning from Visual Speech Recognition (VSR) to mouthing recognition in German Sign Language. We leverage three VSR datasets: one in English, one in German with unrelated words and one in German containing the same target words as the mouthing dataset, to investigate the impact of task similarity in this setting. Our results demonstrate that multi-task learning improves both mouthing recognition and VSR accuracy as well as model robustness, suggesting that mouthing recognition should be treated as a distinct but related task to VSR. This research contributes to the field of SLR by proposing knowledge transfer from VSR to SLR datasets with limited mouthing annotations.",
        "authors": [
            "Dinh Nam Pham",
            "Eleftherios Avramidis"
        ],
        "submitted_date": "2025-05-20T00:15:51+00:00",
        "pdf_url": "https://arxiv.org/pdf/2505.13784v2",
        "similarity_score": 0.6439616680145264
    },
    {
        "arxiv_id": "2411.12865v2",
        "title": "AzSLD: Azerbaijani Sign Language Dataset for Fingerspelling, Word, and Sentence Translation with Baseline Software",
        "abstract": "Sign language processing technology development relies on extensive and reliable datasets, instructions, and ethical guidelines. We present a comprehensive Azerbaijani Sign Language Dataset (AzSLD) collected from diverse sign language users and linguistic parameters to facilitate advancements in sign recognition and translation systems and support the local sign language community. The dataset was created within the framework of a vision-based AzSL translation project. This study introduces the dataset as a summary of the fingerspelling alphabet and sentence- and word-level sign language datasets. The dataset was collected from signers of different ages, genders, and signing styles, with videos recorded from two camera angles to capture each sign in full detail. This approach ensures robust training and evaluation of gesture recognition models. AzSLD contains 30,000 videos, each carefully annotated with accurate sign labels and corresponding linguistic translations. The dataset is accompanied by technical documentation and source code to facilitate its use in training and testing. This dataset offers a valuable resource of labeled data for researchers and developers working on sign language recognition, translation, or synthesis. Ethical guidelines were strictly followed throughout the project, with all participants providing informed consent for collecting, publishing, and using the data.",
        "authors": [
            "Nigar Alishzade",
            "Jamaladdin Hasanov"
        ],
        "submitted_date": "2024-11-19T21:15:47+00:00",
        "pdf_url": "https://arxiv.org/pdf/2411.12865v2",
        "similarity_score": 0.6437768936157227
    },
    {
        "arxiv_id": "2210.05404v2",
        "title": "Machine Translation between Spoken Languages and Signed Languages Represented in SignWriting",
        "abstract": "This paper presents work on novel machine translation (MT) systems between spoken and signed languages, where signed languages are represented in SignWriting, a sign language writing system. Our work seeks to address the lack of out-of-the-box support for signed languages in current MT systems and is based on the SignBank dataset, which contains pairs of spoken language text and SignWriting content. We introduce novel methods to parse, factorize, decode, and evaluate SignWriting, leveraging ideas from neural factored MT. In a bilingual setup--translating from American Sign Language to (American) English--our method achieves over 30 BLEU, while in two multilingual setups--translating in both directions between spoken languages and signed languages--we achieve over 20 BLEU. We find that common MT techniques used to improve spoken language translation similarly affect the performance of sign language translation. These findings validate our use of an intermediate text representation for signed languages to include them in natural language processing research.",
        "authors": [
            "Zifan Jiang",
            "Amit Moryossef",
            "Mathias Müller",
            "Sarah Ebling"
        ],
        "submitted_date": "2022-10-11T12:28:06+00:00",
        "pdf_url": "https://arxiv.org/pdf/2210.05404v2",
        "similarity_score": 0.6431000232696533
    },
    {
        "arxiv_id": "2407.15668v2",
        "title": "SLVideo: A Sign Language Video Moment Retrieval Framework",
        "abstract": "SLVideo is a video moment retrieval system for Sign Language videos that incorporates facial expressions, addressing this gap in existing technology. The system extracts embedding representations for the hand and face signs from video frames to capture the signs in their entirety, enabling users to search for a specific sign language video segment with text queries. A collection of eight hours of annotated Portuguese Sign Language videos is used as the dataset, and a CLIP model is used to generate the embeddings. The initial results are promising in a zero-shot setting. In addition, SLVideo incorporates a thesaurus that enables users to search for similar signs to those retrieved, using the video segment embeddings, and also supports the edition and creation of video sign language annotations. Project web page: https://novasearch.github.io/SLVideo/",
        "authors": [
            "Gonçalo Vinagre Martins",
            "João Magalhães",
            "Afonso Quinaz",
            "Carla Viegas",
            "Sofia Cavaco"
        ],
        "submitted_date": "2024-07-22T14:29:36+00:00",
        "pdf_url": "https://arxiv.org/pdf/2407.15668v2",
        "similarity_score": 0.6407006978988647
    },
    {
        "arxiv_id": "2302.05759v1",
        "title": "Improving Sign Recognition with Phonology",
        "abstract": "We use insights from research on American Sign Language (ASL) phonology to train models for isolated sign language recognition (ISLR), a step towards automatic sign language understanding. Our key insight is to explicitly recognize the role of phonology in sign production to achieve more accurate ISLR than existing work which does not consider sign language phonology. We train ISLR models that take in pose estimations of a signer producing a single sign to predict not only the sign but additionally its phonological characteristics, such as the handshape. These auxiliary predictions lead to a nearly 9% absolute gain in sign recognition accuracy on the WLASL benchmark, with consistent improvements in ISLR regardless of the underlying prediction model architecture. This work has the potential to accelerate linguistic research in the domain of signed languages and reduce communication barriers between deaf and hearing people.",
        "authors": [
            "Lee Kezar",
            "Jesse Thomason",
            "Zed Sevcikova Sehyr"
        ],
        "submitted_date": "2023-02-11T18:51:23+00:00",
        "pdf_url": "https://arxiv.org/pdf/2302.05759v1",
        "similarity_score": 0.6404997110366821
    },
    {
        "arxiv_id": "2509.10845v1",
        "title": "Text2Sign Diffusion: A Generative Approach for Gloss-Free Sign Language Production",
        "abstract": "Sign language production (SLP) aims to translate spoken language sentences into a sequence of pose frames in a sign language, bridging the communication gap and promoting digital inclusion for deaf and hard-of-hearing communities. Existing methods typically rely on gloss, a symbolic representation of sign language words or phrases that serves as an intermediate step in SLP. This limits the flexibility and generalization of SLP, as gloss annotations are often unavailable and language-specific. Therefore, we present a novel diffusion-based generative approach - Text2Sign Diffusion (Text2SignDiff) for gloss-free SLP. Specifically, a gloss-free latent diffusion model is proposed to generate sign language sequences from noisy latent sign codes and spoken text jointly, reducing the potential error accumulation through a non-autoregressive iterative denoising process. We also design a cross-modal signing aligner that learns a shared latent space to bridge visual and textual content in sign and spoken languages. This alignment supports the conditioned diffusion-based process, enabling more accurate and contextually relevant sign language generation without gloss. Extensive experiments on the commonly used PHOENIX14T and How2Sign datasets demonstrate the effectiveness of our method, achieving the state-of-the-art performance.",
        "authors": [
            "Liqian Feng",
            "Lintao Wang",
            "Kun Hu",
            "Dehui Kong",
            "Zhiyong Wang"
        ],
        "submitted_date": "2025-09-13T15:05:19+00:00",
        "pdf_url": "https://arxiv.org/pdf/2509.10845v1",
        "similarity_score": 0.6404287815093994
    },
    {
        "arxiv_id": "2306.02144v2",
        "title": "A two-way translation system of Chinese sign language based on computer vision",
        "abstract": "As the main means of communication for deaf people, sign language has a special grammatical order, so it is meaningful and valuable to develop a real-time translation system for sign language. In the research process, we added a TSM module to the lightweight neural network model for the large Chinese continuous sign language dataset . It effectively improves the network performance with high accuracy and fast recognition speed. At the same time, we improve the Bert-Base-Chinese model to divide Chinese sentences into words and mapping the natural word order to the statute sign language order, and finally use the corresponding word videos in the isolated sign language dataset to generate the sentence video, so as to achieve the function of text-to-sign language translation. In the last of our research we built a system with sign language recognition and translation functions, and conducted performance tests on the complete dataset. The sign language video recognition accuracy reached about 99.3% with a time of about 0.05 seconds, and the sign language generation video time was about 1.3 seconds. The sign language system has good performance performance and is feasible.",
        "authors": [
            "Shengzhuo Wei",
            "Yan Lan"
        ],
        "submitted_date": "2023-06-03T16:00:57+00:00",
        "pdf_url": "https://arxiv.org/pdf/2306.02144v2",
        "similarity_score": 0.6400244235992432
    },
    {
        "arxiv_id": "2510.22011v1",
        "title": "Reconnaissance Automatique des Langues des Signes : Une Approche Hybridée CNN-LSTM Basée sur Mediapipe",
        "abstract": "Sign languages play a crucial role in the communication of deaf communities, but they are often marginalized, limiting access to essential services such as healthcare and education. This study proposes an automatic sign language recognition system based on a hybrid CNN-LSTM architecture, using Mediapipe for gesture keypoint extraction. Developed with Python, TensorFlow and Streamlit, the system provides real-time gesture translation. The results show an average accuracy of 92\\%, with very good performance for distinct gestures such as ``Hello'' and ``Thank you''. However, some confusions remain for visually similar gestures, such as ``Call'' and ``Yes''. This work opens up interesting perspectives for applications in various fields such as healthcare, education and public services.",
        "authors": [
            "Fraisse Sacré Takouchouang",
            "Ho Tuong Vinh"
        ],
        "submitted_date": "2025-10-24T20:25:25+00:00",
        "pdf_url": "https://arxiv.org/pdf/2510.22011v1",
        "similarity_score": 0.6382156610488892
    },
    {
        "arxiv_id": "2406.03729v2",
        "title": "Enhancing Sign Language Detection through Mediapipe and Convolutional Neural Networks (CNN)",
        "abstract": "This research combines MediaPipe and CNNs for the efficient and accurate interpretation of ASL dataset for the real-time detection of sign language. The system presented here captures and processes hands' gestures in real time. the intended purpose was to create a very easy, accurate, and fast way of entering commands without the necessity of touching something.MediaPipe supports one of the powerful frameworks in real-time hand tracking capabilities for the ability to capture and preprocess hand movements, which increases the accuracy of the gesture recognition system. Actually, the integration of CNN with the MediaPipe results in higher efficiency in using the model of real-time processing.The accuracy achieved by the model on ASL datasets is 99.12\\%.The model was tested using American Sign Language (ASL) datasets. The results were then compared to those of existing methods to evaluate how well it performed, using established evaluation techniques. The system will have applications in the communication, education, and accessibility domains. Making systems such as described in this paper even better will assist people with hearing impairment and make things accessible to them. We tested the recognition and translation performance on an ASL dataset and achieved better accuracy over previous models.It is meant to the research is to identify the characters that American signs recognize using hand images taken from a web camera by based on mediapipe and CNNs",
        "authors": [
            "Aditya Raj Verma",
            "Gagandeep Singh",
            "Karnim Meghwal",
            "Banawath Ramji",
            "Praveen Kumar Dadheech"
        ],
        "submitted_date": "2024-06-06T04:05:12+00:00",
        "pdf_url": "https://arxiv.org/pdf/2406.03729v2",
        "similarity_score": 0.6376264095306396
    },
    {
        "arxiv_id": "2508.16076v2",
        "title": "Prompting with Sign Parameters for Low-resource Sign Language Instruction Generation",
        "abstract": "Sign Language (SL) enables two-way communication for the deaf and hard-of-hearing community, yet many sign languages remain under-resourced in the AI space. Sign Language Instruction Generation (SLIG) produces step-by-step textual instructions that enable non-SL users to imitate and learn SL gestures, promoting two-way interaction. We introduce BdSLIG, the first Bengali SLIG dataset, used to evaluate Vision Language Models (VLMs) (i) on under-resourced SLIG tasks, and (ii) on long-tail visual concepts, as Bengali SL is unlikely to appear in the VLM pre-training data. To enhance zero-shot performance, we introduce Sign Parameter-Infused (SPI) prompting, which integrates standard SL parameters, like hand shape, motion, and orientation, directly into the textual prompts. Subsuming standard sign parameters into the prompt makes the instructions more structured and reproducible than free-form natural text from vanilla prompting. We envision that our work would promote inclusivity and advancement in SL learning systems for the under-resourced communities.",
        "authors": [
            "Md Tariquzzaman",
            "Md Farhan Ishmam",
            "Saiyma Sittul Muna",
            "Md Kamrul Hasan",
            "Hasan Mahmud"
        ],
        "submitted_date": "2025-08-22T04:11:28+00:00",
        "pdf_url": "https://arxiv.org/pdf/2508.16076v2",
        "similarity_score": 0.637363612651825
    },
    {
        "arxiv_id": "2009.14139v1",
        "title": "Score-level Multi Cue Fusion for Sign Language Recognition",
        "abstract": "Sign Languages are expressed through hand and upper body gestures as well as facial expressions. Therefore, Sign Language Recognition (SLR) needs to focus on all such cues. Previous work uses hand-crafted mechanisms or network aggregation to extract the different cue features, to increase SLR performance. This is slow and involves complicated architectures. We propose a more straightforward approach that focuses on training separate cue models specializing on the dominant hand, hands, face, and upper body regions. We compare the performance of 3D Convolutional Neural Network (CNN) models specializing in these regions, combine them through score-level fusion, and use the weighted alternative. Our experimental results have shown the effectiveness of mixed convolutional models. Their fusion yields up to 19% accuracy improvement over the baseline using the full upper body. Furthermore, we include a discussion for fusion settings, which can help future work on Sign Language Translation (SLT).",
        "authors": [
            "Çağrı Gökçe",
            "Oğulcan Özdemir",
            "Ahmet Alp Kındıroğlu",
            "Lale Akarun"
        ],
        "submitted_date": "2020-09-29T16:32:51+00:00",
        "pdf_url": "https://arxiv.org/pdf/2009.14139v1",
        "similarity_score": 0.6354460716247559
    },
    {
        "arxiv_id": "2407.10975v1",
        "title": "Stream State-tying for Sign Language Recognition",
        "abstract": "In this paper, a novel approach to sign language recognition based on state tying in each of data streams is presented. In this framework, it is assumed that hand gesture signal is represented in terms of six synchronous data streams, i.e., the left/right hand position, left/right hand orientation and left/right handshape. This approach offers a very accurate representation of the sign space and keeps the number of parameters reasonably small in favor of a fast decoding. Experiments were carried out for 5177 Chinese signs. The real time isolated recognition rate is 94.8%. For continuous sign recognition, the word correct rate is 91.4%. Keywords: Sign language recognition; Automatic sign language translation; Hand gesture recognition; Hidden Markov models; State-tying; Multimodal user interface; Virtual reality; Man-machine systems.",
        "authors": [
            "Jiyong Ma",
            "Wen Gao",
            "Chunli Wang"
        ],
        "submitted_date": "2024-04-21T23:21:52+00:00",
        "pdf_url": "https://arxiv.org/pdf/2407.10975v1",
        "similarity_score": 0.6352327466011047
    },
    {
        "arxiv_id": "2111.03635v1",
        "title": "BBC-Oxford British Sign Language Dataset",
        "abstract": "In this work, we introduce the BBC-Oxford British Sign Language (BOBSL) dataset, a large-scale video collection of British Sign Language (BSL). BOBSL is an extended and publicly released dataset based on the BSL-1K dataset introduced in previous work. We describe the motivation for the dataset, together with statistics and available annotations. We conduct experiments to provide baselines for the tasks of sign recognition, sign language alignment, and sign language translation. Finally, we describe several strengths and limitations of the data from the perspectives of machine learning and linguistics, note sources of bias present in the dataset, and discuss potential applications of BOBSL in the context of sign language technology. The dataset is available at https://www.robots.ox.ac.uk/~vgg/data/bobsl/.",
        "authors": [
            "Samuel Albanie",
            "Gül Varol",
            "Liliane Momeni",
            "Hannah Bull",
            "Triantafyllos Afouras",
            "Himel Chowdhury",
            "Neil Fox",
            "Bencie Woll",
            "Rob Cooper",
            "Andrew McParland",
            "Andrew Zisserman"
        ],
        "submitted_date": "2021-11-05T17:35:58+00:00",
        "pdf_url": "https://arxiv.org/pdf/2111.03635v1",
        "similarity_score": 0.6346351504325867
    },
    {
        "arxiv_id": "2005.01497v1",
        "title": "Towards A Sign Language Gloss Representation Of Modern Standard Arabic",
        "abstract": "Over 5% of the world's population (466 million people) has disabling hearing loss. 4 million are children. They can be hard of hearing or deaf. Deaf people mostly have profound hearing loss. Which implies very little or no hearing. Over the world, deaf people often communicate using a sign language with gestures of both hands and facial expressions. The sign language is a full-fledged natural language with its own grammar and lexicon. Therefore, there is a need for translation models from and to sign languages. In this work, we are interested in the translation of Modern Standard Arabic(MSAr) into sign language. We generated a gloss representation from MSAr that extracts the features mandatory for the generation of animation signs. Our approach locates the most pertinent features that maintain the meaning of the input Arabic sentence.",
        "authors": [
            "Salma El Anigri",
            "Mohammed Majid Himmi",
            "Abdelhak Mahmoudi"
        ],
        "submitted_date": "2020-05-04T13:56:43+00:00",
        "pdf_url": "https://arxiv.org/pdf/2005.01497v1",
        "similarity_score": 0.6345953345298767
    },
    {
        "arxiv_id": "1908.01341v1",
        "title": "SF-Net: Structured Feature Network for Continuous Sign Language Recognition",
        "abstract": "Continuous sign language recognition (SLR) aims to translate a signing sequence into a sentence. It is very challenging as sign language is rich in vocabulary, while many among them contain similar gestures and motions. Moreover, it is weakly supervised as the alignment of signing glosses is not available. In this paper, we propose Structured Feature Network (SF-Net) to address these challenges by effectively learn multiple levels of semantic information in the data. The proposed SF-Net extracts features in a structured manner and gradually encodes information at the frame level, the gloss level and the sentence level into the feature representation. The proposed SF-Net can be trained end-to-end without the help of other models or pre-training. We tested the proposed SF-Net on two large scale public SLR datasets collected from different continuous SLR scenarios. Results show that the proposed SF-Net clearly outperforms previous sequence level supervision based methods in terms of both accuracy and adaptability.",
        "authors": [
            "Zhaoyang Yang",
            "Zhenmei Shi",
            "Xiaoyong Shen",
            "Yu-Wing Tai"
        ],
        "submitted_date": "2019-08-04T13:34:41+00:00",
        "pdf_url": "https://arxiv.org/pdf/1908.01341v1",
        "similarity_score": 0.6334229707717896
    },
    {
        "arxiv_id": "2204.08747v1",
        "title": "Multi-View Spatial-Temporal Network for Continuous Sign Language Recognition",
        "abstract": "Sign language is a beautiful visual language and is also the primary language used by speaking and hearing-impaired people. However, sign language has many complex expressions, which are difficult for the public to understand and master. Sign language recognition algorithms will significantly facilitate communication between hearing-impaired people and normal people. Traditional continuous sign language recognition often uses a sequence learning method based on Convolutional Neural Network (CNN) and Long Short-Term Memory Network (LSTM). These methods can only learn spatial and temporal features separately, which cannot learn the complex spatial-temporal features of sign language. LSTM is also difficult to learn long-term dependencies. To alleviate these problems, this paper proposes a multi-view spatial-temporal continuous sign language recognition network. The network consists of three parts. The first part is a Multi-View Spatial-Temporal Feature Extractor Network (MSTN), which can directly extract the spatial-temporal features of RGB and skeleton data; the second is a sign language encoder network based on Transformer, which can learn long-term dependencies; the third is a Connectionist Temporal Classification (CTC) decoder network, which is used to predict the whole meaning of the continuous sign language. Our algorithm is tested on two public sign language datasets SLR-100 and PHOENIX-Weather 2014T (RWTH). As a result, our method achieves excellent performance on both datasets. The word error rate on the SLR-100 dataset is 1.9%, and the word error rate on the RWTHPHOENIX-Weather dataset is 22.8%.",
        "authors": [
            "Ronghui Li",
            "Lu Meng"
        ],
        "submitted_date": "2022-04-19T08:43:03+00:00",
        "pdf_url": "https://arxiv.org/pdf/2204.08747v1",
        "similarity_score": 0.6330463886260986
    },
    {
        "arxiv_id": "2402.19118v1",
        "title": "Continuous Sign Language Recognition Based on Motor attention mechanism and frame-level Self-distillation",
        "abstract": "Changes in facial expression, head movement, body movement and gesture movement are remarkable cues in sign language recognition, and most of the current continuous sign language recognition(CSLR) research methods mainly focus on static images in video sequences at the frame-level feature extraction stage, while ignoring the dynamic changes in the images. In this paper, we propose a novel motor attention mechanism to capture the distorted changes in local motion regions during sign language expression, and obtain a dynamic representation of image changes. And for the first time, we apply the self-distillation method to frame-level feature extraction for continuous sign language, which improves the feature expression without increasing the computational resources by self-distilling the features of adjacent stages and using the higher-order features as teachers to guide the lower-order features. The combination of the two constitutes our proposed holistic model of CSLR Based on motor attention mechanism and frame-level Self-Distillation (MAM-FSD), which improves the inference ability and robustness of the model. We conduct experiments on three publicly available datasets, and the experimental results show that our proposed method can effectively extract the sign language motion information in videos, improve the accuracy of CSLR and reach the state-of-the-art level.",
        "authors": [
            "Qidan Zhu",
            "Jing Li",
            "Fei Yuan",
            "Quan Gan"
        ],
        "submitted_date": "2024-02-29T12:52:50+00:00",
        "pdf_url": "https://arxiv.org/pdf/2402.19118v1",
        "similarity_score": 0.6330280303955078
    },
    {
        "arxiv_id": "2106.15989v2",
        "title": "Word-level Sign Language Recognition with Multi-stream Neural Networks Focusing on Local Regions and Skeletal Information",
        "abstract": "Word-level sign language recognition (WSLR) has attracted attention because it is expected to overcome the communication barrier between people with speech impairment and those who can hear. In the WSLR problem, a method designed for action recognition has achieved the state-of-the-art accuracy. Indeed, it sounds reasonable for an action recognition method to perform well on WSLR because sign language is regarded as an action. However, a careful evaluation of the tasks reveals that the tasks of action recognition and WSLR are inherently different. Hence, in this paper, we propose a novel WSLR method that takes into account information specifically useful for the WSLR problem. We realize it as a multi-stream neural network (MSNN), which consist of three streams: 1) base stream, 2) local image stream, and 3) skeleton stream. Each stream is designed to handle different types of information. The base stream deals with quick and detailed movements of the hands and body, the local image stream focuses on handshapes and facial expressions, and the skeleton stream captures the relative positions of the body and both hands. This approach allows us to combine various types of data for more comprehensive gesture analysis. Experimental results on the WLASL and MS-ASL datasets show the effectiveness of the proposed method; it achieved an improvement of approximately 10\\%--15\\% in Top-1 accuracy when compared with conventional methods.",
        "authors": [
            "Mizuki Maruyama",
            "Shrey Singh",
            "Katsufumi Inoue",
            "Partha Pratim Roy",
            "Masakazu Iwamura",
            "Michifumi Yoshioka"
        ],
        "submitted_date": "2021-06-30T11:30:06+00:00",
        "pdf_url": "https://arxiv.org/pdf/2106.15989v2",
        "similarity_score": 0.6316695809364319
    },
    {
        "arxiv_id": "2405.10718v3",
        "title": "SignLLM: Sign Language Production Large Language Models",
        "abstract": "In this paper, we propose SignLLM, a multilingual Sign Language Production (SLP) large language model, which includes two novel multilingual SLP modes MLSF and Prompt2LangGloss that allow sign language gestures generation from query texts input and question-style prompts input respectively. Both modes can use a new RL loss based on reinforcement learning and a new RL module named Priority Learning Channel. These RL components can accelerate the training by enhancing the model's capability to sample high-quality data. To train SignLLM, we introduce Prompt2Sign, a comprehensive multilingual sign language dataset, which builds from public data, including American Sign Language (ASL) and seven others. This dataset standardizes information by extracting pose information from sign language videos into a unified compressed format. We extensively evaluate SignLLM, demonstrating that our model achieves state-of-the-art performance on SLP tasks across eight sign languages.",
        "authors": [
            "Sen Fang",
            "Chen Chen",
            "Lei Wang",
            "Ce Zheng",
            "Chunyu Sui",
            "Yapeng Tian"
        ],
        "submitted_date": "2024-05-17T12:01:43+00:00",
        "pdf_url": "https://arxiv.org/pdf/2405.10718v3",
        "similarity_score": 0.6289441585540771
    },
    {
        "arxiv_id": "1906.02851v1",
        "title": "Recognizing American Sign Language Manual Signs from RGB-D Videos",
        "abstract": "In this paper, we propose a 3D Convolutional Neural Network (3DCNN) based multi-stream framework to recognize American Sign Language (ASL) manual signs (consisting of movements of the hands, as well as non-manual face movements in some cases) in real-time from RGB-D videos, by fusing multimodality features including hand gestures, facial expressions, and body poses from multi-channels (RGB, depth, motion, and skeleton joints). To learn the overall temporal dynamics in a video, a proxy video is generated by selecting a subset of frames for each video which are then used to train the proposed 3DCNN model. We collect a new ASL dataset, ASL-100-RGBD, which contains 42 RGB-D videos captured by a Microsoft Kinect V2 camera, each of 100 ASL manual signs, including RGB channel, depth maps, skeleton joints, face features, and HDface. The dataset is fully annotated for each semantic region (i.e. the time duration of each word that the human signer performs). Our proposed method achieves 92.88 accuracy for recognizing 100 ASL words in our newly collected ASL-100-RGBD dataset. The effectiveness of our framework for recognizing hand gestures from RGB-D videos is further demonstrated on the Chalearn IsoGD dataset and achieves 76 accuracy which is 5.51 higher than the state-of-the-art work in terms of average fusion by using only 5 channels instead of 12 channels in the previous work.",
        "authors": [
            "Longlong Jing",
            "Elahe Vahdani",
            "Matt Huenerfauth",
            "Yingli Tian"
        ],
        "submitted_date": "2019-06-07T00:56:11+00:00",
        "pdf_url": "https://arxiv.org/pdf/1906.02851v1",
        "similarity_score": 0.6282238960266113
    },
    {
        "arxiv_id": "2409.01901v1",
        "title": "3D-LEX v1.0: 3D Lexicons for American Sign Language and Sign Language of the Netherlands",
        "abstract": "In this work, we present an efficient approach for capturing sign language in 3D, introduce the 3D-LEX v1.0 dataset, and detail a method for semi-automatic annotation of phonetic properties. Our procedure integrates three motion capture techniques encompassing high-resolution 3D poses, 3D handshapes, and depth-aware facial features, and attains an average sampling rate of one sign every 10 seconds. This includes the time for presenting a sign example, performing and recording the sign, and archiving the capture. The 3D-LEX dataset includes 1,000 signs from American Sign Language and an additional 1,000 signs from the Sign Language of the Netherlands. We showcase the dataset utility by presenting a simple method for generating handshape annotations directly from 3D-LEX. We produce handshape labels for 1,000 signs from American Sign Language and evaluate the labels in a sign recognition task. The labels enhance gloss recognition accuracy by 5% over using no handshape annotations, and by 1% over expert annotations. Our motion capture data supports in-depth analysis of sign features and facilitates the generation of 2D projections from any viewpoint. The 3D-LEX collection has been aligned with existing sign language benchmarks and linguistic resources, to support studies in 3D-aware sign language processing.",
        "authors": [
            "Oline Ranum",
            "Gomer Otterspeer",
            "Jari I. Andersen",
            "Robert G. Belleman",
            "Floris Roelofsen"
        ],
        "submitted_date": "2024-09-03T13:44:56+00:00",
        "pdf_url": "https://arxiv.org/pdf/2409.01901v1",
        "similarity_score": 0.6281780004501343
    },
    {
        "arxiv_id": "2511.05772v1",
        "title": "Sign language recognition from skeletal data using graph and recurrent neural networks",
        "abstract": "This work presents an approach for recognizing isolated sign language gestures using skeleton-based pose data extracted from video sequences. A Graph-GRU temporal network is proposed to model both spatial and temporal dependencies between frames, enabling accurate classification. The model is trained and evaluated on the AUTSL (Ankara university Turkish sign language) dataset, achieving high accuracy. Experimental results demonstrate the effectiveness of integrating graph-based spatial representations with temporal modeling, providing a scalable framework for sign language recognition. The results of this approach highlight the potential of pose-driven methods for sign language understanding.",
        "authors": [
            "B. Mederos",
            "J. Mejía",
            "A. Medina-Reyes",
            "Y. Espinosa-Almeyda",
            "J. D. Díaz-Roman",
            "I. Rodríguez-Mederos",
            "M. Mejía-Carreon",
            "F. Gonzalez-Lopez"
        ],
        "submitted_date": "2025-11-08T00:04:42+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.05772v1",
        "similarity_score": 0.6281049847602844
    },
    {
        "arxiv_id": "2508.04049v1",
        "title": "Motion is the Choreographer: Learning Latent Pose Dynamics for Seamless Sign Language Generation",
        "abstract": "Sign language video generation requires producing natural signing motions with realistic appearances under precise semantic control, yet faces two critical challenges: excessive signer-specific data requirements and poor generalization. We propose a new paradigm for sign language video generation that decouples motion semantics from signer identity through a two-phase synthesis framework. First, we construct a signer-independent multimodal motion lexicon, where each gloss is stored as identity-agnostic pose, gesture, and 3D mesh sequences, requiring only one recording per sign. This compact representation enables our second key innovation: a discrete-to-continuous motion synthesis stage that transforms retrieved gloss sequences into temporally coherent motion trajectories, followed by identity-aware neural rendering to produce photorealistic videos of arbitrary signers. Unlike prior work constrained by signer-specific datasets, our method treats motion as a first-class citizen: the learned latent pose dynamics serve as a portable \"choreography layer\" that can be visually realized through different human appearances. Extensive experiments demonstrate that disentangling motion from identity is not just viable but advantageous - enabling both high-quality synthesis and unprecedented flexibility in signer personalization.",
        "authors": [
            "Jiayi He",
            "Xu Wang",
            "Shengeng Tang",
            "Yaxiong Wang",
            "Lechao Cheng",
            "Dan Guo"
        ],
        "submitted_date": "2025-08-06T03:23:10+00:00",
        "pdf_url": "https://arxiv.org/pdf/2508.04049v1",
        "similarity_score": 0.6273025870323181
    },
    {
        "arxiv_id": "2412.13609v2",
        "title": "Sign-IDD: Iconicity Disentangled Diffusion for Sign Language Production",
        "abstract": "Sign Language Production (SLP) aims to generate semantically consistent sign videos from textual statements, where the conversion from textual glosses to sign poses (G2P) is a crucial step. Existing G2P methods typically treat sign poses as discrete three-dimensional coordinates and directly fit them, which overlooks the relative positional relationships among joints. To this end, we provide a new perspective, constraining joint associations and gesture details by modeling the limb bones to improve the accuracy and naturalness of the generated poses. In this work, we propose a pioneering iconicity disentangled diffusion framework, termed Sign-IDD, specifically designed for SLP. Sign-IDD incorporates a novel Iconicity Disentanglement (ID) module to bridge the gap between relative positions among joints. The ID module disentangles the conventional 3D joint representation into a 4D bone representation, comprising the 3D spatial direction vector and 1D spatial distance vector between adjacent joints. Additionally, an Attribute Controllable Diffusion (ACD) module is introduced to further constrain joint associations, in which the attribute separation layer aims to separate the bone direction and length attributes, and the attribute control layer is designed to guide the pose generation by leveraging the above attributes. The ACD module utilizes the gloss embeddings as semantic conditions and finally generates sign poses from noise embeddings. Extensive experiments on PHOENIX14T and USTC-CSL datasets validate the effectiveness of our method. The code is available at: https://github.com/NaVi-start/Sign-IDD.",
        "authors": [
            "Shengeng Tang",
            "Jiayi He",
            "Dan Guo",
            "Yanyan Wei",
            "Feng Li",
            "Richang Hong"
        ],
        "submitted_date": "2024-12-18T08:36:35+00:00",
        "pdf_url": "https://arxiv.org/pdf/2412.13609v2",
        "similarity_score": 0.6265214681625366
    },
    {
        "arxiv_id": "2205.12261v1",
        "title": "Action Recognition for American Sign Language",
        "abstract": "In this research, we present our findings to recognize American Sign Language from series of hand gestures. While most researches in literature focus only on static handshapes, our work target dynamic hand gestures. Since dynamic signs dataset are very few, we collect an initial dataset of 150 videos for 10 signs and an extension of 225 videos for 15 signs. We apply transfer learning models in combination with deep neural networks and background subtraction for videos in different temporal settings. Our primarily results show that we can get an accuracy of $0.86$ and $0.71$ using DenseNet201, LSTM with video sequence of 12 frames accordingly.",
        "authors": [
            "Nguyen Huu Phong",
            "Bernardete Ribeiro"
        ],
        "submitted_date": "2022-05-20T23:53:19+00:00",
        "pdf_url": "https://arxiv.org/pdf/2205.12261v1",
        "similarity_score": 0.6263043284416199
    },
    {
        "arxiv_id": "2407.12842v1",
        "title": "MS2SL: Multimodal Spoken Data-Driven Continuous Sign Language Production",
        "abstract": "Sign language understanding has made significant strides; however, there is still no viable solution for generating sign sequences directly from entire spoken content, e.g., text or speech. In this paper, we propose a unified framework for continuous sign language production, easing communication between sign and non-sign language users. In particular, a sequence diffusion model, utilizing embeddings extracted from text or speech, is crafted to generate sign predictions step by step. Moreover, by creating a joint embedding space for text, audio, and sign, we bind these modalities and leverage the semantic consistency among them to provide informative feedback for the model training. This embedding-consistency learning strategy minimizes the reliance on sign triplets and ensures continuous model refinement, even with a missing audio modality. Experiments on How2Sign and PHOENIX14T datasets demonstrate that our model achieves competitive performance in sign language production.",
        "authors": [
            "Jian Ma",
            "Wenguan Wang",
            "Yi Yang",
            "Feng Zheng"
        ],
        "submitted_date": "2024-07-04T13:53:50+00:00",
        "pdf_url": "https://arxiv.org/pdf/2407.12842v1",
        "similarity_score": 0.6251413822174072
    },
    {
        "arxiv_id": "2009.03988v1",
        "title": "A new architecture for hand-worn Sign language to Speech translator",
        "abstract": "People with speech and hearing impairments often rely on sign language to communicate with others but most of the general population cannot understand sign language and sign language itself is a difficult language to learn, so there is a definite need for technologies to translate sign language to speech. In this paper, we describe the design and implementation of Smart glove, a hand-worn hardware device capable of translating American Sign Language gestures into English speech by tracking the finger's orientation, gestures and hand motion. It uses hardware sensors like Flex, Accelerometer and gyroscope and intelligent software to capture and translate the gestures into speech. This paper explains the translation of both Alphabet and Word gestures. New approaches and algorithms are proposed and implemented to address hardware-dependent issues in existing glove based designs. The whole device is designed to be modular with distributed processing units to encourage modular enhancement, reducing complexity, and interrelation between subsystems.Decision Trees are used in gesture recognition and error correction. We hope that the henceforth mentioned design and architecture would be the basis for the advancement in research related to sensor-based sign language translation along with research for smart glove and cybernetic accessories.",
        "authors": [
            "Sai Charan Bodda",
            "Palki Gupta",
            "Gaurav Joshi",
            "Ayush Chaturvedi"
        ],
        "submitted_date": "2020-09-08T20:37:34+00:00",
        "pdf_url": "https://arxiv.org/pdf/2009.03988v1",
        "similarity_score": 0.6250671148300171
    },
    {
        "arxiv_id": "2412.16497v1",
        "title": "Real-time Bangla Sign Language Translator",
        "abstract": "The human body communicates through various meaningful gestures, with sign language using hands being a prominent example. Bangla Sign Language Translation (BSLT) aims to bridge communication gaps for the deaf and mute community. Our approach involves using Mediapipe Holistic to gather key points, LSTM architecture for data training, and Computer Vision for realtime sign language detection with an accuracy of 94%. Keywords=Recurrent Neural Network, LSTM, Computer Vision, Bangla font.",
        "authors": [
            "Rotan Hawlader Pranto",
            "Shahnewaz Siddique"
        ],
        "submitted_date": "2024-12-21T05:56:32+00:00",
        "pdf_url": "https://arxiv.org/pdf/2412.16497v1",
        "similarity_score": 0.6240614652633667
    },
    {
        "arxiv_id": "2110.06161v1",
        "title": "Sign Language Recognition via Skeleton-Aware Multi-Model Ensemble",
        "abstract": "Sign language is commonly used by deaf or mute people to communicate but requires extensive effort to master. It is usually performed with the fast yet delicate movement of hand gestures, body posture, and even facial expressions. Current Sign Language Recognition (SLR) methods usually extract features via deep neural networks and suffer overfitting due to limited and noisy data. Recently, skeleton-based action recognition has attracted increasing attention due to its subject-invariant and background-invariant nature, whereas skeleton-based SLR is still under exploration due to the lack of hand annotations. Some researchers have tried to use off-line hand pose trackers to obtain hand keypoints and aid in recognizing sign language via recurrent neural networks. Nevertheless, none of them outperforms RGB-based approaches yet. To this end, we propose a novel Skeleton Aware Multi-modal Framework with a Global Ensemble Model (GEM) for isolated SLR (SAM-SLR-v2) to learn and fuse multi-modal feature representations towards a higher recognition rate. Specifically, we propose a Sign Language Graph Convolution Network (SL-GCN) to model the embedded dynamics of skeleton keypoints and a Separable Spatial-Temporal Convolution Network (SSTCN) to exploit skeleton features. The skeleton-based predictions are fused with other RGB and depth based modalities by the proposed late-fusion GEM to provide global information and make a faithful SLR prediction. Experiments on three isolated SLR datasets demonstrate that our proposed SAM-SLR-v2 framework is exceedingly effective and achieves state-of-the-art performance with significant margins. Our code will be available at https://github.com/jackyjsy/SAM-SLR-v2",
        "authors": [
            "Songyao Jiang",
            "Bin Sun",
            "Lichen Wang",
            "Yue Bai",
            "Kunpeng Li",
            "Yun Fu"
        ],
        "submitted_date": "2021-10-12T16:57:18+00:00",
        "pdf_url": "https://arxiv.org/pdf/2110.06161v1",
        "similarity_score": 0.6239811182022095
    },
    {
        "arxiv_id": "2211.12723v3",
        "title": "A Classification Model Utilizing Facial Landmark Tracking to Determine Sentence Types for American Sign Language Recognition",
        "abstract": "The deaf and hard of hearing community relies on American Sign Language (ASL) as their primary mode of communication, but communication with others who do not know ASL can be difficult, especially during emergencies where no interpreter is available. As an effort to alleviate this problem, research in computer vision based real time ASL interpreting models is ongoing. However, most of these models are hand shape (gesture) based and lack the integration of facial cues, which are crucial in ASL to convey tone and distinguish sentence types. Thus, the integration of facial cues in computer vision based ASL interpreting models has the potential to improve performance and reliability. In this paper, we introduce a simple, computationally efficient facial expression based classification model that can be used to improve ASL interpreting models. This model utilizes the relative angles of facial landmarks with principal component analysis and a Random Forest Classification tree model to classify frames taken from videos of ASL users signing a complete sentence. The model classifies the frames as statements or assertions. The model was able to achieve an accuracy of 86.5%.",
        "authors": [
            "Janice Nguyen",
            "Y. Curtis Wang"
        ],
        "submitted_date": "2022-11-23T06:09:03+00:00",
        "pdf_url": "https://arxiv.org/pdf/2211.12723v3",
        "similarity_score": 0.6208888292312622
    },
    {
        "arxiv_id": "2110.05877v1",
        "title": "OpenHands: Making Sign Language Recognition Accessible with Pose-based Pretrained Models across Languages",
        "abstract": "AI technologies for Natural Languages have made tremendous progress recently. However, commensurate progress has not been made on Sign Languages, in particular, in recognizing signs as individual words or as complete sentences. We introduce OpenHands, a library where we take four key ideas from the NLP community for low-resource languages and apply them to sign languages for word-level recognition. First, we propose using pose extracted through pretrained models as the standard modality of data to reduce training time and enable efficient inference, and we release standardized pose datasets for 6 different sign languages - American, Argentinian, Chinese, Greek, Indian, and Turkish. Second, we train and release checkpoints of 4 pose-based isolated sign language recognition models across all 6 languages, providing baselines and ready checkpoints for deployment. Third, to address the lack of labelled data, we propose self-supervised pretraining on unlabelled data. We curate and release the largest pose-based pretraining dataset on Indian Sign Language (Indian-SL). Fourth, we compare different pretraining strategies and for the first time establish that pretraining is effective for sign language recognition by demonstrating (a) improved fine-tuning performance especially in low-resource settings, and (b) high crosslingual transfer from Indian-SL to few other sign languages. We open-source all models and datasets in OpenHands with a hope that it makes research in sign languages more accessible, available here at https://github.com/AI4Bharat/OpenHands .",
        "authors": [
            "Prem Selvaraj",
            "Gokul NC",
            "Pratyush Kumar",
            "Mitesh Khapra"
        ],
        "submitted_date": "2021-10-12T10:33:02+00:00",
        "pdf_url": "https://arxiv.org/pdf/2110.05877v1",
        "similarity_score": 0.6208170056343079
    },
    {
        "arxiv_id": "2305.05296v3",
        "title": "Mediapipe and CNNs for Real-Time ASL Gesture Recognition",
        "abstract": "This research paper describes a realtime system for identifying American Sign Language (ASL) movements that employs modern computer vision and machine learning approaches. The suggested method makes use of the Mediapipe library for feature extraction and a Convolutional Neural Network (CNN) for ASL gesture classification. The testing results show that the suggested system can detect all ASL alphabets with an accuracy of 99.95%, indicating its potential for use in communication devices for people with hearing impairments. The proposed approach can also be applied to additional sign languages with similar hand motions, potentially increasing the quality of life for people with hearing loss. Overall, the study demonstrates the effectiveness of using Mediapipe and CNN for real-time sign language recognition, making a significant contribution to the field of computer vision and machine learning.",
        "authors": [
            "Rupesh Kumar",
            "Ashutosh Bajpai",
            "Ayush Sinha"
        ],
        "submitted_date": "2023-05-09T09:35:45+00:00",
        "pdf_url": "https://arxiv.org/pdf/2305.05296v3",
        "similarity_score": 0.6205417513847351
    },
    {
        "arxiv_id": "2110.05382v1",
        "title": "SignBERT: Pre-Training of Hand-Model-Aware Representation for Sign Language Recognition",
        "abstract": "Hand gesture serves as a critical role in sign language. Current deep-learning-based sign language recognition (SLR) methods may suffer insufficient interpretability and overfitting due to limited sign data sources. In this paper, we introduce the first self-supervised pre-trainable SignBERT with incorporated hand prior for SLR. SignBERT views the hand pose as a visual token, which is derived from an off-the-shelf pose extractor. The visual tokens are then embedded with gesture state, temporal and hand chirality information. To take full advantage of available sign data sources, SignBERT first performs self-supervised pre-training by masking and reconstructing visual tokens. Jointly with several mask modeling strategies, we attempt to incorporate hand prior in a model-aware method to better model hierarchical context over the hand sequence. Then with the prediction head added, SignBERT is fine-tuned to perform the downstream SLR task. To validate the effectiveness of our method on SLR, we perform extensive experiments on four public benchmark datasets, i.e., NMFs-CSL, SLR500, MSASL and WLASL. Experiment results demonstrate the effectiveness of both self-supervised learning and imported hand prior. Furthermore, we achieve state-of-the-art performance on all benchmarks with a notable gain.",
        "authors": [
            "Hezhen Hu",
            "Weichao Zhao",
            "Wengang Zhou",
            "Yuechen Wang",
            "Houqiang Li"
        ],
        "submitted_date": "2021-10-11T16:18:09+00:00",
        "pdf_url": "https://arxiv.org/pdf/2110.05382v1",
        "similarity_score": 0.6198621392250061
    },
    {
        "arxiv_id": "2107.13647v1",
        "title": "Egyptian Sign Language Recognition Using CNN and LSTM",
        "abstract": "Sign language is a set of gestures that deaf people use to communicate. Unfortunately, normal people don't understand it, which creates a communication gap that needs to be filled. Because of the variations in (Egyptian Sign Language) ESL from one region to another, ESL provides a challenging research problem. In this work, we are providing applied research with its video-based Egyptian sign language recognition system that serves the local community of deaf people in Egypt, with a moderate and reasonable accuracy. We present a computer vision system with two different neural networks architectures. The first is a Convolutional Neural Network (CNN) for extracting spatial features. The CNN model was retrained on the inception mod. The second architecture is a CNN followed by a Long Short-Term Memory (LSTM) for extracting both spatial and temporal features. The two models achieved an accuracy of 90% and 72%, respectively. We examined the power of these two architectures to distinguish between 9 common words (with similar signs) among some deaf people community in Egypt.",
        "authors": [
            "Ahmed Elhagry",
            "Rawan Glalal Elrayes"
        ],
        "submitted_date": "2021-07-28T21:33:35+00:00",
        "pdf_url": "https://arxiv.org/pdf/2107.13647v1",
        "similarity_score": 0.618850588798523
    },
    {
        "arxiv_id": "2005.00253v1",
        "title": "Recognizing American Sign Language Nonmanual Signal Grammar Errors in Continuous Videos",
        "abstract": "As part of the development of an educational tool that can help students achieve fluency in American Sign Language (ASL) through independent and interactive practice with immediate feedback, this paper introduces a near real-time system to recognize grammatical errors in continuous signing videos without necessarily identifying the entire sequence of signs. Our system automatically recognizes if performance of ASL sentences contains grammatical errors made by ASL students. We first recognize the ASL grammatical elements including both manual gestures and nonmanual signals independently from multiple modalities (i.e. hand gestures, facial expressions, and head movements) by 3D-ResNet networks. Then the temporal boundaries of grammatical elements from different modalities are examined to detect ASL grammatical mistakes by using a sliding window-based approach. We have collected a dataset of continuous sign language, ASL-HW-RGBD, covering different aspects of ASL grammars for training and testing. Our system is able to recognize grammatical elements on ASL-HW-RGBD from manual gestures, facial expressions, and head movements and successfully detect 8 ASL grammatical mistakes.",
        "authors": [
            "Elahe Vahdani",
            "Longlong Jing",
            "Yingli Tian",
            "Matt Huenerfauth"
        ],
        "submitted_date": "2020-05-01T07:25:07+00:00",
        "pdf_url": "https://arxiv.org/pdf/2005.00253v1",
        "similarity_score": 0.6179242134094238
    },
    {
        "arxiv_id": "2305.04868v1",
        "title": "SignBERT+: Hand-model-aware Self-supervised Pre-training for Sign Language Understanding",
        "abstract": "Hand gesture serves as a crucial role during the expression of sign language. Current deep learning based methods for sign language understanding (SLU) are prone to over-fitting due to insufficient sign data resource and suffer limited interpretability. In this paper, we propose the first self-supervised pre-trainable SignBERT+ framework with model-aware hand prior incorporated. In our framework, the hand pose is regarded as a visual token, which is derived from an off-the-shelf detector. Each visual token is embedded with gesture state and spatial-temporal position encoding. To take full advantage of current sign data resource, we first perform self-supervised learning to model its statistics. To this end, we design multi-level masked modeling strategies (joint, frame and clip) to mimic common failure detection cases. Jointly with these masked modeling strategies, we incorporate model-aware hand prior to better capture hierarchical context over the sequence. After the pre-training, we carefully design simple yet effective prediction heads for downstream tasks. To validate the effectiveness of our framework, we perform extensive experiments on three main SLU tasks, involving isolated and continuous sign language recognition (SLR), and sign language translation (SLT). Experimental results demonstrate the effectiveness of our method, achieving new state-of-the-art performance with a notable gain.",
        "authors": [
            "Hezhen Hu",
            "Weichao Zhao",
            "Wengang Zhou",
            "Houqiang Li"
        ],
        "submitted_date": "2023-05-08T17:16:38+00:00",
        "pdf_url": "https://arxiv.org/pdf/2305.04868v1",
        "similarity_score": 0.6156026124954224
    },
    {
        "arxiv_id": "1607.06356v1",
        "title": "Reasoning about Body-Parts Relations for Sign Language Recognition",
        "abstract": "Over the years, hand gesture recognition has been mostly addressed considering hand trajectories in isolation. However, in most sign languages, hand gestures are defined on a particular context (body region). We propose a pipeline to perform sign language recognition which models hand movements in the context of other parts of the body captured in the 3D space using the MS Kinect sensor. In addition, we perform sign recognition based on the different hand postures that occur during a sign. Our experiments show that considering different body parts brings improved performance when compared to other methods which only consider global hand trajectories. Finally, we demonstrate that the combination of hand postures features with hand gestures features helps to improve the prediction of a given sign.",
        "authors": [
            "Marc Martínez-Camarena",
            "Jose Oramas",
            "Mario Montagud-Climent",
            "Tinne Tuytelaars"
        ],
        "submitted_date": "2016-07-21T15:10:41+00:00",
        "pdf_url": "https://arxiv.org/pdf/1607.06356v1",
        "similarity_score": 0.6153360605239868
    },
    {
        "arxiv_id": "2411.16765v3",
        "title": "SHuBERT: Self-Supervised Sign Language Representation Learning via Multi-Stream Cluster Prediction",
        "abstract": "Sign language processing has traditionally relied on task-specific models, limiting the potential for transfer learning across tasks. Pre-training methods for sign language have typically focused on either supervised pre-training, which cannot take advantage of unlabeled data, or context-independent (frame or video segment) representations, which ignore the effects of relationships across time in sign language. We introduce SHuBERT (Sign Hidden-Unit BERT), a self-supervised contextual representation model learned from approximately 1,000 hours of American Sign Language video. SHuBERT adapts masked token prediction objectives to multi-stream visual sign language input, learning to predict multiple targets corresponding to clustered hand, face, and body pose streams. SHuBERT achieves state-of-the-art performance across multiple tasks including sign language translation, isolated sign language recognition, and fingerspelling detection.",
        "authors": [
            "Shester Gueuwou",
            "Xiaodan Du",
            "Greg Shakhnarovich",
            "Karen Livescu",
            "Alexander H. Liu"
        ],
        "submitted_date": "2024-11-25T03:13:08+00:00",
        "pdf_url": "https://arxiv.org/pdf/2411.16765v3",
        "similarity_score": 0.6145999431610107
    },
    {
        "arxiv_id": "2504.16640v1",
        "title": "SSLR: A Semi-Supervised Learning Method for Isolated Sign Language Recognition",
        "abstract": "Sign language is the primary communication language for people with disabling hearing loss. Sign language recognition (SLR) systems aim to recognize sign gestures and translate them into spoken language. One of the main challenges in SLR is the scarcity of annotated datasets. To address this issue, we propose a semi-supervised learning (SSL) approach for SLR (SSLR), employing a pseudo-label method to annotate unlabeled samples. The sign gestures are represented using pose information that encodes the signer's skeletal joint points. This information is used as input for the Transformer backbone model utilized in the proposed approach. To demonstrate the learning capabilities of SSL across various labeled data sizes, several experiments were conducted using different percentages of labeled data with varying numbers of classes. The performance of the SSL approach was compared with a fully supervised learning-based model on the WLASL-100 dataset. The obtained results of the SSL model outperformed the supervised learning-based model with less labeled data in many cases.",
        "authors": [
            "Hasan Algafri",
            "Hamzah Luqman",
            "Sarah Alyami",
            "Issam Laradji"
        ],
        "submitted_date": "2025-04-23T11:59:52+00:00",
        "pdf_url": "https://arxiv.org/pdf/2504.16640v1",
        "similarity_score": 0.6115591526031494
    },
    {
        "arxiv_id": "2004.11836v1",
        "title": "Convolutional Neural Network Array for Sign Language Recognition using Wearable IMUs",
        "abstract": "Advancements in gesture recognition algorithms have led to a significant growth in sign language translation. By making use of efficient intelligent models, signs can be recognized with precision. The proposed work presents a novel one-dimensional Convolutional Neural Network (CNN) array architecture for recognition of signs from the Indian sign language using signals recorded from a custom designed wearable IMU device. The IMU device makes use of tri-axial accelerometer and gyroscope. The signals recorded using the IMU device are segregated on the basis of their context, such as whether they correspond to signing for a general sentence or an interrogative sentence. The array comprises of two individual CNNs, one classifying the general sentences and the other classifying the interrogative sentence. Performances of individual CNNs in the array architecture are compared to that of a conventional CNN classifying the unsegregated dataset. Peak classification accuracies of 94.20% for general sentences and 95.00% for interrogative sentences achieved with the proposed CNN array in comparison to 93.50% for conventional CNN assert the suitability of the proposed approach.",
        "authors": [
            "Karush Suri",
            "Rinki Gupta"
        ],
        "submitted_date": "2020-04-21T23:11:04+00:00",
        "pdf_url": "https://arxiv.org/pdf/2004.11836v1",
        "similarity_score": 0.6112044453620911
    },
    {
        "arxiv_id": "1701.01875v1",
        "title": "Sign Language Recognition Using Temporal Classification",
        "abstract": "Devices like the Myo armband available in the market today enable us to collect data about the position of a user's hands and fingers over time. We can use these technologies for sign language translation since each sign is roughly a combination of gestures across time. In this work, we utilize a dataset collected by a group at the University of South Wales, which contains parameters, such as hand position, hand rotation, and finger bend, for 95 unique signs. For each input stream representing a sign, we predict which sign class this stream falls into. We begin by implementing baseline SVM and logistic regression models, which perform reasonably well on high quality data. Lower quality data requires a more sophisticated approach, so we explore different methods in temporal classification, including long short term memory architectures and sequential pattern mining methods.",
        "authors": [
            "Hardie Cate",
            "Fahim Dalvi",
            "Zeshan Hussain"
        ],
        "submitted_date": "2017-01-07T20:09:52+00:00",
        "pdf_url": "https://arxiv.org/pdf/1701.01875v1",
        "similarity_score": 0.6107635498046875
    },
    {
        "arxiv_id": "2407.11144v1",
        "title": "YouTube-SL-25: A Large-Scale, Open-Domain Multilingual Sign Language Parallel Corpus",
        "abstract": "Even for better-studied sign languages like American Sign Language (ASL), data is the bottleneck for machine learning research. The situation is worse yet for the many other sign languages used by Deaf/Hard of Hearing communities around the world. In this paper, we present YouTube-SL-25, a large-scale, open-domain multilingual corpus of sign language videos with seemingly well-aligned captions drawn from YouTube. With >3000 hours of videos across >25 sign languages, YouTube-SL-25 is a) >3x the size of YouTube-ASL, b) the largest parallel sign language dataset to date, and c) the first or largest parallel dataset for many of its component languages. We provide baselines for sign-to-text tasks using a unified multilingual multitask model based on T5 and report scores on benchmarks across 4 sign languages. The results demonstrate that multilingual transfer benefits both higher- and lower-resource sign languages within YouTube-SL-25.",
        "authors": [
            "Garrett Tanzer",
            "Biao Zhang"
        ],
        "submitted_date": "2024-07-15T18:08:34+00:00",
        "pdf_url": "https://arxiv.org/pdf/2407.11144v1",
        "similarity_score": 0.6094461679458618
    },
    {
        "arxiv_id": "2012.05698v1",
        "title": "Independent Sign Language Recognition with 3D Body, Hands, and Face Reconstruction",
        "abstract": "Independent Sign Language Recognition is a complex visual recognition problem that combines several challenging tasks of Computer Vision due to the necessity to exploit and fuse information from hand gestures, body features and facial expressions. While many state-of-the-art works have managed to deeply elaborate on these features independently, to the best of our knowledge, no work has adequately combined all three information channels to efficiently recognize Sign Language. In this work, we employ SMPL-X, a contemporary parametric model that enables joint extraction of 3D body shape, face and hands information from a single image. We use this holistic 3D reconstruction for SLR, demonstrating that it leads to higher accuracy than recognition from raw RGB images and their optical flow fed into the state-of-the-art I3D-type network for 3D action recognition and from 2D Openpose skeletons fed into a Recurrent Neural Network. Finally, a set of experiments on the body, face and hand features showed that neglecting any of these, significantly reduces the classification accuracy, proving the importance of jointly modeling body shape, facial expression and hand pose for Sign Language Recognition.",
        "authors": [
            "Agelos Kratimenos",
            "Georgios Pavlakos",
            "Petros Maragos"
        ],
        "submitted_date": "2020-11-24T23:50:26+00:00",
        "pdf_url": "https://arxiv.org/pdf/2012.05698v1",
        "similarity_score": 0.6089726686477661
    },
    {
        "arxiv_id": "2303.12793v1",
        "title": "CiCo: Domain-Aware Sign Language Retrieval via Cross-Lingual Contrastive Learning",
        "abstract": "This work focuses on sign language retrieval-a recently proposed task for sign language understanding. Sign language retrieval consists of two sub-tasks: text-to-sign-video (T2V) retrieval and sign-video-to-text (V2T) retrieval. Different from traditional video-text retrieval, sign language videos, not only contain visual signals but also carry abundant semantic meanings by themselves due to the fact that sign languages are also natural languages. Considering this character, we formulate sign language retrieval as a cross-lingual retrieval problem as well as a video-text retrieval task. Concretely, we take into account the linguistic properties of both sign languages and natural languages, and simultaneously identify the fine-grained cross-lingual (i.e., sign-to-word) mappings while contrasting the texts and the sign videos in a joint embedding space. This process is termed as cross-lingual contrastive learning. Another challenge is raised by the data scarcity issue-sign language datasets are orders of magnitude smaller in scale than that of speech recognition. We alleviate this issue by adopting a domain-agnostic sign encoder pre-trained on large-scale sign videos into the target domain via pseudo-labeling. Our framework, termed as domain-aware sign language retrieval via Cross-lingual Contrastive learning or CiCo for short, outperforms the pioneering method by large margins on various datasets, e.g., +22.4 T2V and +28.0 V2T R@1 improvements on How2Sign dataset, and +13.7 T2V and +17.1 V2T R@1 improvements on PHOENIX-2014T dataset. Code and models are available at: https://github.com/FangyunWei/SLRT.",
        "authors": [
            "Yiting Cheng",
            "Fangyun Wei",
            "Jianmin Bao",
            "Dong Chen",
            "Wenqiang Zhang"
        ],
        "submitted_date": "2023-03-22T17:59:59+00:00",
        "pdf_url": "https://arxiv.org/pdf/2303.12793v1",
        "similarity_score": 0.608676552772522
    },
    {
        "arxiv_id": "2505.24266v2",
        "title": "SignBot: Learning Human-to-Humanoid Sign Language Interaction",
        "abstract": "Sign language is a natural and visual form of language that uses movements and expressions to convey meaning, serving as a crucial means of communication for individuals who are deaf or hard-of-hearing (DHH). However, the number of people proficient in sign language remains limited, highlighting the need for technological advancements to bridge communication gaps and foster interactions with minorities. Based on recent advancements in embodied humanoid robots, we propose SignBot, a novel framework for human-robot sign language interaction. SignBot integrates a cerebellum-inspired motion control component and a cerebral-oriented module for comprehension and interaction. Specifically, SignBot consists of: 1) Motion Retargeting, which converts human sign language datasets into robot-compatible kinematics; 2) Motion Control, which leverages a learning-based paradigm to develop a robust humanoid control policy for tracking sign language gestures; and 3) Generative Interaction, which incorporates translator, responser, and generator of sign language, thereby enabling natural and effective communication between robots and humans. Simulation and real-world experimental results demonstrate that SignBot can effectively facilitate human-robot interaction and perform sign language motions with diverse robots and datasets. SignBot represents a significant advancement in automatic sign language interaction on embodied humanoid robot platforms, providing a promising solution to improve communication accessibility for the DHH community.",
        "authors": [
            "Guanren Qiao",
            "Sixu Lin",
            "Ronglai Zuo",
            "Zhizheng Wu",
            "Kui Jia",
            "Guiliang Liu"
        ],
        "submitted_date": "2025-05-30T06:42:09+00:00",
        "pdf_url": "https://arxiv.org/pdf/2505.24266v2",
        "similarity_score": 0.6085280179977417
    },
    {
        "arxiv_id": "2308.15402v1",
        "title": "Bornil: An open-source sign language data crowdsourcing platform for AI enabled dialect-agnostic communication",
        "abstract": "The absence of annotated sign language datasets has hindered the development of sign language recognition and translation technologies. In this paper, we introduce Bornil; a crowdsource-friendly, multilingual sign language data collection, annotation, and validation platform. Bornil allows users to record sign language gestures and lets annotators perform sentence and gloss-level annotation. It also allows validators to make sure of the quality of both the recorded videos and the annotations through manual validation to develop high-quality datasets for deep learning-based Automatic Sign Language Recognition. To demonstrate the system's efficacy; we collected the largest sign language dataset for Bangladeshi Sign Language dialect, perform deep learning based Sign Language Recognition modeling, and report the benchmark performance. The Bornil platform, BornilDB v1.0 Dataset, and the codebases are available on https://bornil.bengali.ai",
        "authors": [
            "Shahriar Elahi Dhruvo",
            "Mohammad Akhlaqur Rahman",
            "Manash Kumar Mandal",
            "Md. Istiak Hossain Shihab",
            "A. A. Noman Ansary",
            "Kaneez Fatema Shithi",
            "Sanjida Khanom",
            "Rabeya Akter",
            "Safaeid Hossain Arib",
            "M. N. Ansary",
            "Sazia Mehnaz",
            "Rezwana Sultana",
            "Sejuti Rahman",
            "Sayma Sultana Chowdhury",
            "Sabbir Ahmed Chowdhury",
            "Farig Sadeque",
            "Asif Sushmit"
        ],
        "submitted_date": "2023-08-29T16:00:06+00:00",
        "pdf_url": "https://arxiv.org/pdf/2308.15402v1",
        "similarity_score": 0.6083569526672363
    },
    {
        "arxiv_id": "2302.05075v3",
        "title": "BEST: BERT Pre-Training for Sign Language Recognition with Coupling Tokenization",
        "abstract": "In this work, we are dedicated to leveraging the BERT pre-training success and modeling the domain-specific statistics to fertilize the sign language recognition~(SLR) model. Considering the dominance of hand and body in sign language expression, we organize them as pose triplet units and feed them into the Transformer backbone in a frame-wise manner. Pre-training is performed via reconstructing the masked triplet unit from the corrupted input sequence, which learns the hierarchical correlation context cues among internal and external triplet units. Notably, different from the highly semantic word token in BERT, the pose unit is a low-level signal originally located in continuous space, which prevents the direct adoption of the BERT cross-entropy objective. To this end, we bridge this semantic gap via coupling tokenization of the triplet unit. It adaptively extracts the discrete pseudo label from the pose triplet unit, which represents the semantic gesture/body state. After pre-training, we fine-tune the pre-trained encoder on the downstream SLR task, jointly with the newly added task-specific layer. Extensive experiments are conducted to validate the effectiveness of our proposed method, achieving new state-of-the-art performance on all four benchmarks with a notable gain.",
        "authors": [
            "Weichao Zhao",
            "Hezhen Hu",
            "Wengang Zhou",
            "Jiaxin Shi",
            "Houqiang Li"
        ],
        "submitted_date": "2023-02-10T06:23:44+00:00",
        "pdf_url": "https://arxiv.org/pdf/2302.05075v3",
        "similarity_score": 0.6080282926559448
    },
    {
        "arxiv_id": "1105.0699v1",
        "title": "Robust Sign Language Recognition System Using ToF Depth Cameras",
        "abstract": "Sign language recognition is a difficult task, yet required for many applications in real-time speed. Using RGB cameras for recognition of sign languages is not very successful in practical situations and accurate 3D imaging requires expensive and complex instruments. With introduction of Time-of-Flight (ToF) depth cameras in recent years, it has become easier to scan the environment for accurate, yet fast depth images of the objects without the need of any extra calibrating object. In this paper, a robust system for sign language recognition using ToF depth cameras is presented for converting the recorded signs to a standard and portable XML sign language named SiGML for easy transferring and converting to real-time 3D virtual characters animations. Feature extraction using moments and classification using nearest neighbor classifier are used to track hand gestures and significant result of 100% is achieved for the proposed approach.",
        "authors": [
            "Morteza Zahedi",
            "Ali Reza Manashty"
        ],
        "submitted_date": "2011-05-03T21:57:18+00:00",
        "pdf_url": "https://arxiv.org/pdf/1105.0699v1",
        "similarity_score": 0.6078447103500366
    },
    {
        "arxiv_id": "2505.17055v1",
        "title": "Enhancing Mathematics Learning for Hard-of-Hearing Students Through Real-Time Palestinian Sign Language Recognition: A New Dataset",
        "abstract": "The study aims to enhance mathematics education accessibility for hard-of-hearing students by developing an accurate Palestinian sign language PSL recognition system using advanced artificial intelligence techniques. Due to the scarcity of digital resources for PSL, a custom dataset comprising 41 mathematical gesture classes was created, and recorded by PSL experts to ensure linguistic accuracy and domain specificity. To leverage state-of-the-art-computer vision techniques, a Vision Transformer ViTModel was fine-tuned for gesture classification. The model achieved an accuracy of 97.59%, demonstrating its effectiveness in recognizing mathematical signs with high precision and reliability. This study highlights the role of deep learning in developing intelligent educational tools that bridge the learning gap for hard-of-hearing students by providing AI-driven interactive solutions to enhance mathematical comprehension. This work represents a significant step toward innovative and inclusive frosting digital integration in specialized learning environments. The dataset is hosted on Hugging Face at https://huggingface.co/datasets/fidaakh/STEM_data.",
        "authors": [
            "Fidaa Khandaqji",
            "Huthaifa I. Ashqar",
            "Abdelrahem Atawnih"
        ],
        "submitted_date": "2025-05-16T19:14:25+00:00",
        "pdf_url": "https://arxiv.org/pdf/2505.17055v1",
        "similarity_score": 0.6069968342781067
    },
    {
        "arxiv_id": "1502.02961v1",
        "title": "Avatar-independent scripting for real-time gesture animation",
        "abstract": "When animation of a humanoid figure is to be generated at run-time, instead of by replaying pre-composed motion clips, some method is required of specifying the avatar's movements in a form from which the required motion data can be automatically generated. This form must be of a more abstract nature than raw motion data: ideally, it should be independent of the particular avatar's proportions, and both writable by hand and suitable for automatic generation from higher-level descriptions of the required actions.   We describe here the development and implementation of such a scripting language for the particular area of sign languages of the deaf, called SiGML (Signing Gesture Markup Language), based on the existing HamNoSys notation for sign languages.   We conclude by suggesting how this work may be extended to more general animation for interactive virtual reality applications.",
        "authors": [
            "Richard Kennaway"
        ],
        "submitted_date": "2015-02-10T16:03:37+00:00",
        "pdf_url": "https://arxiv.org/pdf/1502.02961v1",
        "similarity_score": 0.6042428016662598
    },
    {
        "arxiv_id": "2411.02768v1",
        "title": "One-Stage-TFS: Thai One-Stage Fingerspelling Dataset for Fingerspelling Recognition Frameworks",
        "abstract": "The Thai One-Stage Fingerspelling (One-Stage-TFS) dataset is a comprehensive resource designed to advance research in hand gesture recognition, explicitly focusing on the recognition of Thai sign language. This dataset comprises 7,200 images capturing 15 one-stage consonant gestures performed by undergraduate students from Rajabhat Maha Sarakham University, Thailand. The contributors include both expert students from the Special Education Department with proficiency in Thai sign language and students from other departments without prior sign language experience. Images were collected between July and December 2021 using a DSLR camera, with contributors demonstrating hand gestures against both simple and complex backgrounds. The One-Stage-TFS dataset presents challenges in detecting and recognizing hand gestures, offering opportunities to develop novel end-to-end recognition frameworks. Researchers can utilize this dataset to explore deep learning methods, such as YOLO, EfficientDet, RetinaNet, and Detectron, for hand detection, followed by feature extraction and recognition using techniques like convolutional neural networks, transformers, and adaptive feature fusion networks. The dataset is accessible via the Mendeley Data repository and supports a wide range of applications in computer science, including deep learning, computer vision, and pattern recognition, thereby encouraging further innovation and exploration in these fields.",
        "authors": [
            "Siriwiwat Lata",
            "Sirawan Phiphitphatphaisit",
            "Emmanuel Okafor",
            "Olarik Surinta"
        ],
        "submitted_date": "2024-11-05T03:26:26+00:00",
        "pdf_url": "https://arxiv.org/pdf/2411.02768v1",
        "similarity_score": 0.60405433177948
    },
    {
        "arxiv_id": "2004.05054v1",
        "title": "ASL Recognition with Metric-Learning based Lightweight Network",
        "abstract": "In the past decades the set of human tasks that are solved by machines was extended dramatically. From simple image classification problems researchers now move towards solving more sophisticated and vital problems, like, autonomous driving and language translation. The case of language translation includes a challenging area of sign language translation that incorporates both image and language processing. We make a step in that direction by proposing a lightweight network for ASL gesture recognition with a performance sufficient for practical applications. The proposed solution demonstrates impressive robustness on MS-ASL dataset and in live mode for continuous sign gesture recognition scenario. Additionally, we describe how to combine action recognition model training with metric-learning to train the network on the database of limited size. The training code is available as part of Intel OpenVINO Training Extensions.",
        "authors": [
            "Evgeny Izutov"
        ],
        "submitted_date": "2020-04-10T14:41:30+00:00",
        "pdf_url": "https://arxiv.org/pdf/2004.05054v1",
        "similarity_score": 0.6034879684448242
    },
    {
        "arxiv_id": "2401.08714v1",
        "title": "Training program on sign language: social inclusion through Virtual Reality in ISENSE project",
        "abstract": "Structured hand gestures that incorporate visual motions and signs are used in sign language. Sign language is a valuable means of daily communication for individuals who are deaf or have speech impairments, but it is still rare among hearing people, and fewer are capable of understand it. Within the academic context, parents and teachers play a crucial role in supporting deaf students from childhood by facilitating their learning of sign language. In the last years, among all the teaching tools useful for learning sign language, the use of Virtual Reality (VR) has increased, as it has been demonstrated to improve retention, memory and attention during the learning process. The ISENSE project has been created to assist students with deafness during their academic life by proposing different technological tools for teaching sign language to the hearing community in the academic context. As part of the ISENSE project, this work aims to develop an application for Spanish and Italian sign language recognition that exploits the VR environment to quickly and easily create a comprehensive database of signs and an Artificial Intelligence (AI)-based software to accurately classify and recognize static and dynamic signs: from letters to sentences.",
        "authors": [
            "Alessia Bisio",
            "Enrique Yeguas-Bolívar",
            "Pilar Aparicio-Martínez",
            "María Dolores Redel-Macías",
            "Sara Pinzi",
            "Stefano Rossi",
            "Juri Taborri"
        ],
        "submitted_date": "2024-01-15T20:40:46+00:00",
        "pdf_url": "https://arxiv.org/pdf/2401.08714v1",
        "similarity_score": 0.603266716003418
    },
    {
        "arxiv_id": "2309.06572v1",
        "title": "Addressing the Blind Spots in Spoken Language Processing",
        "abstract": "This paper explores the critical but often overlooked role of non-verbal cues, including co-speech gestures and facial expressions, in human communication and their implications for Natural Language Processing (NLP). We argue that understanding human communication requires a more holistic approach that goes beyond textual or spoken words to include non-verbal elements. Borrowing from advances in sign language processing, we propose the development of universal automatic gesture segmentation and transcription models to transcribe these non-verbal cues into textual form. Such a methodology aims to bridge the blind spots in spoken language understanding, enhancing the scope and applicability of NLP models. Through motivating examples, we demonstrate the limitations of relying solely on text-based models. We propose a computationally efficient and flexible approach for incorporating non-verbal cues, which can seamlessly integrate with existing NLP pipelines. We conclude by calling upon the research community to contribute to the development of universal transcription methods and to validate their effectiveness in capturing the complexities of real-world, multi-modal interactions.",
        "authors": [
            "Amit Moryossef"
        ],
        "submitted_date": "2023-09-06T10:29:25+00:00",
        "pdf_url": "https://arxiv.org/pdf/2309.06572v1",
        "similarity_score": 0.6025148034095764
    },
    {
        "arxiv_id": "2411.03568v1",
        "title": "The American Sign Language Knowledge Graph: Infusing ASL Models with Linguistic Knowledge",
        "abstract": "Language models for American Sign Language (ASL) could make language technologies substantially more accessible to those who sign. To train models on tasks such as isolated sign recognition (ISR) and ASL-to-English translation, datasets provide annotated video examples of ASL signs. To facilitate the generalizability and explainability of these models, we introduce the American Sign Language Knowledge Graph (ASLKG), compiled from twelve sources of expert linguistic knowledge. We use the ASLKG to train neuro-symbolic models for 3 ASL understanding tasks, achieving accuracies of 91% on ISR, 14% for predicting the semantic features of unseen signs, and 36% for classifying the topic of Youtube-ASL videos.",
        "authors": [
            "Lee Kezar",
            "Nidhi Munikote",
            "Zian Zeng",
            "Zed Sehyr",
            "Naomi Caselli",
            "Jesse Thomason"
        ],
        "submitted_date": "2024-11-06T00:16:16+00:00",
        "pdf_url": "https://arxiv.org/pdf/2411.03568v1",
        "similarity_score": 0.6016349792480469
    },
    {
        "arxiv_id": "2007.09859v1",
        "title": "Novel Approach to Use HU Moments with Image Processing Techniques for Real Time Sign Language Communication",
        "abstract": "Sign language is the fundamental communication method among people who suffer from speech and hearing defects. The rest of the world doesn't have a clear idea of sign language. \"Sign Language Communicator\" (SLC) is designed to solve the language barrier between the sign language users and the rest of the world. The main objective of this research is to provide a low cost affordable method of sign language interpretation. This system will also be very useful to the sign language learners as they can practice the sign language. During the research available human computer interaction techniques in posture recognition was tested and evaluated. A series of image processing techniques with Hu-moment classification was identified as the best approach. To improve the accuracy of the system, a new approach height to width ratio filtration was implemented along with Hu-moments. System is able to recognize selected Sign Language signs with the accuracy of 84% without a controlled background with small light adjustments",
        "authors": [
            "Matheesha Fernando",
            "Janaka Wijayanayake"
        ],
        "submitted_date": "2020-07-20T03:10:18+00:00",
        "pdf_url": "https://arxiv.org/pdf/2007.09859v1",
        "similarity_score": 0.6005784273147583
    },
    {
        "arxiv_id": "2304.05934v2",
        "title": "ASL Citizen: A Community-Sourced Dataset for Advancing Isolated Sign Language Recognition",
        "abstract": "Sign languages are used as a primary language by approximately 70 million D/deaf people world-wide. However, most communication technologies operate in spoken and written languages, creating inequities in access. To help tackle this problem, we release ASL Citizen, the first crowdsourced Isolated Sign Language Recognition (ISLR) dataset, collected with consent and containing 83,399 videos for 2,731 distinct signs filmed by 52 signers in a variety of environments. We propose that this dataset be used for sign language dictionary retrieval for American Sign Language (ASL), where a user demonstrates a sign to their webcam to retrieve matching signs from a dictionary. We show that training supervised machine learning classifiers with our dataset advances the state-of-the-art on metrics relevant for dictionary retrieval, achieving 63% accuracy and a recall-at-10 of 91%, evaluated entirely on videos of users who are not present in the training or validation sets. An accessible PDF of this article is available at the following link: https://aashakadesai.github.io/research/ASLCitizen_arxiv_updated.pdf",
        "authors": [
            "Aashaka Desai",
            "Lauren Berger",
            "Fyodor O. Minakov",
            "Vanessa Milan",
            "Chinmay Singh",
            "Kriston Pumphrey",
            "Richard E. Ladner",
            "Hal Daumé",
            "Alex X. Lu",
            "Naomi Caselli",
            "Danielle Bragg"
        ],
        "submitted_date": "2023-04-12T15:52:53+00:00",
        "pdf_url": "https://arxiv.org/pdf/2304.05934v2",
        "similarity_score": 0.5987163186073303
    },
    {
        "arxiv_id": "2406.10231v1",
        "title": "Sign Language Recognition based on YOLOv5 Algorithm for the Telugu Sign Language",
        "abstract": "Sign language recognition (SLR) technology has enormous promise to improve communication and accessibility for the difficulty of hearing. This paper presents a novel approach for identifying gestures in TSL using the YOLOv5 object identification framework. The main goal is to create an accurate and successful method for identifying TSL gestures so that the deaf community can use slr. After that, a deep learning model was created that used the YOLOv5 to recognize and classify gestures. This model benefited from the YOLOv5 architecture's high accuracy, speed, and capacity to handle complex sign language features. Utilizing transfer learning approaches, the YOLOv5 model was customized to TSL gestures. To attain the best outcomes, careful parameter and hyperparameter adjustment was carried out during training. With F1-score and mean Average Precision (mAP) ratings of 90.5% and 98.1%, the YOLOv5-medium model stands out for its outstanding performance metrics, demonstrating its efficacy in Telugu sign language identification tasks. Surprisingly, this model strikes an acceptable balance between computational complexity and training time to produce these amazing outcomes. Because it offers a convincing blend of accuracy and efficiency, the YOLOv5-medium model, trained for 200 epochs, emerges as the recommended choice for real-world deployment. The system's stability and generalizability across various TSL gestures and settings were evaluated through rigorous testing and validation, which yielded outstanding accuracy. This research lays the foundation for future advancements in accessible technology for linguistic communities by providing a cutting-edge application of deep learning and computer vision techniques to TSL gesture identification. It also offers insightful perspectives and novel approaches to the field of sign language recognition.",
        "authors": [
            "Vipul Reddy. P",
            "Vishnu Vardhan Reddy. B",
            "Sukriti"
        ],
        "submitted_date": "2024-04-24T18:39:27+00:00",
        "pdf_url": "https://arxiv.org/pdf/2406.10231v1",
        "similarity_score": 0.5986499786376953
    },
    {
        "arxiv_id": "1803.10435v1",
        "title": "Exploiting Recurrent Neural Networks and Leap Motion Controller for Sign Language and Semaphoric Gesture Recognition",
        "abstract": "In human interactions, hands are a powerful way of expressing information that, in some cases, can be used as a valid substitute for voice, as it happens in Sign Language. Hand gesture recognition has always been an interesting topic in the areas of computer vision and multimedia. These gestures can be represented as sets of feature vectors that change over time. Recurrent Neural Networks (RNNs) are suited to analyse this type of sets thanks to their ability to model the long term contextual information of temporal sequences. In this paper, a RNN is trained by using as features the angles formed by the finger bones of human hands. The selected features, acquired by a Leap Motion Controller (LMC) sensor, have been chosen because the majority of human gestures produce joint movements that generate truly characteristic corners. A challenging subset composed by a large number of gestures defined by the American Sign Language (ASL) is used to test the proposed solution and the effectiveness of the selected angles. Moreover, the proposed method has been compared to other state of the art works on the SHREC dataset, thus demonstrating its superiority in hand gesture recognition accuracy.",
        "authors": [
            "Danilo Avola",
            "Marco Bernardi",
            "Luigi Cinque",
            "Gian Luca Foresti",
            "Cristiano Massaroni"
        ],
        "submitted_date": "2018-03-28T07:25:51+00:00",
        "pdf_url": "https://arxiv.org/pdf/1803.10435v1",
        "similarity_score": 0.5959679484367371
    },
    {
        "arxiv_id": "2108.10970v1",
        "title": "Real-time Indian Sign Language (ISL) Recognition",
        "abstract": "This paper presents a system which can recognise hand poses & gestures from the Indian Sign Language (ISL) in real-time using grid-based features. This system attempts to bridge the communication gap between the hearing and speech impaired and the rest of the society. The existing solutions either provide relatively low accuracy or do not work in real-time. This system provides good results on both the parameters. It can identify 33 hand poses and some gestures from the ISL. Sign Language is captured from a smartphone camera and its frames are transmitted to a remote server for processing. The use of any external hardware (such as gloves or the Microsoft Kinect sensor) is avoided, making it user-friendly. Techniques such as Face detection, Object stabilisation and Skin Colour Segmentation are used for hand detection and tracking. The image is further subjected to a Grid-based Feature Extraction technique which represents the hand's pose in the form of a Feature Vector. Hand poses are then classified using the k-Nearest Neighbours algorithm. On the other hand, for gesture classification, the motion and intermediate hand poses observation sequences are fed to Hidden Markov Model chains corresponding to the 12 pre-selected gestures defined in ISL. Using this methodology, the system is able to achieve an accuracy of 99.7% for static hand poses, and an accuracy of 97.23% for gesture recognition.",
        "authors": [
            "Kartik Shenoy",
            "Tejas Dastane",
            "Varun Rao",
            "Devendra Vyavaharkar"
        ],
        "submitted_date": "2021-08-24T21:49:21+00:00",
        "pdf_url": "https://arxiv.org/pdf/2108.10970v1",
        "similarity_score": 0.5944820642471313
    },
    {
        "arxiv_id": "2303.10782v1",
        "title": "On the Importance of Signer Overlap for Sign Language Detection",
        "abstract": "Sign language detection, identifying if someone is signing or not, is becoming crucially important for its applications in remote conferencing software and for selecting useful sign data for training sign language recognition or translation tasks. We argue that the current benchmark data sets for sign language detection estimate overly positive results that do not generalize well due to signer overlap between train and test partitions. We quantify this with a detailed analysis of the effect of signer overlap on current sign detection benchmark data sets. Comparing accuracy with and without overlap on the DGS corpus and Signing in the Wild, we observed a relative decrease in accuracy of 4.17% and 6.27%, respectively. Furthermore, we propose new data set partitions that are free of overlap and allow for more realistic performance assessment. We hope this work will contribute to improving the accuracy and generalization of sign language detection systems.",
        "authors": [
            "Abhilash Pal",
            "Stephan Huber",
            "Cyrine Chaabani",
            "Alessandro Manzotti",
            "Oscar Koller"
        ],
        "submitted_date": "2023-03-19T22:15:05+00:00",
        "pdf_url": "https://arxiv.org/pdf/2303.10782v1",
        "similarity_score": 0.593679666519165
    },
    {
        "arxiv_id": "2401.12210v1",
        "title": "Connecting the Dots: Leveraging Spatio-Temporal Graph Neural Networks for Accurate Bangla Sign Language Recognition",
        "abstract": "Recent advances in Deep Learning and Computer Vision have been successfully leveraged to serve marginalized communities in various contexts. One such area is Sign Language - a primary means of communication for the deaf community. However, so far, the bulk of research efforts and investments have gone into American Sign Language, and research activity into low-resource sign languages - especially Bangla Sign Language - has lagged significantly. In this research paper, we present a new word-level Bangla Sign Language dataset - BdSL40 - consisting of 611 videos over 40 words, along with two different approaches: one with a 3D Convolutional Neural Network model and another with a novel Graph Neural Network approach for the classification of BdSL40 dataset. This is the first study on word-level BdSL recognition, and the dataset was transcribed from Indian Sign Language (ISL) using the Bangla Sign Language Dictionary (1997). The proposed GNN model achieved an F1 score of 89%. The study highlights the significant lexical and semantic similarity between BdSL, West Bengal Sign Language, and ISL, and the lack of word-level datasets for BdSL in the literature. We release the dataset and source code to stimulate further research.",
        "authors": [
            "Haz Sameen Shahgir",
            "Khondker Salman Sayeed",
            "Md Toki Tahmid",
            "Tanjeem Azwad Zaman",
            "Md. Zarif Ul Alam"
        ],
        "submitted_date": "2024-01-22T18:52:51+00:00",
        "pdf_url": "https://arxiv.org/pdf/2401.12210v1",
        "similarity_score": 0.5922484397888184
    },
    {
        "arxiv_id": "2311.10174v1",
        "title": "JWSign: A Highly Multilingual Corpus of Bible Translations for more Diversity in Sign Language Processing",
        "abstract": "Advancements in sign language processing have been hindered by a lack of sufficient data, impeding progress in recognition, translation, and production tasks. The absence of comprehensive sign language datasets across the world's sign languages has widened the gap in this field, resulting in a few sign languages being studied more than others, making this research area extremely skewed mostly towards sign languages from high-income countries. In this work we introduce a new large and highly multilingual dataset for sign language translation: JWSign. The dataset consists of 2,530 hours of Bible translations in 98 sign languages, featuring more than 1,500 individual signers. On this dataset, we report neural machine translation experiments. Apart from bilingual baseline systems, we also train multilingual systems, including some that take into account the typological relatedness of signed or spoken languages. Our experiments highlight that multilingual systems are superior to bilingual baselines, and that in higher-resource scenarios, clustering language pairs that are related improves translation quality.",
        "authors": [
            "Shester Gueuwou",
            "Sophie Siake",
            "Colin Leong",
            "Mathias Müller"
        ],
        "submitted_date": "2023-11-16T20:02:44+00:00",
        "pdf_url": "https://arxiv.org/pdf/2311.10174v1",
        "similarity_score": 0.5922071933746338
    },
    {
        "arxiv_id": "2008.10428v2",
        "title": "Global-local Enhancement Network for NMFs-aware Sign Language Recognition",
        "abstract": "Sign language recognition (SLR) is a challenging problem, involving complex manual features, i.e., hand gestures, and fine-grained non-manual features (NMFs), i.e., facial expression, mouth shapes, etc. Although manual features are dominant, non-manual features also play an important role in the expression of a sign word. Specifically, many sign words convey different meanings due to non-manual features, even though they share the same hand gestures. This ambiguity introduces great challenges in the recognition of sign words. To tackle the above issue, we propose a simple yet effective architecture called Global-local Enhancement Network (GLE-Net), including two mutually promoted streams towards different crucial aspects of SLR. Of the two streams, one captures the global contextual relationship, while the other stream captures the discriminative fine-grained cues. Moreover, due to the lack of datasets explicitly focusing on this kind of features, we introduce the first non-manual-features-aware isolated Chinese sign language dataset~(NMFs-CSL) with a total vocabulary size of 1,067 sign words in daily life. Extensive experiments on NMFs-CSL and SLR500 datasets demonstrate the effectiveness of our method.",
        "authors": [
            "Hezhen Hu",
            "Wengang Zhou",
            "Junfu Pu",
            "Houqiang Li"
        ],
        "submitted_date": "2020-08-24T13:28:55+00:00",
        "pdf_url": "https://arxiv.org/pdf/2008.10428v2",
        "similarity_score": 0.5916363596916199
    },
    {
        "arxiv_id": "2412.18187v1",
        "title": "Learning Sign Language Representation using CNN LSTM, 3DCNN, CNN RNN LSTM and CCN TD",
        "abstract": "Existing Sign Language Learning applications focus on the demonstration of the sign in the hope that the student will copy a sign correctly. In these cases, only a teacher can confirm that the sign was completed correctly, by reviewing a video captured manually. Sign Language Translation is a widely explored field in visual recognition. This paper seeks to explore the algorithms that will allow for real-time, video sign translation, and grading of sign language accuracy for new sign language users. This required algorithms capable of recognizing and processing spatial and temporal features. The aim of this paper is to evaluate and identify the best neural network algorithm that can facilitate a sign language tuition system of this nature. Modern popular algorithms including CNN and 3DCNN are compared on a dataset not yet explored, Trinidad and Tobago Sign Language as well as an American Sign Language dataset. The 3DCNN algorithm was found to be the best performing neural network algorithm from these systems with 91% accuracy in the TTSL dataset and 83% accuracy in the ASL dataset.",
        "authors": [
            "Nikita Louison",
            "Wayne Goodridge",
            "Koffka Khan"
        ],
        "submitted_date": "2024-12-24T05:47:08+00:00",
        "pdf_url": "https://arxiv.org/pdf/2412.18187v1",
        "similarity_score": 0.589077889919281
    },
    {
        "arxiv_id": "2209.02402v1",
        "title": "Topic Detection in Continuous Sign Language Videos",
        "abstract": "Significant progress has been made recently on challenging tasks in automatic sign language understanding, such as sign language recognition, translation and production. However, these works have focused on datasets with relatively few samples, short recordings and limited vocabulary and signing space. In this work, we introduce the novel task of sign language topic detection. We base our experiments on How2Sign, a large-scale video dataset spanning multiple semantic domains. We provide strong baselines for the task of topic detection and present a comparison between different visual features commonly used in the domain of sign language.",
        "authors": [
            "Alvaro Budria",
            "Laia Tarres",
            "Gerard I. Gallego",
            "Francesc Moreno-Noguer",
            "Jordi Torres",
            "Xavier Giro-i-Nieto"
        ],
        "submitted_date": "2022-09-01T19:17:35+00:00",
        "pdf_url": "https://arxiv.org/pdf/2209.02402v1",
        "similarity_score": 0.5888068675994873
    },
    {
        "arxiv_id": "2506.11154v1",
        "title": "SLRNet: A Real-Time LSTM-Based Sign Language Recognition System",
        "abstract": "Sign Language Recognition (SLR) plays a crucial role in bridging the communication gap between the hearing-impaired community and society. This paper introduces SLRNet, a real-time webcam-based ASL recognition system using MediaPipe Holistic and Long Short-Term Memory (LSTM) networks. The model processes video streams to recognize both ASL alphabet letters and functional words. With a validation accuracy of 86.7%, SLRNet demonstrates the feasibility of inclusive, hardware-independent gesture recognition.",
        "authors": [
            "Sharvari Kamble"
        ],
        "submitted_date": "2025-06-11T14:30:47+00:00",
        "pdf_url": "https://arxiv.org/pdf/2506.11154v1",
        "similarity_score": 0.5848924517631531
    },
    {
        "arxiv_id": "2301.02693v1",
        "title": "Design of Arabic Sign Language Recognition Model",
        "abstract": "Deaf people are using sign language for communication, and it is a combination of gestures, movements, postures, and facial expressions that correspond to alphabets and words in spoken languages. The proposed Arabic sign language recognition model helps deaf and hard hearing people communicate effectively with ordinary people. The recognition has four stages of converting the alphabet into letters as follows: Image Loading stage, which loads the images of Arabic sign language alphabets that were used later to train and test the model, a pre-processing stage which applies image processing techniques such as normalization, Image augmentation, resizing, and filtering to extract the features which are necessary to accomplish the recognition perfectly, a training stage which is achieved by deep learning techniques like CNN, a testing stage which demonstrates how effectively the model performs for images did not see it before, and the model was built and tested mainly using PyTorch library. The model is tested on ArASL2018, consisting of 54,000 images for 32 alphabet signs gathered from 40 signers, and the dataset has two sets: training dataset and testing dataset. We had to ensure that the system is reliable in terms of accuracy, time, and flexibility of use explained in detail in this report. Finally, the future work will be a model that converts Arabic sign language into Arabic text.",
        "authors": [
            "Muhammad Al-Barham",
            "Ahmad Jamal",
            "Musa Al-Yaman"
        ],
        "submitted_date": "2023-01-06T19:19:25+00:00",
        "pdf_url": "https://arxiv.org/pdf/2301.02693v1",
        "similarity_score": 0.5839213132858276
    },
    {
        "arxiv_id": "2003.08759v2",
        "title": "Facial Expression Phoenix (FePh): An Annotated Sequenced Dataset for Facial and Emotion-Specified Expressions in Sign Language",
        "abstract": "Facial expressions are important parts of both gesture and sign language recognition systems. Despite the recent advances in both fields, annotated facial expression datasets in the context of sign language are still scarce resources. In this manuscript, we introduce an annotated sequenced facial expression dataset in the context of sign language, comprising over $3000$ facial images extracted from the daily news and weather forecast of the public tv-station PHOENIX. Unlike the majority of currently existing facial expression datasets, FePh provides sequenced semi-blurry facial images with different head poses, orientations, and movements. In addition, in the majority of images, identities are mouthing the words, which makes the data more challenging. To annotate this dataset we consider primary, secondary, and tertiary dyads of seven basic emotions of \"sad\", \"surprise\", \"fear\", \"angry\", \"neutral\", \"disgust\", and \"happy\". We also considered the \"None\" class if the image's facial expression could not be described by any of the aforementioned emotions. Although we provide FePh as a facial expression dataset of signers in sign language, it has a wider application in gesture recognition and Human Computer Interaction (HCI) systems.",
        "authors": [
            "Marie Alaghband",
            "Niloofar Yousefi",
            "Ivan Garibay"
        ],
        "submitted_date": "2020-03-03T03:42:36+00:00",
        "pdf_url": "https://arxiv.org/pdf/2003.08759v2",
        "similarity_score": 0.5836836099624634
    },
    {
        "arxiv_id": "2309.16898v1",
        "title": "A Sign Language Recognition System with Pepper, Lightweight-Transformer, and LLM",
        "abstract": "This research explores using lightweight deep neural network architectures to enable the humanoid robot Pepper to understand American Sign Language (ASL) and facilitate non-verbal human-robot interaction. First, we introduce a lightweight and efficient model for ASL understanding optimized for embedded systems, ensuring rapid sign recognition while conserving computational resources. Building upon this, we employ large language models (LLMs) for intelligent robot interactions. Through intricate prompt engineering, we tailor interactions to allow the Pepper Robot to generate natural Co-Speech Gesture responses, laying the foundation for more organic and intuitive humanoid-robot dialogues. Finally, we present an integrated software pipeline, embodying advancements in a socially aware AI interaction model. Leveraging the Pepper Robot's capabilities, we demonstrate the practicality and effectiveness of our approach in real-world scenarios. The results highlight a profound potential for enhancing human-robot interaction through non-verbal interactions, bridging communication gaps, and making technology more accessible and understandable.",
        "authors": [
            "JongYoon Lim",
            "Inkyu Sa",
            "Bruce MacDonald",
            "Ho Seok Ahn"
        ],
        "submitted_date": "2023-09-28T23:54:41+00:00",
        "pdf_url": "https://arxiv.org/pdf/2309.16898v1",
        "similarity_score": 0.5835714936256409
    },
    {
        "arxiv_id": "2505.08072v1",
        "title": "Perspectives on Capturing Emotional Expressiveness in Sign Language",
        "abstract": "Significant advances have been made in our ability to understand and generate emotionally expressive content such as text and speech, yet comparable progress in sign language technologies remain limited. While computational approaches to sign language translation have focused on capturing lexical content, the emotional dimensions of sign language communication remain largely unexplored. Through semi-structured interviews with eight sign language users across Singapore, Sri Lanka and the United States, including both Deaf and Hard of hearing (DHH) and hearing signers, we investigate how emotions are expressed and perceived in sign languages. Our findings highlight the role of both manual and non-manual elements in emotional expression, revealing universal patterns as well as individual and cultural variations in how signers communicate emotions. We identify key challenges in capturing emotional nuance for sign language translation, and propose design considerations for developing more emotionally-aware sign language technologies. This work contributes to both theoretical understanding of emotional expression in sign language and practical development of interfaces to better serve diverse signing communities.",
        "authors": [
            "Phoebe Chua",
            "Cathy Mengying Fang",
            "Yasith Samaradivakara",
            "Pattie Maes",
            "Suranga Nanayakkara"
        ],
        "submitted_date": "2025-05-12T21:17:51+00:00",
        "pdf_url": "https://arxiv.org/pdf/2505.08072v1",
        "similarity_score": 0.5832290649414062
    },
    {
        "arxiv_id": "1304.3265v1",
        "title": "Extension of hidden markov model for recognizing large vocabulary of sign language",
        "abstract": "Computers still have a long way to go before they can interact with users in a truly natural fashion. From a users perspective, the most natural way to interact with a computer would be through a speech and gesture interface. Although speech recognition has made significant advances in the past ten years, gesture recognition has been lagging behind. Sign Languages (SL) are the most accomplished forms of gestural communication. Therefore, their automatic analysis is a real challenge, which is interestingly implied to their lexical and syntactic organization levels. Statements dealing with sign language occupy a significant interest in the Automatic Natural Language Processing (ANLP) domain. In this work, we are dealing with sign language recognition, in particular of French Sign Language (FSL). FSL has its own specificities, such as the simultaneity of several parameters, the important role of the facial expression or movement and the use of space for the proper utterance organization. Unlike speech recognition, Frensh sign language (FSL) events occur both sequentially and simultaneously. Thus, the computational processing of FSL is too complex than the spoken languages. We present a novel approach based on HMM to reduce the recognition complexity.",
        "authors": [
            "Maher Jebali",
            "Patrice Dalle",
            "Mohamed Jemni"
        ],
        "submitted_date": "2013-04-11T11:56:39+00:00",
        "pdf_url": "https://arxiv.org/pdf/1304.3265v1",
        "similarity_score": 0.5819200277328491
    },
    {
        "arxiv_id": "2201.01486v2",
        "title": "Sign Language Recognition System using TensorFlow Object Detection API",
        "abstract": "Communication is defined as the act of sharing or exchanging information, ideas or feelings. To establish communication between two people, both of them are required to have knowledge and understanding of a common language. But in the case of deaf and dumb people, the means of communication are different. Deaf is the inability to hear and dumb is the inability to speak. They communicate using sign language among themselves and with normal people but normal people do not take seriously the importance of sign language. Not everyone possesses the knowledge and understanding of sign language which makes communication difficult between a normal person and a deaf and dumb person. To overcome this barrier, one can build a model based on machine learning. A model can be trained to recognize different gestures of sign language and translate them into English. This will help a lot of people in communicating and conversing with deaf and dumb people. The existing Indian Sing Language Recognition systems are designed using machine learning algorithms with single and double-handed gestures but they are not real-time. In this paper, we propose a method to create an Indian Sign Language dataset using a webcam and then using transfer learning, train a TensorFlow model to create a real-time Sign Language Recognition system. The system achieves a good level of accuracy even with a limited size dataset.",
        "authors": [
            "Sharvani Srivastava",
            "Amisha Gangwar",
            "Richa Mishra",
            "Sudhakar Singh"
        ],
        "submitted_date": "2022-01-05T07:13:03+00:00",
        "pdf_url": "https://arxiv.org/pdf/2201.01486v2",
        "similarity_score": 0.5776716470718384
    },
    {
        "arxiv_id": "2409.01975v2",
        "title": "Attention vs LSTM: Improving Word-level BISINDO Recognition",
        "abstract": "Indonesia ranks fourth globally in the number of deaf cases. Individuals with hearing impairments often find communication challenging, necessitating the use of sign language. However, there are limited public services that offer such inclusivity. On the other hand, advancements in artificial intelligence (AI) present promising solutions to overcome communication barriers faced by the deaf. This study aims to explore the application of AI in developing models for a simplified sign language translation app and dictionary, designed for integration into public service facilities, to facilitate communication for individuals with hearing impairments, thereby enhancing inclusivity in public services. The researchers compared the performance of LSTM and 1D CNN + Transformer (1DCNNTrans) models for sign language recognition. Through rigorous testing and validation, it was found that the LSTM model achieved an accuracy of 94.67%, while the 1DCNNTrans model achieved an accuracy of 96.12%. Model performance evaluation indicated that although the LSTM exhibited lower inference latency, it showed weaknesses in classifying classes with similar keypoints. In contrast, the 1DCNNTrans model demonstrated greater stability and higher F1 scores for classes with varying levels of complexity compared to the LSTM model. Both models showed excellent performance, exceeding 90% validation accuracy and demonstrating rapid classification of 50 sign language gestures.",
        "authors": [
            "Muchammad Daniyal Kautsar",
            "Afra Majida Hariono",
            "Ridwan Akmal"
        ],
        "submitted_date": "2024-09-03T15:17:39+00:00",
        "pdf_url": "https://arxiv.org/pdf/2409.01975v2",
        "similarity_score": 0.5750058889389038
    },
    {
        "arxiv_id": "1410.4012v1",
        "title": "Online interpretation of numeric sign language using 2-d skeletal model",
        "abstract": "Gesturing is one of the natural modes of human communication. Signs produced by gestures can have a basic meaning coupled with additional information that is layered over the basic meaning of the sign. Sign language is an important example of communicative gestures that are highly structured and well accepted across the world as a communication medium for deaf and dumb. In this paper, an online recognition scheme is proposed to interpret the standard numeric sign language comprising of 10 basic hand symbols. A web camera is used to capture the real time hand movements as input to the system. The basic meaning of the hand gesture is extracted from the input data frame by analysing the shape of the hand, considering its orientation, movement and location to be fixed. The input hand shape is processed to identify the palm structure, fingertips and their relative positions and the presence of the extended thumb. A 2-dimensional skeletal model is generated from the acquired shape information to represent and subsequently interpret the basic meaning of the hand gesture.",
        "authors": [
            "Subhadip Basu",
            "S. Dey",
            "K. Mukherjee",
            "T. S. Jana"
        ],
        "submitted_date": "2014-10-15T11:18:55+00:00",
        "pdf_url": "https://arxiv.org/pdf/1410.4012v1",
        "similarity_score": 0.5716423988342285
    },
    {
        "arxiv_id": "2408.00611v1",
        "title": "Using CSNNs to Perform Event-based Data Processing & Classification on ASL-DVS",
        "abstract": "Recent advancements in bio-inspired visual sensing and neuromorphic computing have led to the development of various highly efficient bio-inspired solutions with real-world applications. One notable application integrates event-based cameras with spiking neural networks (SNNs) to process event-based sequences that are asynchronous and sparse, making them difficult to handle. In this project, we develop a convolutional spiking neural network (CSNN) architecture that leverages convolutional operations and recurrent properties of a spiking neuron to learn the spatial and temporal relations in the ASL-DVS gesture dataset. The ASL-DVS gesture dataset is a neuromorphic dataset containing hand gestures when displaying 24 letters (A to Y, excluding J and Z due to the nature of their symbols) from the American Sign Language (ASL). We performed classification on a pre-processed subset of the full ASL-DVS dataset to identify letter signs and achieved 100\\% training accuracy. Specifically, this was achieved by training in the Google Cloud compute platform while using a learning rate of 0.0005, batch size of 25 (total of 20 batches), 200 iterations, and 10 epochs.",
        "authors": [
            "Ria Patel",
            "Sujit Tripathy",
            "Zachary Sublett",
            "Seoyoung An",
            "Riya Patel"
        ],
        "submitted_date": "2024-08-01T14:49:43+00:00",
        "pdf_url": "https://arxiv.org/pdf/2408.00611v1",
        "similarity_score": 0.5706040859222412
    },
    {
        "arxiv_id": "2510.23056v2",
        "title": "Enabling American Sign Language Communication Under Low Data Rates",
        "abstract": "In recent years, video conferencing applications have become increasingly prevalent, relying heavily on high-speed internet connectivity. When such connectivity is lacking, users often default to audio-only communication, a mode that significantly disadvantages American Sign Language (ASL) users, whose communication relies on hand gestures, body movement, and facial expressions. In this work, we introduce VC4ASL, a system designed to enable ASL communication over the audio channel of existing video conferencing applications, even in the absence of reliable video. VC4ASL integrates seamlessly with current platforms without requiring any modifications. Our approach establishes a communication channel through audio by encoding and transmitting human pose information, which is then rendered to reconstruct signed content. We propose novel receive-side error detection and correction mechanisms that exploit the inherent structural constraints of human pose data. To evaluate the system, we simulate network-degraded environments, generate pose-based ASL video sequences, and conduct user studies to assess comprehension among ASL users. Experimental results demonstrate that VC4ASL effectively facilitates intelligible ASL communication over audio in low-bandwidth scenarios where video transmission is impaired.",
        "authors": [
            "Panneer Selvam Santhalingam",
            "Swann Thantsin",
            "Ahmad Kamari",
            "Parth Pathak",
            "Kenneth DeHaan"
        ],
        "submitted_date": "2025-10-27T06:39:50+00:00",
        "pdf_url": "https://arxiv.org/pdf/2510.23056v2",
        "similarity_score": 0.5702846646308899
    },
    {
        "arxiv_id": "2010.10590v3",
        "title": "American Sign Language Identification Using Hand Trackpoint Analysis",
        "abstract": "Sign Language helps people with Speaking and Hearing Disabilities communicate with others efficiently. Sign Language identification is a challenging area in the field of computer vision and recent developments have been able to achieve near perfect results for the task, though some challenges are yet to be solved. In this paper we propose a novel machine learning based pipeline for American Sign Language identification using hand track points. We convert a hand gesture into a series of hand track point coordinates that serve as an input to our system. In order to make the solution more efficient, we experimented with 28 different combinations of pre-processing techniques, each run on three different machine learning algorithms namely k-Nearest Neighbours, Random Forests and a Neural Network. Their performance was contrasted to determine the best pre-processing scheme and algorithm pair. Our system achieved an Accuracy of 95.66% to identify American sign language gestures.",
        "authors": [
            "Yugam Bajaj",
            "Puru Malhotra"
        ],
        "submitted_date": "2020-10-20T19:59:16+00:00",
        "pdf_url": "https://arxiv.org/pdf/2010.10590v3",
        "similarity_score": 0.5693106651306152
    },
    {
        "arxiv_id": "0802.2428v1",
        "title": "Sign Language Tutoring Tool",
        "abstract": "In this project, we have developed a sign language tutor that lets users learn isolated signs by watching recorded videos and by trying the same signs. The system records the user's video and analyses it. If the sign is recognized, both verbal and animated feedback is given to the user. The system is able to recognize complex signs that involve both hand gestures and head movements and expressions. Our performance tests yield a 99% recognition rate on signs involving only manual gestures and 85% recognition rate on signs that involve both manual and non manual components, such as head movement and facial expressions.",
        "authors": [
            "Oya Aran",
            "Ismail Ari",
            "Alexandre Benoit",
            "Ana Huerta Carrillo",
            "François-Xavier Fanard",
            "Pavel Campr",
            "Lale Akarun",
            "Alice Caplier",
            "Michele Rombaut",
            "Bulent Sankur"
        ],
        "submitted_date": "2008-02-18T07:28:44+00:00",
        "pdf_url": "https://arxiv.org/pdf/0802.2428v1",
        "similarity_score": 0.5675359964370728
    },
    {
        "arxiv_id": "2306.01944v1",
        "title": "EdGCon: Auto-assigner of Iconicity Ratings Grounded by Lexical Properties to Aid in Generation of Technical Gestures",
        "abstract": "Gestures that share similarities in their forms and are related in their meanings, should be easier for learners to recognize and incorporate into their existing lexicon. In that regard, to be more readily accepted as standard by the Deaf and Hard of Hearing community, technical gestures in American Sign Language (ASL) will optimally share similar in forms with their lexical neighbors. We utilize a lexical database of ASL, ASL-LEX, to identify lexical relations within a set of technical gestures. We use automated identification for 3 unique sub-lexical properties in ASL- location, handshape and movement. EdGCon assigned an iconicity rating based on the lexical property similarities of the new gesture with an existing set of technical gestures and the relatedness of the meaning of the new technical word to that of the existing set of technical words. We collected 30 ad hoc crowdsourced technical gestures from different internet websites and tested them against 31 gestures from the DeafTEC technical corpus. We found that EdGCon was able to correctly auto-assign the iconicity ratings 80.76% of the time.",
        "authors": [
            "Sameena Hossain",
            "Payal Kamboj",
            "Aranyak Maity",
            "Tamiko Azuma",
            "Ayan Banerjee",
            "Sandeep K. S. Gupta"
        ],
        "submitted_date": "2023-06-02T23:04:01+00:00",
        "pdf_url": "https://arxiv.org/pdf/2306.01944v1",
        "similarity_score": 0.5675254464149475
    },
    {
        "arxiv_id": "2508.09522v1",
        "title": "Generation of Indian Sign Language Letters, Numbers, and Words",
        "abstract": "Sign language, which contains hand movements, facial expressions and bodily gestures, is a significant medium for communicating with hard-of-hearing people. A well-trained sign language community communicates easily, but those who don't know sign language face significant challenges. Recognition and generation are basic communication methods between hearing and hard-of-hearing individuals. Despite progress in recognition, sign language generation still needs to be explored. The Progressive Growing of Generative Adversarial Network (ProGAN) excels at producing high-quality images, while the Self-Attention Generative Adversarial Network (SAGAN) generates feature-rich images at medium resolutions. Balancing resolution and detail is crucial for sign language image generation. We are developing a Generative Adversarial Network (GAN) variant that combines both models to generate feature-rich, high-resolution, and class-conditional sign language images. Our modified Attention-based model generates high-quality images of Indian Sign Language letters, numbers, and words, outperforming the traditional ProGAN in Inception Score (IS) and Fréchet Inception Distance (FID), with improvements of 3.2 and 30.12, respectively. Additionally, we are publishing a large dataset incorporating high-quality images of Indian Sign Language alphabets, numbers, and 129 words.",
        "authors": [
            "Ajeet Kumar Yadav",
            "Nishant Kumar",
            "Rathna G N"
        ],
        "submitted_date": "2025-08-13T06:10:20+00:00",
        "pdf_url": "https://arxiv.org/pdf/2508.09522v1",
        "similarity_score": 0.5656325817108154
    },
    {
        "arxiv_id": "2504.10822v1",
        "title": "IlluSign: Illustrating Sign Language Videos by Leveraging the Attention Mechanism",
        "abstract": "Sign languages are dynamic visual languages that involve hand gestures, in combination with non manual elements such as facial expressions. While video recordings of sign language are commonly used for education and documentation, the dynamic nature of signs can make it challenging to study them in detail, especially for new learners and educators. This work aims to convert sign language video footage into static illustrations, which serve as an additional educational resource to complement video content. This process is usually done by an artist, and is therefore quite costly. We propose a method that illustrates sign language videos by leveraging generative models' ability to understand both the semantic and geometric aspects of images. Our approach focuses on transferring a sketch like illustration style to video footage of sign language, combining the start and end frames of a sign into a single illustration, and using arrows to highlight the hand's direction and motion. While many style transfer methods address domain adaptation at varying levels of abstraction, applying a sketch like style to sign languages, especially for hand gestures and facial expressions, poses a significant challenge. To tackle this, we intervene in the denoising process of a diffusion model, injecting style as keys and values into high resolution attention layers, and fusing geometric information from the image and edges as queries. For the final illustration, we use the attention mechanism to combine the attention weights from both the start and end illustrations, resulting in a soft combination. Our method offers a cost effective solution for generating sign language illustrations at inference time, addressing the lack of such resources in educational materials.",
        "authors": [
            "Janna Bruner",
            "Amit Moryossef",
            "Lior Wolf"
        ],
        "submitted_date": "2025-04-15T02:53:32+00:00",
        "pdf_url": "https://arxiv.org/pdf/2504.10822v1",
        "similarity_score": 0.5651705265045166
    },
    {
        "arxiv_id": "2105.05222v2",
        "title": "Including Signed Languages in Natural Language Processing",
        "abstract": "Signed languages are the primary means of communication for many deaf and hard of hearing individuals. Since signed languages exhibit all the fundamental linguistic properties of natural language, we believe that tools and theories of Natural Language Processing (NLP) are crucial towards its modeling. However, existing research in Sign Language Processing (SLP) seldom attempt to explore and leverage the linguistic organization of signed languages. This position paper calls on the NLP community to include signed languages as a research area with high social and scientific impact. We first discuss the linguistic properties of signed languages to consider during their modeling. Then, we review the limitations of current SLP models and identify the open challenges to extend NLP to signed languages. Finally, we urge (1) the adoption of an efficient tokenization method; (2) the development of linguistically-informed models; (3) the collection of real-world signed language data; (4) the inclusion of local signed language communities as an active and leading voice in the direction of research.",
        "authors": [
            "Kayo Yin",
            "Amit Moryossef",
            "Julie Hochgesang",
            "Yoav Goldberg",
            "Malihe Alikhani"
        ],
        "submitted_date": "2021-05-11T17:37:55+00:00",
        "pdf_url": "https://arxiv.org/pdf/2105.05222v2",
        "similarity_score": 0.5646384954452515
    },
    {
        "arxiv_id": "2501.11992v3",
        "title": "Survey on Hand Gesture Recognition from Visual Input",
        "abstract": "Hand gesture recognition has become an important research area, driven by the growing demand for human-computer interaction in fields such as sign language recognition, virtual and augmented reality, and robotics. Despite the rapid growth of the field, there are few surveys that comprehensively cover recent research developments, available solutions, and benchmark datasets. This survey addresses this gap by examining the latest advancements in hand gesture and 3D hand pose recognition from various types of camera input data including RGB images, depth images, and videos from monocular or multiview cameras, examining the differing methodological requirements of each approach. Furthermore, an overview of widely used datasets is provided, detailing their main characteristics and application domains. Finally, open challenges such as achieving robust recognition in real-world environments, handling occlusions, ensuring generalization across diverse users, and addressing computational efficiency for real-time applications are highlighted to guide future research directions. By synthesizing the objectives, methodologies, and applications of recent studies, this survey offers valuable insights into current trends, challenges, and opportunities for future research in human hand gesture recognition.",
        "authors": [
            "Manousos Linardakis",
            "Iraklis Varlamis",
            "Georgios Th. Papadopoulos"
        ],
        "submitted_date": "2025-01-21T09:23:22+00:00",
        "pdf_url": "https://arxiv.org/pdf/2501.11992v3",
        "similarity_score": 0.5614507794380188
    },
    {
        "arxiv_id": "2308.09515v1",
        "title": "Learnt Contrastive Concept Embeddings for Sign Recognition",
        "abstract": "In natural language processing (NLP) of spoken languages, word embeddings have been shown to be a useful method to encode the meaning of words. Sign languages are visual languages, which require sign embeddings to capture the visual and linguistic semantics of sign. Unlike many common approaches to Sign Recognition, we focus on explicitly creating sign embeddings that bridge the gap between sign language and spoken language. We propose a learning framework to derive LCC (Learnt Contrastive Concept) embeddings for sign language, a weakly supervised contrastive approach to learning sign embeddings. We train a vocabulary of embeddings that are based on the linguistic labels for sign video. Additionally, we develop a conceptual similarity loss which is able to utilise word embeddings from NLP methods to create sign embeddings that have better sign language to spoken language correspondence. These learnt representations allow the model to automatically localise the sign in time. Our approach achieves state-of-the-art keypoint-based sign recognition performance on the WLASL and BOBSL datasets.",
        "authors": [
            "Ryan Wong",
            "Necati Cihan Camgoz",
            "Richard Bowden"
        ],
        "submitted_date": "2023-08-18T12:47:18+00:00",
        "pdf_url": "https://arxiv.org/pdf/2308.09515v1",
        "similarity_score": 0.5607444643974304
    },
    {
        "arxiv_id": "2507.10972v1",
        "title": "Teach Me Sign: Stepwise Prompting LLM for Sign Language Production",
        "abstract": "Large language models, with their strong reasoning ability and rich knowledge, have brought revolution to many tasks of AI, but their impact on sign language generation remains limited due to its complexity and unique rules. In this paper, we propose TEAch Me Sign (TEAM-Sign), treating sign language as another natural language. By fine-tuning an LLM, we enable it to learn the correspondence between text and sign language, and facilitate generation. Considering the differences between sign and spoken language, we employ a stepwise prompting strategy to extract the inherent sign language knowledge within the LLM, thereby supporting the learning and generation process. Experimental results on How2Sign and Phoenix14T datasets demonstrate that our approach effectively leverages both the sign language knowledge and reasoning capabilities of LLM to align the different distribution and grammatical rules between sign and spoken language.",
        "authors": [
            "Zhaoyi An",
            "Rei Kawakami"
        ],
        "submitted_date": "2025-07-15T04:31:52+00:00",
        "pdf_url": "https://arxiv.org/pdf/2507.10972v1",
        "similarity_score": 0.5594418048858643
    },
    {
        "arxiv_id": "2003.08753v1",
        "title": "FineHand: Learning Hand Shapes for American Sign Language Recognition",
        "abstract": "American Sign Language recognition is a difficult gesture recognition problem, characterized by fast, highly articulate gestures. These are comprised of arm movements with different hand shapes, facial expression and head movements. Among these components, hand shape is the vital, often the most discriminative part of a gesture. In this work, we present an approach for effective learning of hand shape embeddings, which are discriminative for ASL gestures. For hand shape recognition our method uses a mix of manually labelled hand shapes and high confidence predictions to train deep convolutional neural network (CNN). The sequential gesture component is captured by recursive neural network (RNN) trained on the embeddings learned in the first stage. We will demonstrate that higher quality hand shape models can significantly improve the accuracy of final video gesture classification in challenging conditions with variety of speakers, different illumination and significant motion blurr. We compare our model to alternative approaches exploiting different modalities and representations of the data and show improved video gesture recognition accuracy on GMU-ASL51 benchmark dataset",
        "authors": [
            "Al Amin Hosain",
            "Panneer Selvam Santhalingam",
            "Parth Pathak",
            "Huzefa Rangwala",
            "Jana Kosecka"
        ],
        "submitted_date": "2020-03-04T23:32:08+00:00",
        "pdf_url": "https://arxiv.org/pdf/2003.08753v1",
        "similarity_score": 0.5560487508773804
    },
    {
        "arxiv_id": "1201.1652v1",
        "title": "Toward a Motor Theory of Sign Language Perception",
        "abstract": "Researches on signed languages still strongly dissociate lin- guistic issues related on phonological and phonetic aspects, and gesture studies for recognition and synthesis purposes. This paper focuses on the imbrication of motion and meaning for the analysis, synthesis and evaluation of sign language gestures. We discuss the relevance and interest of a motor theory of perception in sign language communication. According to this theory, we consider that linguistic knowledge is mapped on sensory-motor processes, and propose a methodology based on the principle of a synthesis-by-analysis approach, guided by an evaluation process that aims to validate some hypothesis and concepts of this theory. Examples from existing studies illustrate the di erent concepts and provide avenues for future work.",
        "authors": [
            "Sylvie Gibet",
            "Pierre-François Marteau",
            "Kyle Duarte"
        ],
        "submitted_date": "2012-01-08T19:24:08+00:00",
        "pdf_url": "https://arxiv.org/pdf/1201.1652v1",
        "similarity_score": 0.5557987093925476
    },
    {
        "arxiv_id": "2209.14591v1",
        "title": "PerSign: Personalized Bangladeshi Sign Letters Synthesis",
        "abstract": "Bangladeshi Sign Language (BdSL) - like other sign languages - is tough to learn for general people, especially when it comes to expressing letters. In this poster, we propose PerSign, a system that can reproduce a person's image by introducing sign gestures in it. We make this operation personalized, which means the generated image keeps the person's initial image profile - face, skin tone, attire, background - unchanged while altering the hand, palm, and finger positions appropriately. We use an image-to-image translation technique and build a corresponding unique dataset to accomplish the task. We believe the translated image can reduce the communication gap between signers (person who uses sign language) and non-signers without having prior knowledge of BdSL.",
        "authors": [
            "Mohammad Imrul Jubair",
            "Ali Ahnaf",
            "Tashfiq Nahiyan Khan",
            "Ullash Bhattacharjee",
            "Tanjila Joti"
        ],
        "submitted_date": "2022-09-29T07:07:34+00:00",
        "pdf_url": "https://arxiv.org/pdf/2209.14591v1",
        "similarity_score": 0.5541122555732727
    },
    {
        "arxiv_id": "2012.11981v1",
        "title": "Speak with signs: Active learning platform for Greek Sign Language, English Sign Language, and their translation",
        "abstract": "Sign Language is used to facilitate the communication between Deaf and non-Deaf people. It uses signs-words with basic structural elements such as handshape, parts of face, body or space, and the orientation of the fingers-palm. Sign Languages vary from people to people and from country to country and evolve as spoken languages. In the current study, an application which aims at Greek Sign Language and English Sign Language learning by hard of hearing people and talking people, has been developed. The application includes grouped signs in alphabetical order. The user can find Greek Sign Language signs, English sign language signs and translate from Greek sign language to English sign language. The written word of each sign, and the corresponding meaning are displayed. In addition, the sound is activated in order to enable users with partial hearing loss to hear the pronunciation of each word. The user is also provided with various tasks in order to enable an interaction of the knowledge acquired by the user. This interaction is offered mainly by multiplechoice tasks, incorporating text or video. The current application is not a simple sign language dictionary as it provides the interactive participation of users. It is a platform for Greek and English sign language active learning.",
        "authors": [
            "Maria Papatsimouli",
            "Lazaros Lazaridis",
            "Konstantinos-Filippos Kollias",
            "Ioannis Skordas",
            "George F. Fragulis"
        ],
        "submitted_date": "2020-12-22T13:06:02+00:00",
        "pdf_url": "https://arxiv.org/pdf/2012.11981v1",
        "similarity_score": 0.5526231527328491
    },
    {
        "arxiv_id": "2201.07899v1",
        "title": "ASL Video Corpora & Sign Bank: Resources Available through the American Sign Language Linguistic Research Project (ASLLRP)",
        "abstract": "The American Sign Language Linguistic Research Project (ASLLRP) provides Internet access to high-quality ASL video data, generally including front and side views and a close-up of the face. The manual and non-manual components of the signing have been linguistically annotated using SignStream(R). The recently expanded video corpora can be browsed and searched through the Data Access Interface (DAI 2) we have designed; it is possible to carry out complex searches. The data from our corpora can also be downloaded; annotations are available in an XML export format. We have also developed the ASLLRP Sign Bank, which contains almost 6,000 sign entries for lexical signs, with distinct English-based glosses, with a total of 41,830 examples of lexical signs (in addition to about 300 gestures, over 1,000 fingerspelled signs, and 475 classifier examples). The Sign Bank is likewise accessible and searchable on the Internet; it can also be accessed from within SignStream(R) (software to facilitate linguistic annotation and analysis of visual language data) to make annotations more accurate and efficient. Here we describe the available resources. These data have been used for many types of research in linguistics and in computer-based sign language recognition from video; examples of such research are provided in the latter part of this article.",
        "authors": [
            "Carol Neidle",
            "Augustine Opoku",
            "Dimitris Metaxas"
        ],
        "submitted_date": "2022-01-19T22:48:36+00:00",
        "pdf_url": "https://arxiv.org/pdf/2201.07899v1",
        "similarity_score": 0.552212119102478
    },
    {
        "arxiv_id": "1911.09923v1",
        "title": "SWift -- A SignWriting editor to bridge between deaf world and e-learning",
        "abstract": "SWift (SignWriting improved fast transcriber) is an advanced editor for SignWriting (SW). At present, SW is a promising alternative to provide documents in an easy-to-grasp written form of (any) Sign Language, the gestural way of communication which is widely adopted by the deaf community. SWift was developed SW users, either deaf or not, to support collaboration and exchange of ideas. The application allows composing and saving desired signs using elementary components, called glyphs. The procedure that was devised guides and simplifies the editing process. SWift aims at breaking the \"electronic\" barriers that keep the deaf community away from ICT in general, and from e-learning in particular. The editor can be contained in a pluggable module; therefore, it can be integrated everywhere the use of SW is an advisable alternative to written \"verbal\" language, which often hinders information grasping by deaf users.",
        "authors": [
            "Claudia S. Bianchini",
            "Fabrizio Borgia",
            "Maria de Marsico"
        ],
        "submitted_date": "2019-11-22T08:44:23+00:00",
        "pdf_url": "https://arxiv.org/pdf/1911.09923v1",
        "similarity_score": 0.5505804419517517
    },
    {
        "arxiv_id": "2408.09567v1",
        "title": "Enhancing ASL Recognition with GCNs and Successive Residual Connections",
        "abstract": "This study presents a novel approach for enhancing American Sign Language (ASL) recognition using Graph Convolutional Networks (GCNs) integrated with successive residual connections. The method leverages the MediaPipe framework to extract key landmarks from each hand gesture, which are then used to construct graph representations. A robust preprocessing pipeline, including translational and scale normalization techniques, ensures consistency across the dataset. The constructed graphs are fed into a GCN-based neural architecture with residual connections to improve network stability. The architecture achieves state-of-the-art results, demonstrating superior generalization capabilities with a validation accuracy of 99.14%.",
        "authors": [
            "Ushnish Sarkar",
            "Archisman Chakraborti",
            "Tapas Samanta",
            "Sarbajit Pal",
            "Amitabha Das"
        ],
        "submitted_date": "2024-08-18T18:40:30+00:00",
        "pdf_url": "https://arxiv.org/pdf/2408.09567v1",
        "similarity_score": 0.5501571297645569
    },
    {
        "arxiv_id": "2508.17362v2",
        "title": "Virtual Reality in Sign Language Education: Opportunities, Challenges, and the Road Ahead",
        "abstract": "Sign language (SL) is an essential mode of communication for Deaf and Hard-of-Hearing (DHH) individuals. Its education remains limited by the lack of qualified instructors, insufficient early exposure, and the inadequacy of traditional teaching methods. Recent advances in Virtual Reality (VR) and Artificial Intelligence (AI) offer promising new approaches to enhance sign language learning through immersive, interactive, and feedback-rich environments. This paper presents a systematic review of 55 peer-reviewed studies on VR-based sign language education, identifying and analyzing five core thematic areas: (1) gesture recognition and real-time feedback mechanisms; (2) interactive VR environments for communicative practice; (3) gamification for immersive and motivating learning experiences; (4) personalized and adaptive learning systems; and (5) accessibility and inclusivity for diverse DHH learners.   The results reveal that AI-driven gesture recognition systems integrated with VR can provide real-time feedback, significantly improving learner engagement and performance. However, the analysis highlights critical challenges: hardware limitations, inconsistent accuracy in gesture recognition, and a lack of inclusive and adaptive design. This review contributes a comprehensive synthesis of technological and pedagogical innovations in the field, outlining current limitations and proposing actionable recommendations for developers and researchers. By bridging technical advancement with inclusive pedagogy, this review lays the foundation for next-generation VR systems that are equitable, effective, and accessible for sign language learners worldwide.",
        "authors": [
            "Santiago Berrezueta-Guzman",
            "Refia Daya",
            "Stefan Wagner"
        ],
        "submitted_date": "2025-08-24T13:43:25+00:00",
        "pdf_url": "https://arxiv.org/pdf/2508.17362v2",
        "similarity_score": 0.5500594973564148
    },
    {
        "arxiv_id": "1706.07530v1",
        "title": "Multiresolution Match Kernels for Gesture Video Classification",
        "abstract": "The emergence of depth imaging technologies like the Microsoft Kinect has renewed interest in computational methods for gesture classification based on videos. For several years now, researchers have used the Bag-of-Features (BoF) as a primary method for generation of feature vectors from video data for recognition of gestures. However, the BoF method is a coarse representation of the information in a video, which often leads to poor similarity measures between videos. Besides, when features extracted from different spatio-temporal locations in the video are pooled to create histogram vectors in the BoF method, there is an intrinsic loss of their original locations in space and time. In this paper, we propose a new Multiresolution Match Kernel (MMK) for video classification, which can be considered as a generalization of the BoF method. We apply this procedure to hand gesture classification based on RGB-D videos of the American Sign Language(ASL) hand gestures and our results show promise and usefulness of this new method.",
        "authors": [
            "Hemanth Venkateswara",
            "Vineeth N. Balasubramanian",
            "Prasanth Lade",
            "Sethuraman Panchanathan"
        ],
        "submitted_date": "2017-06-23T00:23:32+00:00",
        "pdf_url": "https://arxiv.org/pdf/1706.07530v1",
        "similarity_score": 0.5491613745689392
    },
    {
        "arxiv_id": "1608.02991v1",
        "title": "Albanian Sign Language (AlbSL) Number Recognition from Both Hand's Gestures Acquired by Kinect Sensors",
        "abstract": "Albanian Sign Language (AlbSL) is relatively new and until now there doesn't exist a system that is able to recognize Albanian signs by using natural user interfaces (NUI). The aim of this paper is to present a real-time gesture recognition system that is able to automatically recognize number signs for Albanian Sign Language, captured from signer's both hands. Kinect device is used to obtain data streams. Every pixel generated from Kinect device contains depth data information which is used to construct a depth map. Hands segmentation process is performed by applying a threshold constant to depth map. In order to differentiate signer's hands a K-means clustering algorithm is applied to partition pixels into two groups corresponding to each signer's hands. Centroid distance function is calculated in each hand after extracting hand's contour pixels. Fourier descriptors, derived form centroid distance is used as a hand shape representation. For each number gesture there are 15 Fourier descriptors coefficients generated which represent uniquely that gesture. Every input data is compared against training data set by calculating Euclidean distance, using Fourier coefficients. Sign with the lowest Euclidean distance is considered as a match. The system is able to recognize number signs captured from one hand or both hands. When both signer's hands are used, some of the methodology processes are executed in parallel in order to improve the overall performance. The proposed system achieves an accuracy of 91% and is able to process 55 frames per second.",
        "authors": [
            "Eriglen Gani",
            "Alda Kika"
        ],
        "submitted_date": "2016-08-09T22:14:23+00:00",
        "pdf_url": "https://arxiv.org/pdf/1608.02991v1",
        "similarity_score": 0.5471351146697998
    },
    {
        "arxiv_id": "2312.00392v1",
        "title": "Study and Survey on Gesture Recognition Systems",
        "abstract": "In recent years, there has been a considerable amount of research in the Gesture Recognition domain, mainly owing to the technological advancements in Computer Vision. Various new applications have been conceptualised and developed in this field. This paper discusses the implementation of gesture recognition systems in multiple sectors such as gaming, healthcare, home appliances, industrial robots, and virtual reality. Different methodologies for capturing gestures are compared and contrasted throughout this survey. Various data sources and data acquisition techniques have been discussed. The role of gestures in sign language has been studied and existing approaches have been reviewed. Common challenges faced while building gesture recognition systems have also been explored.",
        "authors": [
            "Kshitij Deshpande",
            "Varad Mashalkar",
            "Kaustubh Mhaisekar",
            "Amaan Naikwadi",
            "Archana Ghotkar"
        ],
        "submitted_date": "2023-12-01T07:29:30+00:00",
        "pdf_url": "https://arxiv.org/pdf/2312.00392v1",
        "similarity_score": 0.5449754595756531
    },
    {
        "arxiv_id": "2003.03703v2",
        "title": "Transferring Cross-domain Knowledge for Video Sign Language Recognition",
        "abstract": "Word-level sign language recognition (WSLR) is a fundamental task in sign language interpretation. It requires models to recognize isolated sign words from videos. However, annotating WSLR data needs expert knowledge, thus limiting WSLR dataset acquisition. On the contrary, there are abundant subtitled sign news videos on the internet. Since these videos have no word-level annotation and exhibit a large domain gap from isolated signs, they cannot be directly used for training WSLR models. We observe that despite the existence of a large domain gap, isolated and news signs share the same visual concepts, such as hand gestures and body movements. Motivated by this observation, we propose a novel method that learns domain-invariant visual concepts and fertilizes WSLR models by transferring knowledge of subtitled news sign to them. To this end, we extract news signs using a base WSLR model, and then design a classifier jointly trained on news and isolated signs to coarsely align these two domain features. In order to learn domain-invariant features within each class and suppress domain-specific features, our method further resorts to an external memory to store the class centroids of the aligned news signs. We then design a temporal attention based on the learnt descriptor to improve recognition performance. Experimental results on standard WSLR datasets show that our method outperforms previous state-of-the-art methods significantly. We also demonstrate the effectiveness of our method on automatically localizing signs from sign news, achieving 28.1 for AP@0.5.",
        "authors": [
            "Dongxu Li",
            "Xin Yu",
            "Chenchen Xu",
            "Lars Petersson",
            "Hongdong Li"
        ],
        "submitted_date": "2020-03-08T03:05:21+00:00",
        "pdf_url": "https://arxiv.org/pdf/2003.03703v2",
        "similarity_score": 0.5422409772872925
    },
    {
        "arxiv_id": "2005.03460v1",
        "title": "Transfer Learning for sEMG-based Hand Gesture Classification using Deep Learning in a Master-Slave Architecture",
        "abstract": "Recent advancements in diagnostic learning and development of gesture-based human machine interfaces have driven surface electromyography (sEMG) towards significant importance. Analysis of hand gestures requires an accurate assessment of sEMG signals. The proposed work presents a novel sequential master-slave architecture consisting of deep neural networks (DNNs) for classification of signs from the Indian sign language using signals recorded from multiple sEMG channels. The performance of the master-slave network is augmented by leveraging additional synthetic feature data generated by long short term memory networks. Performance of the proposed network is compared to that of a conventional DNN prior to and after the addition of synthetic data. Up to 14% improvement is observed in the conventional DNN and up to 9% improvement in master-slave network on addition of synthetic data with an average accuracy value of 93.5% asserting the suitability of the proposed approach.",
        "authors": [
            "Karush Suri",
            "Rinki Gupta"
        ],
        "submitted_date": "2020-04-27T01:16:17+00:00",
        "pdf_url": "https://arxiv.org/pdf/2005.03460v1",
        "similarity_score": 0.5391955971717834
    },
    {
        "arxiv_id": "1811.11997v1",
        "title": "Hand Gesture Detection and Conversion to Speech and Text",
        "abstract": "The hand gestures are one of the typical methods used in sign language. It is very difficult for the hearing-impaired people to communicate with the world. This project presents a solution that will not only automatically recognize the hand gestures but will also convert it into speech and text output so that impaired person can easily communicate with normal people. A camera attached to computer will capture images of hand and the contour feature extraction is used to recognize the hand gestures of the person. Based on the recognized gestures, the recorded soundtrack will be played.",
        "authors": [
            "K. Manikandan",
            "Ayush Patidar",
            "Pallav Walia",
            "Aneek Barman Roy"
        ],
        "submitted_date": "2018-11-29T07:37:07+00:00",
        "pdf_url": "https://arxiv.org/pdf/1811.11997v1",
        "similarity_score": 0.5389212369918823
    },
    {
        "arxiv_id": "2203.05602v1",
        "title": "Gesture based Arabic Sign Language Recognition for Impaired People based on Convolution Neural Network",
        "abstract": "The Arabic Sign Language has endorsed outstanding research achievements for identifying gestures and hand signs using the deep learning methodology. The term \"forms of communication\" refers to the actions used by hearing-impaired people to communicate. These actions are difficult for ordinary people to comprehend. The recognition of Arabic Sign Language (ArSL) has become a difficult study subject due to variations in Arabic Sign Language (ArSL) from one territory to another and then within states. The Convolution Neural Network has been encapsulated in the proposed system which is based on the machine learning technique. For the recognition of the Arabic Sign Language, the wearable sensor is utilized. This approach has been used a different system that could suit all Arabic gestures. This could be used by the impaired people of the local Arabic community. The research method has been used with reasonable and moderate accuracy. A deep Convolutional network is initially developed for feature extraction from the data gathered by the sensing devices. These sensors can reliably recognize the Arabic sign language's 30 hand sign letters. The hand movements in the dataset were captured using DG5-V hand gloves with wearable sensors. For categorization purposes, the CNN technique is used. The suggested system takes Arabic sign language hand gestures as input and outputs vocalized speech as output. The results were recognized by 90% of the people.",
        "authors": [
            "Rady El Rwelli",
            "Osama R. Shahin",
            "Ahmed I. Taloba"
        ],
        "submitted_date": "2022-03-10T19:36:04+00:00",
        "pdf_url": "https://arxiv.org/pdf/2203.05602v1",
        "similarity_score": 0.5382727384567261
    },
    {
        "arxiv_id": "2409.10079v1",
        "title": "Protocol for identifying shared articulatory features of gestures and LSF: application to epistemic gesture",
        "abstract": "This article focuses on the articulatory characteristics of epistemic gestures (i.e., gestures used to express certainty or uncertainty) in co-speech gestures (CSG) in French and in French Sign Language (LSF). It presents a new methodology for analysis, which relies on the complementary use of manual annotation (using Typannot) and semi-automatic annotation (using AlphaPose) to highlight the kinesiological characteristics of these epistemic gestures. The presented methodology allows to analyze the flexion/extension movements of the head in epistemic contexts. The results of this analysis show that in CSG and LSF: (1) head nods passing through the neutral position (i.e., head straight with no flexion/extension) and high movement speed are markers of certainty; and (2) holding the head position away from the neutral position and low movement speed indicate uncertainty. This study is conducted within the framework of the ANR LexiKHuM project, which develops kinesthetic communication solutions for human-machine interaction.",
        "authors": [
            "Fanny Catteau",
            "Claudia S Bianchini"
        ],
        "submitted_date": "2024-09-16T08:32:32+00:00",
        "pdf_url": "https://arxiv.org/pdf/2409.10079v1",
        "similarity_score": 0.5315867066383362
    },
    {
        "arxiv_id": "2406.04024v2",
        "title": "American Sign Language Handshapes Reflect Pressures for Communicative Efficiency",
        "abstract": "Communicative efficiency is a key topic in linguistics and cognitive psychology, with many studies demonstrating how the pressure to communicate with minimal effort guides the form of natural language. However, this phenomenon is rarely explored in signed languages. This paper shows how handshapes in American Sign Language (ASL) reflect these efficiency pressures and provides new evidence of communicative efficiency in the visual-gestural modality.   We focus on hand configurations in native ASL signs and signs borrowed from English to compare efficiency pressures from both ASL and English usage. First, we develop new methodologies to quantify the articulatory effort needed to produce handshapes and the perceptual effort required to recognize them. Then, we analyze correlations between communicative effort and usage statistics in ASL or English. Our findings reveal that frequent ASL handshapes are easier to produce and that pressures for communicative efficiency mostly come from ASL usage, rather than from English lexical borrowing.",
        "authors": [
            "Kayo Yin",
            "Terry Regier",
            "Dan Klein"
        ],
        "submitted_date": "2024-06-06T12:46:21+00:00",
        "pdf_url": "https://arxiv.org/pdf/2406.04024v2",
        "similarity_score": 0.5296610593795776
    },
    {
        "arxiv_id": "1312.4640v1",
        "title": "A Review of Temporal Aspects of Hand Gesture Analysis Applied to Discourse Analysis and Natural Conversation",
        "abstract": "Lately, there has been an increasing interest in hand gesture analysis systems. Recent works have employed pattern recognition techniques and have focused on the development of systems with more natural user interfaces. These systems may use gestures to control interfaces or recognize sign language gestures, which can provide systems with multimodal interaction; or consist in multimodal tools to help psycholinguists to understand new aspects of discourse analysis and to automate laborious tasks. Gestures are characterized by several aspects, mainly by movements and sequence of postures. Since data referring to movements or sequences carry temporal information, this paper presents a literature review about temporal aspects of hand gesture analysis, focusing on applications related to natural conversation and psycholinguistic analysis, using Systematic Literature Review methodology. In our results, we organized works according to type of analysis, methods, highlighting the use of Machine Learning techniques, and applications.",
        "authors": [
            "Renata Cristina Barros Madeo",
            "Priscilla Koch Wagner",
            "Sarajane Marques Peres"
        ],
        "submitted_date": "2013-12-17T05:00:23+00:00",
        "pdf_url": "https://arxiv.org/pdf/1312.4640v1",
        "similarity_score": 0.529109537601471
    },
    {
        "arxiv_id": "1901.05613v1",
        "title": "Hand Sign to Bangla Speech: A Deep Learning in Vision based system for Recognizing Hand Sign Digits and Generating Bangla Speech",
        "abstract": "Recent advancements in the field of computer vision with the help of deep neural networks have led us to explore and develop many existing challenges that were once unattended due to the lack of necessary technologies. Hand Sign/Gesture Recognition is one of the significant areas where the deep neural network is making a substantial impact. In the last few years, a large number of researches has been conducted to recognize hand signs and hand gestures, which we aim to extend to our mother-tongue, Bangla (also known as Bengali). The primary goal of our work is to make an automated tool to aid the people who are unable to speak. We developed a system that automatically detects hand sign based digits and speaks out the result in Bangla language. According to the report of the World Health Organization (WHO), 15% of people in the world live with some kind of disabilities. Among them, individuals with communication impairment such as speech disabilities experience substantial barrier in social interaction. The proposed system can be invaluable to mitigate such a barrier. The core of the system is built with a deep learning model which is based on convolutional neural networks (CNN). The model classifies hand sign based digits with 92% accuracy over validation data which ensures it a highly trustworthy system. Upon classification of the digits, the resulting output is fed to the text to speech engine and the translator unit eventually which generates audio output in Bangla language. A web application to demonstrate our tool is available at http://bit.ly/signdigits2banglaspeech.",
        "authors": [
            "Shahjalal Ahmed",
            "Md. Rafiqul Islam",
            "Jahid Hassan",
            "Minhaz Uddin Ahmed",
            "Bilkis Jamal Ferdosi",
            "Sanjay Saha",
            "Md. Shopon"
        ],
        "submitted_date": "2019-01-17T04:27:34+00:00",
        "pdf_url": "https://arxiv.org/pdf/1901.05613v1",
        "similarity_score": 0.5280286073684692
    },
    {
        "arxiv_id": "2006.00114v1",
        "title": "Design and Implementation of a Virtual 3D Educational Environment to improve Deaf Education",
        "abstract": "Advances in NLP, knowledge representation and computer graphic technologies can provide us insights into the development of educational tool for Deaf people. Actual education materials and tools for deaf pupils present several problems, since textbooks are designed to support normal students in the classroom and most of them are not suitable for people with hearing disabilities. Virtual Reality (VR) technologies appear to be a good tool and a promising framework in the education of pupils with hearing disabilities. In this paper, we present a current research tasks surrounding the design and implementation of a virtual 3D educational environment based on X3D and H-Anim standards. The system generates and animates automatically Sign language sentence from a semantic representation that encode the whole meaning of the Arabic input text. Some aspects and issues in Sign language generation will be discussed, including the model of Sign representation that facilitate reuse and reduces the time of Sign generation, conversion of semantic components to sign features representation with regard to Sign language linguistics characteristics and how to generate realistic smooth gestural sequences using X3D content to performs transition between signs for natural-looking of animated avatar. Sign language sentences were evaluated by Algerian native Deaf people. The goal of the project is the development of a machine translation system from Arabic to Algerian Sign Language that can be used as educational tool for Deaf children in algerian primary schools.",
        "authors": [
            "Abdelaziz Lakhfif"
        ],
        "submitted_date": "2020-05-29T22:56:43+00:00",
        "pdf_url": "https://arxiv.org/pdf/2006.00114v1",
        "similarity_score": 0.5258758068084717
    },
    {
        "arxiv_id": "2208.05318v2",
        "title": "Generative Action Description Prompts for Skeleton-based Action Recognition",
        "abstract": "Skeleton-based action recognition has recently received considerable attention. Current approaches to skeleton-based action recognition are typically formulated as one-hot classification tasks and do not fully exploit the semantic relations between actions. For example, \"make victory sign\" and \"thumb up\" are two actions of hand gestures, whose major difference lies in the movement of hands. This information is agnostic from the categorical one-hot encoding of action classes but could be unveiled from the action description. Therefore, utilizing action description in training could potentially benefit representation learning. In this work, we propose a Generative Action-description Prompts (GAP) approach for skeleton-based action recognition. More specifically, we employ a pre-trained large-scale language model as the knowledge engine to automatically generate text descriptions for body parts movements of actions, and propose a multi-modal training scheme by utilizing the text encoder to generate feature vectors for different body parts and supervise the skeleton encoder for action representation learning. Experiments show that our proposed GAP method achieves noticeable improvements over various baseline models without extra computation cost at inference. GAP achieves new state-of-the-arts on popular skeleton-based action recognition benchmarks, including NTU RGB+D, NTU RGB+D 120 and NW-UCLA. The source code is available at https://github.com/MartinXM/GAP.",
        "authors": [
            "Wangmeng Xiang",
            "Chao Li",
            "Yuxuan Zhou",
            "Biao Wang",
            "Lei Zhang"
        ],
        "submitted_date": "2022-08-10T12:55:56+00:00",
        "pdf_url": "https://arxiv.org/pdf/2208.05318v2",
        "similarity_score": 0.523924708366394
    },
    {
        "arxiv_id": "0912.1830v1",
        "title": "Gesture Recognition with a Focus on Important Actions by Using a Path Searching Method in Weighted Graph",
        "abstract": "This paper proposes a method of gesture recognition with a focus on important actions for distinguishing similar gestures. The method generates a partial action sequence by using optical flow images, expresses the sequence in the eigenspace, and checks the feature vector sequence by applying an optimum path-searching method of weighted graph to focus the important actions. Also presented are the results of an experiment on the recognition of similar sign language words.",
        "authors": [
            "Kazumoto Tanaka"
        ],
        "submitted_date": "2009-12-09T18:41:49+00:00",
        "pdf_url": "https://arxiv.org/pdf/0912.1830v1",
        "similarity_score": 0.5182759761810303
    },
    {
        "arxiv_id": "1911.13231v1",
        "title": "Towards improving the e-learning experience for deaf students: e-LUX",
        "abstract": "Deaf people are more heavily affected by the digital divide than many would expect. Moreover, most accessibility guidelines addressing their needs just deal with captioning and audio-content transcription. However, this approach to the problem does not consider that deaf people have big troubles with vocal languages, even in their written form. At present, only a few organizations, like W3C, produced guidelines dealing with one of their most distinctive expressions: Sign Language (SL). SL is, in fact, the visual-gestural language used by many deaf people to communicate with each other. The present work aims at supporting e-learning user experience (e-LUX) for these specific users by enhancing the accessibility of content and container services. In particular, we propose preliminary solutions to tailor activities which can be more fruitful when performed in one's own \"native\" language, which for most deaf people, especially younger ones, is represented by national SL.",
        "authors": [
            "Fabrizio Borgia",
            "Claudia S. Bianchini",
            "Maria de Marsico"
        ],
        "submitted_date": "2019-11-27T14:55:16+00:00",
        "pdf_url": "https://arxiv.org/pdf/1911.13231v1",
        "similarity_score": 0.5164955854415894
    },
    {
        "arxiv_id": "0912.1768v1",
        "title": "Frequency of Occurrence and Information Entropy of American Sign Language",
        "abstract": "American Sign Language (ASL) uses a series of hand based gestures as a replacement for words to allow the deaf to communicate. Previous work has shown that although it takes longer to make signs than to say the equivalent words, on average sentences can be completed in about the same time. This leaves unresolved, however, precisely why that should be the case. This paper reports a determination of the empirical entropy and redundancy in the set of handshapes of ASL. In this context, the entropy refers to the average information content in a unit of data. It is found that the handshapes, as fundamental units of ASL, are less redundant than phonemes, the equivalent fundamental units of spoken English, and that their entropy is much closer to the maximum possible information content. This explains why the slower signs can produce sentences in the same time as speaking; the low redundancy compensates for the slow rate of sign production. In addition to this precise quantification, this work is also novel in its approach towards quantifying an aspect of the ASL alphabet. Unlike spoken and written languages, frequency analysis of ASL is difficult due to the fact that every sign is composed of phonemes that are created through a combination of manual and a relatively large and imprecise set of bodily features. Focusing on handshapes as the ubiquitous and universal feature of all sign languages permits a precise quantitative analysis. As interest in visual electronic communication explodes within the deaf community, this work also paves the way for more precise automated sign recognition and synthesis.",
        "authors": [
            "Andrew Chong",
            "Lalitha Sankar",
            "H. Vincent Poor"
        ],
        "submitted_date": "2009-12-09T15:07:01+00:00",
        "pdf_url": "https://arxiv.org/pdf/0912.1768v1",
        "similarity_score": 0.4998222589492798
    },
    {
        "arxiv_id": "2007.08847v1",
        "title": "Two-stream Fusion Model for Dynamic Hand Gesture Recognition using 3D-CNN and 2D-CNN Optical Flow guided Motion Template",
        "abstract": "The use of hand gestures can be a useful tool for many applications in the human-computer interaction community. In a broad range of areas hand gesture techniques can be applied specifically in sign language recognition, robotic surgery, etc. In the process of hand gesture recognition, proper detection, and tracking of the moving hand become challenging due to the varied shape and size of the hand. Here the objective is to track the movement of the hand irrespective of the shape, size, and color of the hand. And, for this, a motion template guided by optical flow (OFMT) is proposed. OFMT is a compact representation of the motion information of a gesture encoded into a single image. In the experimentation, different datasets using bare hand with an open palm, and folded palm wearing green-glove are used, and in both cases, we could generate the OFMT images with equal precision. Recently, deep network-based techniques have shown impressive improvements as compared to conventional hand-crafted feature-based techniques. Moreover, in the literature, it is seen that the use of different streams with informative input data helps to increase the performance in the recognition accuracy. This work basically proposes a two-stream fusion model for hand gesture recognition and a compact yet efficient motion template based on optical flow. Specifically, the two-stream network consists of two layers: a 3D convolutional neural network (C3D) that takes gesture videos as input and a 2D-CNN that takes OFMT images as input. C3D has shown its efficiency in capturing spatio-temporal information of a video. Whereas OFMT helps to eliminate irrelevant gestures providing additional motion information. Though each stream can work independently, they are combined with a fusion scheme to boost the recognition results. We have shown the efficiency of the proposed two-stream network on two databases.",
        "authors": [
            "Debajit Sarma",
            "V. Kavyasree",
            "M. K. Bhuyan"
        ],
        "submitted_date": "2020-07-17T09:20:20+00:00",
        "pdf_url": "https://arxiv.org/pdf/2007.08847v1",
        "similarity_score": 0.49673324823379517
    },
    {
        "arxiv_id": "2502.17710v1",
        "title": "Mind the Gesture: Evaluating AI Sensitivity to Culturally Offensive Non-Verbal Gestures",
        "abstract": "Gestures are an integral part of non-verbal communication, with meanings that vary across cultures, and misinterpretations that can have serious social and diplomatic consequences. As AI systems become more integrated into global applications, ensuring they do not inadvertently perpetuate cultural offenses is critical. To this end, we introduce Multi-Cultural Set of Inappropriate Gestures and Nonverbal Signs (MC-SIGNS), a dataset of 288 gesture-country pairs annotated for offensiveness, cultural significance, and contextual factors across 25 gestures and 85 countries. Through systematic evaluation using MC-SIGNS, we uncover critical limitations: text-to-image (T2I) systems exhibit strong US-centric biases, performing better at detecting offensive gestures in US contexts than in non-US ones; large language models (LLMs) tend to over-flag gestures as offensive; and vision-language models (VLMs) default to US-based interpretations when responding to universal concepts like wishing someone luck, frequently suggesting culturally inappropriate gestures. These findings highlight the urgent need for culturally-aware AI safety mechanisms to ensure equitable global deployment of AI technologies.",
        "authors": [
            "Akhila Yerukola",
            "Saadia Gabriel",
            "Nanyun Peng",
            "Maarten Sap"
        ],
        "submitted_date": "2025-02-24T23:10:08+00:00",
        "pdf_url": "https://arxiv.org/pdf/2502.17710v1",
        "similarity_score": 0.49182650446891785
    },
    {
        "arxiv_id": "1408.1759v1",
        "title": "Real-Time and Robust Method for Hand Gesture Recognition System Based on Cross-Correlation Coefficient",
        "abstract": "Hand gesture recognition possesses extensive applications in virtual reality, sign language recognition, and computer games. The direct interface of hand gestures provides us a new way for communicating with the virtual environment. In this paper a novel and real-time approach for hand gesture recognition system is presented. In the suggested method, first, the hand gesture is extracted from the main image by the image segmentation and morphological operation and then is sent to feature extraction stage. In feature extraction stage the Cross-correlation coefficient is applied on the gesture to recognize it. In the result part, the proposed approach is applied on American Sign Language (ASL) database and the accuracy rate obtained 98.34%.",
        "authors": [
            "Reza Azad",
            "Babak Azad",
            "Iman Tavakoli Kazerooni"
        ],
        "submitted_date": "2014-08-08T04:52:59+00:00",
        "pdf_url": "https://arxiv.org/pdf/1408.1759v1",
        "similarity_score": 0.47335952520370483
    },
    {
        "arxiv_id": "2412.00514v1",
        "title": "Alexa, I Wanna See You: Envisioning Smart Home Assistants for the Deaf and Hard-of-Hearing",
        "abstract": "Smart Home Assistants (SHAs) have become ubiquitous in modern households, offering convenience and efficiency through its voice interface. However, for Deaf and Hard-of-Hearing (DHH) individuals, the reliance on auditory and textual feedback through a screen poses significant challenges. Existing solutions primarily focus on sign language input but overlook the need for seamless interaction and feedback modalities. This paper envisions SHAs designed specifically for DHH users, focusing on accessibility and inclusion. We discuss integrating augmented reality (AR) for visual feedback, support for multimodal input, including sign language and gestural commands, and context awareness through sound detection. Our vision highlights the importance of considering the diverse communication needs of the DHH community in developing SHA to ensure equitable access to smart home technology.",
        "authors": [
            "Tyrone Justin Sta. Maria",
            "Jordan Aiko Deja"
        ],
        "submitted_date": "2024-11-30T15:53:04+00:00",
        "pdf_url": "https://arxiv.org/pdf/2412.00514v1",
        "similarity_score": 0.44833990931510925
    },
    {
        "arxiv_id": "2302.11053v1",
        "title": "ChameleonControl: Teleoperating Real Human Surrogates through Mixed Reality Gestural Guidance for Remote Hands-on Classrooms",
        "abstract": "We present ChameleonControl, a real-human teleoperation system for scalable remote instruction in hands-on classrooms. In contrast to existing video or AR/VR-based remote hands-on education, ChameleonControl uses a real human as a surrogate of a remote instructor. Building on existing human-based telepresence approaches, we contribute a novel method to teleoperate a human surrogate through synchronized mixed reality hand gestural navigation and verbal communication. By overlaying the remote instructor's virtual hands in the local user's MR view, the remote instructor can guide and control the local user as if they were physically present. This allows the local user/surrogate to synchronize their hand movements and gestures with the remote instructor, effectively teleoperating a real human. We deploy and evaluate our system in classrooms of physiotherapy training, as well as other application domains such as mechanical assembly, sign language and cooking lessons. The study results confirm that our approach can increase engagement and the sense of co-presence, showing potential for the future of remote hands-on classrooms.",
        "authors": [
            "Mehrad Faridan",
            "Bheesha Kumari",
            "Ryo Suzuki"
        ],
        "submitted_date": "2023-02-21T23:11:41+00:00",
        "pdf_url": "https://arxiv.org/pdf/2302.11053v1",
        "similarity_score": 0.44679757952690125
    },
    {
        "arxiv_id": "1709.08945v1",
        "title": "Gesture-based Human-robot Interaction for Field Programmable Autonomous Underwater Robots",
        "abstract": "The uncertainty and variability of underwater environment propose the request to control underwater robots in real time and dynamically, especially in the scenarios where human and robots need to work collaboratively in the field. However, the underwater environment imposes harsh restrictions on the application of typical control and communication methods. Considering that gestures are a natural and efficient interactive way for human, we, utilizing convolution neural network, implement a real-time gesture-based recognition system, who can recognize 50 kinds of gestures from images captured by one normal monocular camera, and apply this recognition system in human and underwater robot interaction. We design A Flexible and Extendable Interaction Scheme (AFEIS) through which underwater robots can be programmed in situ underwater by human operators using customized gesture-based sign language. This paper elaborates the design of gesture recognition system and AFEIS, and presents our field trial results when applying this system and scheme on underwater robots.",
        "authors": [
            "Pei Xu"
        ],
        "submitted_date": "2017-09-26T11:26:08+00:00",
        "pdf_url": "https://arxiv.org/pdf/1709.08945v1",
        "similarity_score": 0.4163152575492859
    },
    {
        "arxiv_id": "2508.05358v1",
        "title": "Evaluation of a Sign Language Avatar on Comprehensibility, User Experience \\& Acceptability",
        "abstract": "This paper presents an investigation into the impact of adding adjustment features to an existing sign language (SL) avatar on a Microsoft Hololens 2 device. Through a detailed analysis of interactions of expert German Sign Language (DGS) users with both adjustable and non-adjustable avatars in a specific use case, this study identifies the key factors influencing the comprehensibility, the user experience (UX), and the acceptability of such a system. Despite user preference for adjustable settings, no significant improvements in UX or comprehensibility were observed, which remained at low levels, amid missing SL elements (mouthings and facial expressions) and implementation issues (indistinct hand shapes, lack of feedback and menu positioning). Hedonic quality was rated higher than pragmatic quality, indicating that users found the system more emotionally or aesthetically pleasing than functionally useful. Stress levels were higher for the adjustable avatar, reflecting lower performance, greater effort and more frustration. Additionally, concerns were raised about whether the Hololens adjustment gestures are intuitive and easy to familiarise oneself with. While acceptability of the concept of adjustability was generally positive, it was strongly dependent on usability and animation quality. This study highlights that personalisation alone is insufficient, and that SL avatars must be comprehensible by default. Key recommendations include enhancing mouthing and facial animation, improving interaction interfaces, and applying participatory design.",
        "authors": [
            "Fenya Wasserroth",
            "Eleftherios Avramidis",
            "Vera Czehmann",
            "Tanja Kojic",
            "Fabrizio Nunnari",
            "Sebastian Möller"
        ],
        "submitted_date": "2025-08-07T13:06:42+00:00",
        "pdf_url": "https://arxiv.org/pdf/2508.05358v1",
        "similarity_score": 0.4112042188644409
    },
    {
        "arxiv_id": "2207.12866v1",
        "title": "Implementation Of Tiny Machine Learning Models On Arduino 33 BLE For Gesture And Speech Recognition",
        "abstract": "In this article gesture recognition and speech recognition applications are implemented on embedded systems with Tiny Machine Learning (TinyML). It features 3-axis accelerometer, 3-axis gyroscope and 3-axis magnetometer. The gesture recognition,provides an innovative approach nonverbal communication. It has wide applications in human-computer interaction and sign language. Here in the implementation of hand gesture recognition, TinyML model is trained and deployed from EdgeImpulse framework for hand gesture recognition and based on the hand movements, Arduino Nano 33 BLE device having 6-axis IMU can find out the direction of movement of hand. The Speech is a mode of communication. Speech recognition is a way by which the statements or commands of human speech is understood by the computer which reacts accordingly. The main aim of speech recognition is to achieve communication between man and machine. Here in the implementation of speech recognition, TinyML model is trained and deployed from EdgeImpulse framework for speech recognition and based on the keywords pronounced by human, Arduino Nano 33 BLE device having built-in microphone can make an RGB LED glow like red, green or blue based on keyword pronounced. The results of each application are obtained and listed in the results section and given the analysis upon the results.",
        "authors": [
            "Viswanatha V",
            "Ramachandra A. C",
            "Raghavendra Prasanna",
            "Prem Chowdary Kakarla",
            "Viveka Simha PJ",
            "Nishant Mohan"
        ],
        "submitted_date": "2022-07-23T10:53:26+00:00",
        "pdf_url": "https://arxiv.org/pdf/2207.12866v1",
        "similarity_score": 0.4055553078651428
    },
    {
        "arxiv_id": "1408.1549v1",
        "title": "Real-Time Human-Computer Interaction Based on Face and Hand Gesture Recognition",
        "abstract": "At the present time, hand gestures recognition system could be used as a more expected and useable approach for human computer interaction. Automatic hand gesture recognition system provides us a new tactic for interactive with the virtual environment. In this paper, a face and hand gesture recognition system which is able to control computer media player is offered. Hand gesture and human face are the key element to interact with the smart system. We used the face recognition scheme for viewer verification and the hand gesture recognition in mechanism of computer media player, for instance, volume down/up, next music and etc. In the proposed technique, first, the hand gesture and face location is extracted from the main image by combination of skin and cascade detector and then is sent to recognition stage. In recognition stage, first, the threshold condition is inspected then the extracted face and gesture will be recognized. In the result stage, the proposed technique is applied on the video dataset and the high precision ratio acquired. Additional the recommended hand gesture recognition method is applied on static American Sign Language (ASL) database and the correctness rate achieved nearby 99.40%. also the planned method could be used in gesture based computer games and virtual reality.",
        "authors": [
            "Reza Azad",
            "Babak Azad",
            "Nabil Belhaj Khalifa",
            "Shahram Jamali"
        ],
        "submitted_date": "2014-08-07T11:38:20+00:00",
        "pdf_url": "https://arxiv.org/pdf/1408.1549v1",
        "similarity_score": 0.39203768968582153
    },
    {
        "arxiv_id": "2503.14901v1",
        "title": "A Cost Effective Deaf-mute Electronic Assistant System Using Myo Armband and Smartphone",
        "abstract": "Communication is essential feature in human communities. For some reasons, deaf-mute disabled people lose their ability to hear, speak, or both which makes them suffer to communicate and convey their ideas, especially with normal people. Sign language is the solution for communication in the deaf-mute societies, but it is difficult for the rest of people to understand. Therefore, in this work, a cost effective Deaf-Mute Electronic Assistant System (DMEAS) has been developed to help in solving the communication problem between the deaf-mute people and the normal people. The system hardware consists only of a Myo armband from \\textit{Thalmic Labs} and a smartphone. The Myo armband reads the electromyographic signals of the muscles of the disabled person's forearm through non-invasive, surface mounted electrodes (sEMG) and sends the data directly to the smartphone via Bluetooth. The smartphone will recognize and interpret them to a predefined word depending on the hand gestures. The recognized gesture will be displayed on the smartphone screen as well as displaying the text of the word related to the gesture and its voice record. All the EMG signals are processed using discrete wavelet transform and classified by neural network classifier. The system was extensively tested through experiments by normal subjects to prove its functionality.",
        "authors": [
            "Hussein Naeem Hasan"
        ],
        "submitted_date": "2025-03-19T04:58:50+00:00",
        "pdf_url": "https://arxiv.org/pdf/2503.14901v1",
        "similarity_score": 0.38765740394592285
    },
    {
        "arxiv_id": "2009.13322v1",
        "title": "Lightweight assistive technology: A wearable, optical-fiber gesture recognition system",
        "abstract": "The goal of this project is to create an inexpensive, lightweight, wearable assistive device that can measure hand or finger movements accurately enough to identify a range of hand gestures. One eventual application is to provide assistive technology and sign language detection for the hearing impaired. My system, called LiTe (Light-based Technology), uses optical fibers embedded into a wristband. The wrist is an optimal place for the band since the light propagation in the optical fibers is impacted even by the slight movements of the tendons in the wrist when gestures are performed. The prototype incorporates light dependent resistors to measure these light propagation changes. When creating LiTe, I considered a variety of fiber materials, light frequencies, and physical shapes to optimize the tendon movement detection so that it can be accurately correlated with different gestures. I implemented and evaluated two approaches for gesture recognition. The first uses an algorithm that combines moving averages of sensor readings with gesture sensor reading signatures to determine the current gesture. The second uses a neural network trained on a labelled set of gesture readings to recognize gestures. Using the signature-based approach, I was able to achieve a 99.8% accuracy at recognizing distinct gestures. Using the neural network the recognition accuracy was 98.8%. This shows that high accuracy is feasible using both approaches. The results indicate that this novel method of using fiber optics-based sensors is a promising first step to creating a gesture recognition system.",
        "authors": [
            "Sanjay Seshan"
        ],
        "submitted_date": "2020-09-11T01:36:44+00:00",
        "pdf_url": "https://arxiv.org/pdf/2009.13322v1",
        "similarity_score": 0.3876403570175171
    },
    {
        "arxiv_id": "2005.00107v1",
        "title": "Activity Detection from Wearable Electromyogram Sensors using Hidden Markov Model",
        "abstract": "Surface electromyography (sEMG) has gained significant importance during recent advancements in consumer electronics for healthcare systems, gesture analysis and recognition and sign language communication. For such a system, it is imperative to determine the regions of activity in a continuously recorded sEMG signal. The proposed work provides a novel activity detection approach based on Hidden Markov Models (HMM) using sEMG signals recorded when various hand gestures are performed. Detection procedure is designed based on a probabilistic outlook by making use of mathematical models. The requirement of a threshold for activity detection is obviated making it subject and activity independent. Correctness of the predicted outputs is asserted by classifying the signal segments around the detected transition regions as activity or rest. Classified outputs are compared with the transition regions in a stimulus given to the subject to perform the activity. The activity onsets are detected with an average of 96.25% accuracy whereas the activity termination regions with an average of 87.5% accuracy with the considered set of six activities and four subjects.",
        "authors": [
            "Rinki Gupta",
            "Karush Suri"
        ],
        "submitted_date": "2020-04-27T01:14:02+00:00",
        "pdf_url": "https://arxiv.org/pdf/2005.00107v1",
        "similarity_score": 0.37021785974502563
    },
    {
        "arxiv_id": "2208.10384v5",
        "title": "The optimality of word lengths. Theoretical foundations and an empirical study",
        "abstract": "Zipf's law of abbreviation, namely the tendency of more frequent words to be shorter, has been viewed as a manifestation of compression, i.e. the minimization of the length of forms -- a universal principle of natural communication. Although the claim that languages are optimized has become trendy, attempts to measure the degree of optimization of languages have been rather scarce. Here we present two optimality scores that are dualy normalized, namely, they are normalized with respect to both the minimum and the random baseline. We analyze the theoretical and statistical pros and cons of these and other scores. Harnessing the best score, we quantify for the first time the degree of optimality of word lengths in languages. This indicates that languages are optimized to 62 or 67 percent on average (depending on the source) when word lengths are measured in characters, and to 65 percent on average when word lengths are measured in time. In general, spoken word durations are more optimized than written word lengths in characters. Our work paves the way to measure the degree of optimality of the vocalizations or gestures of other species, and to compare them against written, spoken, or signed human languages.",
        "authors": [
            "Sonia Petrini",
            "Antoni Casas-i-Muñoz",
            "Jordi Cluet-i-Martinell",
            "Mengxue Wang",
            "Christian Bentz",
            "Ramon Ferrer-i-Cancho"
        ],
        "submitted_date": "2022-08-22T15:03:31+00:00",
        "pdf_url": "https://arxiv.org/pdf/2208.10384v5",
        "similarity_score": 0.3132895827293396
    }
]