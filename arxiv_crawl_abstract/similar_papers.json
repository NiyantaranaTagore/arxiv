[
    {
        "arxiv_id": "2512.04062v1",
        "title": "Eval Factsheets: A Structured Framework for Documenting AI Evaluations",
        "abstract": "The rapid proliferation of benchmarks has created significant challenges in reproducibility, transparency, and informed decision-making. However, unlike datasets and models -- which benefit from structured documentation frameworks like Datasheets and Model Cards -- evaluation methodologies lack systematic documentation standards. We introduce Eval Factsheets, a structured, descriptive framework for documenting AI system evaluations through a comprehensive taxonomy and questionnaire-based approach. Our framework organizes evaluation characteristics across five fundamental dimensions: Context (Who made the evaluation and when?), Scope (What does it evaluate?), Structure (With what the evaluation is built?), Method (How does it work?) and Alignment (In what ways is it reliable/valid/robust?). We implement this taxonomy as a practical questionnaire spanning five sections with mandatory and recommended documentation elements. Through case studies on multiple benchmarks, we demonstrate that Eval Factsheets effectively captures diverse evaluation paradigms -- from traditional benchmarks to LLM-as-judge methodologies -- while maintaining consistency and comparability. We hope Eval Factsheets are incorporated into both existing and newly released evaluation frameworks and lead to more transparency and reproducibility.",
        "authors": [
            "Florian Bordes",
            "Candace Ross",
            "Justine T Kao",
            "Evangelia Spiliopoulou",
            "Adina Williams"
        ],
        "submitted_date": "2025-12-03T18:46:50+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.04062v1",
        "similarity_score": 0.9485751390457153
    },
    {
        "arxiv_id": "2510.19864v1",
        "title": "SODBench: A Large Language Model Approach to Documenting Spreadsheet Operations",
        "abstract": "Numerous knowledge workers utilize spreadsheets in business, accounting, and finance. However, a lack of systematic documentation methods for spreadsheets hinders automation, collaboration, and knowledge transfer, which risks the loss of crucial institutional knowledge. This paper introduces Spreadsheet Operations Documentation (SOD), an AI task that involves generating human-readable explanations from spreadsheet operations. Many previous studies have utilized Large Language Models (LLMs) for generating spreadsheet manipulation code; however, translating that code into natural language for SOD is a less-explored area. To address this, we present a benchmark of 111 spreadsheet manipulation code snippets, each paired with a corresponding natural language summary. We evaluate five LLMs, GPT-4o, GPT-4o-mini, LLaMA-3.3-70B, Mixtral-8x7B, and Gemma2-9B, using BLEU, GLEU, ROUGE-L, and METEOR metrics. Our findings suggest that LLMs can generate accurate spreadsheet documentation, making SOD a feasible prerequisite step toward enhancing reproducibility, maintainability, and collaborative workflows in spreadsheets, although there are challenges that need to be addressed.",
        "authors": [
            "Amila Indika",
            "Igor Molybog"
        ],
        "submitted_date": "2025-10-22T01:36:13+00:00",
        "pdf_url": "https://arxiv.org/pdf/2510.19864v1",
        "similarity_score": 0.6944026947021484
    },
    {
        "arxiv_id": "2303.10854v1",
        "title": "Dynamic Documentation for AI Systems",
        "abstract": "AI documentation is a rapidly-growing channel for coordinating the design of AI technologies with policies for transparency and accessibility. Calls to standardize and enact documentation of algorithmic harms and impacts are now commonplace. However, documentation standards for AI remain inchoate, and fail to match the capabilities and social effects of increasingly impactful architectures such as Large Language Models (LLMs). In this paper, we show the limits of present documentation protocols, and argue for dynamic documentation as a new paradigm for understanding and evaluating AI systems. We first review canonical approaches to system documentation outside the context of AI, focusing on the complex history of Environmental Impact Statements (EISs). We next compare critical elements of the EIS framework to present challenges with algorithmic documentation, which have inherited the limitations of EISs without incorporating their strengths. These challenges are specifically illustrated through the growing popularity of Model Cards and two case studies of algorithmic impact assessment in China and Canada. Finally, we evaluate more recent proposals, including Reward Reports, as potential components of fully dynamic AI documentation protocols.",
        "authors": [
            "Soham Mehta",
            "Anderson Rogers",
            "Thomas Krendl Gilbert"
        ],
        "submitted_date": "2023-03-20T04:23:07+00:00",
        "pdf_url": "https://arxiv.org/pdf/2303.10854v1",
        "similarity_score": 0.6930580139160156
    },
    {
        "arxiv_id": "2206.02923v2",
        "title": "Understanding Machine Learning Practitioners' Data Documentation Perceptions, Needs, Challenges, and Desiderata",
        "abstract": "Data is central to the development and evaluation of machine learning (ML) models. However, the use of problematic or inappropriate datasets can result in harms when the resulting models are deployed. To encourage responsible AI practice through more deliberate reflection on datasets and transparency around the processes by which they are created, researchers and practitioners have begun to advocate for increased data documentation and have proposed several data documentation frameworks. However, there is little research on whether these data documentation frameworks meet the needs of ML practitioners, who both create and consume datasets. To address this gap, we set out to understand ML practitioners' data documentation perceptions, needs, challenges, and desiderata, with the goal of deriving design requirements that can inform future data documentation frameworks. We conducted a series of semi-structured interviews with 14 ML practitioners at a single large, international technology company. We had them answer a list of questions taken from datasheets for datasets (Gebru, 2021). Our findings show that current approaches to data documentation are largely ad hoc and myopic in nature. Participants expressed needs for data documentation frameworks to be adaptable to their contexts, integrated into their existing tools and workflows, and automated wherever possible. Despite the fact that data documentation frameworks are often motivated from the perspective of responsible AI, participants did not make the connection between the questions that they were asked to answer and their responsible AI implications. In addition, participants often had difficulties prioritizing the needs of dataset consumers and providing information that someone unfamiliar with their datasets might need to know. Based on these findings, we derive seven design requirements for future data documentation frameworks.",
        "authors": [
            "Amy K. Heger",
            "Liz B. Marquis",
            "Mihaela Vorvoreanu",
            "Hanna Wallach",
            "Jennifer Wortman Vaughan"
        ],
        "submitted_date": "2022-06-06T21:55:39+00:00",
        "pdf_url": "https://arxiv.org/pdf/2206.02923v2",
        "similarity_score": 0.6786683797836304
    },
    {
        "arxiv_id": "2511.05496v1",
        "title": "DOCUEVAL: An LLM-based AI Engineering Tool for Building Customisable Document Evaluation Workflows",
        "abstract": "Foundation models, such as large language models (LLMs), have the potential to streamline evaluation workflows and improve their performance. However, practical adoption faces challenges, such as customisability, accuracy, and scalability. In this paper, we present DOCUEVAL, an AI engineering tool for building customisable DOCUment EVALuation workflows. DOCUEVAL supports advanced document processing and customisable workflow design which allow users to define theory-grounded reviewer roles, specify evaluation criteria, experiment with different reasoning strategies and choose the assessment style. To ensure traceability, DOCUEVAL provides comprehensive logging of every run, along with source attribution and configuration management, allowing systematic comparison of results across alternative setups. By integrating these capabilities, DOCUEVAL directly addresses core software engineering challenges, including how to determine whether evaluators are \"good enough\" for deployment and how to empirically compare different evaluation strategies. We demonstrate the usefulness of DOCUEVAL through a real-world academic peer review case, showing how DOCUEVAL enables both the engineering of evaluators and scalable, reliable document evaluation.",
        "authors": [
            "Hao Zhang",
            "Qinghua Lu",
            "Liming Zhu"
        ],
        "submitted_date": "2025-09-12T08:09:09+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.05496v1",
        "similarity_score": 0.6778223514556885
    },
    {
        "arxiv_id": "2510.11143v1",
        "title": "Spec-Driven AI for Science: The ARIA Framework for Automated and Reproducible Data Analysis",
        "abstract": "The rapid expansion of scientific data has widened the gap between analytical capability and research intent. Existing AI-based analysis tools, ranging from AutoML frameworks to agentic research assistants, either favor automation over transparency or depend on manual scripting that hinders scalability and reproducibility. We present ARIA (Automated Research Intelligence Assistant), a spec-driven, human-in-the-loop framework for automated and interpretable data analysis. ARIA integrates six interoperable layers, namely Command, Context, Code, Data, Orchestration, and AI Module, within a document-centric workflow that unifies human reasoning and machine execution. Through natural-language specifications, researchers define analytical goals while ARIA autonomously generates executable code, validates computations, and produces transparent documentation. Beyond achieving high predictive accuracy, ARIA can rapidly identify optimal feature sets and select suitable models, minimizing redundant tuning and repetitive experimentation. In the Boston Housing case, ARIA discovered 25 key features and determined XGBoost as the best performing model (R square = 0.93) with minimal overfitting. Evaluations across heterogeneous domains demonstrate ARIA's strong performance, interpretability, and efficiency compared with state-of-the-art systems. By combining AI for research and AI for science principles within a spec-driven architecture, ARIA establishes a new paradigm for transparent, collaborative, and reproducible scientific discovery.",
        "authors": [
            "Chuke Chen",
            "Biao Luo",
            "Nan Li",
            "Boxiang Wang",
            "Hang Yang",
            "Jing Guo",
            "Ming Xu"
        ],
        "submitted_date": "2025-10-13T08:32:43+00:00",
        "pdf_url": "https://arxiv.org/pdf/2510.11143v1",
        "similarity_score": 0.6613978743553162
    },
    {
        "arxiv_id": "2102.12592v5",
        "title": "Documentation Matters: Human-Centered AI System to Assist Data Science Code Documentation in Computational Notebooks",
        "abstract": "Computational notebooks allow data scientists to express their ideas through a combination of code and documentation. However, data scientists often pay attention only to the code, and neglect creating or updating their documentation during quick iterations. Inspired by human documentation practices learned from 80 highly-voted Kaggle notebooks, we design and implement Themisto, an automated documentation generation system to explore how human-centered AI systems can support human data scientists in the machine learning code documentation scenario. Themisto facilitates the creation of documentation via three approaches: a deep-learning-based approach to generate documentation for source code, a query-based approach to retrieve online API documentation for source code, and a user prompt approach to nudge users to write documentation. We evaluated Themisto in a within-subjects experiment with 24 data science practitioners, and found that automated documentation generation techniques reduced the time for writing documentation, reminded participants to document code they would have ignored, and improved participants' satisfaction with their computational notebook.",
        "authors": [
            "April Yi Wang",
            "Dakuo Wang",
            "Jaimie Drozdal",
            "Michael Muller",
            "Soya Park",
            "Justin D. Weisz",
            "Xuye Liu",
            "Lingfei Wu",
            "Casey Dugan"
        ],
        "submitted_date": "2021-02-24T22:46:16+00:00",
        "pdf_url": "https://arxiv.org/pdf/2102.12592v5",
        "similarity_score": 0.6481649875640869
    },
    {
        "arxiv_id": "2508.08804v1",
        "title": "TechOps: Technical Documentation Templates for the AI Act",
        "abstract": "Operationalizing the EU AI Act requires clear technical documentation to ensure AI systems are transparent, traceable, and accountable. Existing documentation templates for AI systems do not fully cover the entire AI lifecycle while meeting the technical documentation requirements of the AI Act.   This paper addresses those shortcomings by introducing open-source templates and examples for documenting data, models, and applications to provide sufficient documentation for certifying compliance with the AI Act. These templates track the system status over the entire AI lifecycle, ensuring traceability, reproducibility, and compliance with the AI Act. They also promote discoverability and collaboration, reduce risks, and align with best practices in AI documentation and governance.   The templates are evaluated and refined based on user feedback to enable insights into their usability and implementability. We then validate the approach on real-world scenarios, providing examples that further guide their implementation: the data template is followed to document a skin tones dataset created to support fairness evaluations of downstream computer vision models and human-centric applications; the model template is followed to document a neural network for segmenting human silhouettes in photos. The application template is tested on a system deployed for construction site safety using real-time video analytics and sensor data. Our results show that TechOps can serve as a practical tool to enable oversight for regulatory compliance and responsible AI development.",
        "authors": [
            "Laura Lucaj",
            "Alex Loosley",
            "Hakan Jonsson",
            "Urs Gasser",
            "Patrick van der Smagt"
        ],
        "submitted_date": "2025-08-12T09:58:33+00:00",
        "pdf_url": "https://arxiv.org/pdf/2508.08804v1",
        "similarity_score": 0.6421175003051758
    },
    {
        "arxiv_id": "2510.06989v2",
        "title": "Human-aligned AI Model Cards with Weighted Hierarchy Architecture",
        "abstract": "The proliferation of Large Language Models (LLMs) has led to a burgeoning ecosystem of specialized, domain-specific models. While this rapid growth accelerates innovation, it has simultaneously created significant challenges in model discovery and adoption. Users struggle to navigate this landscape due to inconsistent, incomplete, and imbalanced documentation across platforms. Existing documentation frameworks, such as Model Cards and FactSheets, attempt to standardize reporting but are often static, predominantly qualitative, and lack the quantitative mechanisms needed for rigorous cross-model comparison. This gap exacerbates model underutilization and hinders responsible adoption. To address these shortcomings, we introduce the Comprehensive Responsible AI Model Card Framework (CRAI-MCF), a novel approach that transitions from static disclosures to actionable, human-aligned documentation. Grounded in Value Sensitive Design (VSD), CRAI-MCF is built upon an empirical analysis of 240 open-source projects, distilling 217 parameters into an eight-module, value-aligned architecture. Our framework introduces a quantitative sufficiency criterion to operationalize evaluation and enables rigorous cross-model comparison under a unified scheme. By balancing technical, ethical, and operational dimensions, CRAI-MCF empowers practitioners to efficiently assess, select, and adopt LLMs with greater confidence and operational integrity.",
        "authors": [
            "Pengyue Yang",
            "Haolin Jin",
            "Qingwen Zeng",
            "Jiawen Wen",
            "Harry Rao",
            "Huaming Chen"
        ],
        "submitted_date": "2025-10-08T13:13:18+00:00",
        "pdf_url": "https://arxiv.org/pdf/2510.06989v2",
        "similarity_score": 0.6372234225273132
    },
    {
        "arxiv_id": "2410.23902v1",
        "title": "Responsible Retrieval Augmented Generation for Climate Decision Making from Documents",
        "abstract": "Climate decision making is constrained by the complexity and inaccessibility of key information within lengthy, technical, and multi-lingual documents. Generative AI technologies offer a promising route for improving the accessibility of information contained within these documents, but suffer from limitations. These include (1) a tendency to hallucinate or mis-represent information, (2) difficulty in steering or guaranteeing properties of generated output, and (3) reduced performance in specific technical domains. To address these challenges, we introduce a novel evaluation framework with domain-specific dimensions tailored for climate-related documents. We then apply this framework to evaluate Retrieval-Augmented Generation (RAG) approaches and assess retrieval- and generation-quality within a prototype tool that answers questions about individual climate law and policy documents. In addition, we publish a human-annotated dataset and scalable automated evaluation tools, with the aim of facilitating broader adoption and robust assessment of these systems in the climate domain. Our findings highlight the key components of responsible deployment of RAG to enhance decision-making, while also providing insights into user experience (UX) considerations for safely deploying such systems to build trust with users in high-risk domains.",
        "authors": [
            "Matyas Juhasz",
            "Kalyan Dutia",
            "Henry Franks",
            "Conor Delahunty",
            "Patrick Fawbert Mills",
            "Harrison Pim"
        ],
        "submitted_date": "2024-10-31T13:05:39+00:00",
        "pdf_url": "https://arxiv.org/pdf/2410.23902v1",
        "similarity_score": 0.6290520429611206
    },
    {
        "arxiv_id": "2406.08446v2",
        "title": "OLMES: A Standard for Language Model Evaluations",
        "abstract": "Progress in AI is often demonstrated by new models claiming improved performance on tasks measuring model capabilities. Evaluating language models can be particularly challenging, as choices of how a model is evaluated on a task can lead to large changes in measured performance. There is no common standard setup, so different models are evaluated on the same tasks in different ways, leading to claims about which models perform best not being reproducible. We propose OLMES, a completely documented, practical, open standard for reproducible LLM evaluations. In developing this standard, we identify and review the varying factors in evaluation practices adopted by the community - such as details of prompt formatting, choice of in-context examples, probability normalizations, and task formulation. In particular, OLMES supports meaningful comparisons between smaller base models that require the unnatural \"cloze\" formulation of multiple-choice questions against larger models that can utilize the original formulation. OLMES includes well-considered, documented recommendations guided by results from existing literature as well as new experiments resolving open questions.",
        "authors": [
            "Yuling Gu",
            "Oyvind Tafjord",
            "Bailey Kuehl",
            "Dany Haddad",
            "Jesse Dodge",
            "Hannaneh Hajishirzi"
        ],
        "submitted_date": "2024-06-12T17:37:09+00:00",
        "pdf_url": "https://arxiv.org/pdf/2406.08446v2",
        "similarity_score": 0.6277803182601929
    },
    {
        "arxiv_id": "2508.00630v2",
        "title": "MCeT: Behavioral Model Correctness Evaluation using Large Language Models",
        "abstract": "Behavioral model diagrams, e.g., sequence diagrams, are an essential form of documentation that are typically designed by system engineers from requirements documentation, either fully manually or assisted by design tools. With the growing use of Large Language Models (LLM) as AI modeling assistants, more automation will be involved in generating diagrams. This necessitates the advancement of automatic model correctness evaluation tools. Such a tool can be used to evaluate both manually and AI automatically generated models; to provide feedback to system engineers, and enable AI assistants to self-evaluate and self-enhance their generated models.   In this paper, we propose MCeT, the first fully automated tool to evaluate the correctness of a behavioral model, sequence diagrams in particular, against its corresponding requirements text and produce a list of issues that the model has. We utilize LLMs for the correctness evaluation tasks as they have shown outstanding natural language understanding ability. However, we show that directly asking an LLM to compare a diagram to requirements finds less than 35% of issues that experienced engineers can find. We propose to supplement the direct check with a fine-grained, multi-perspective approach; we split the diagram into atomic, non-divisible interactions, and split the requirements text into atomic, self-contained items. We compare the diagram with atomic requirements and each diagram-atom with the requirements. We also propose a self-consistency checking approach that combines perspectives to mitigate LLM hallucinated issues. Our combined approach improves upon the precision of the direct approach from 0.58 to 0.81 in a dataset of real requirements. Moreover, the approach finds 90% more issues that the experienced engineers found than the direct approach, and reports an average of 6 new issues per diagram.",
        "authors": [
            "Khaled Ahmed",
            "Jialing Song",
            "Boqi Chen",
            "Ou Wei",
            "Bingzhou Zheng"
        ],
        "submitted_date": "2025-08-01T13:41:58+00:00",
        "pdf_url": "https://arxiv.org/pdf/2508.00630v2",
        "similarity_score": 0.6259095668792725
    },
    {
        "arxiv_id": "2201.13224v2",
        "title": "Evaluating a Methodology for Increasing AI Transparency: A Case Study",
        "abstract": "In reaction to growing concerns about the potential harms of artificial intelligence (AI), societies have begun to demand more transparency about how AI models and systems are created and used. To address these concerns, several efforts have proposed documentation templates containing questions to be answered by model developers. These templates provide a useful starting point, but no single template can cover the needs of diverse documentation consumers. It is possible in principle, however, to create a repeatable methodology to generate truly useful documentation. Richards et al. [25] proposed such a methodology for identifying specific documentation needs and creating templates to address those needs. Although this is a promising proposal, it has not been evaluated.   This paper presents the first evaluation of this user-centered methodology in practice, reporting on the experiences of a team in the domain of AI for healthcare that adopted it to increase transparency for several AI models. The methodology was found to be usable by developers not trained in user-centered techniques, guiding them to creating a documentation template that addressed the specific needs of their consumers while still being reusable across different models and use cases. Analysis of the benefits and costs of this methodology are reviewed and suggestions for further improvement in both the methodology and supporting tools are summarized.",
        "authors": [
            "David Piorkowski",
            "John Richards",
            "Michael Hind"
        ],
        "submitted_date": "2022-01-24T20:01:01+00:00",
        "pdf_url": "https://arxiv.org/pdf/2201.13224v2",
        "similarity_score": 0.6233620643615723
    },
    {
        "arxiv_id": "2409.19019v1",
        "title": "RAGProbe: An Automated Approach for Evaluating RAG Applications",
        "abstract": "Retrieval Augmented Generation (RAG) is increasingly being used when building Generative AI applications. Evaluating these applications and RAG pipelines is mostly done manually, via a trial and error process. Automating evaluation of RAG pipelines requires overcoming challenges such as context misunderstanding, wrong format, incorrect specificity, and missing content. Prior works therefore focused on improving evaluation metrics as well as enhancing components within the pipeline using available question and answer datasets. However, they have not focused on 1) providing a schema for capturing different types of question-answer pairs or 2) creating a set of templates for generating question-answer pairs that can support automation of RAG pipeline evaluation. In this paper, we present a technique for generating variations in question-answer pairs to trigger failures in RAG pipelines. We validate 5 open-source RAG pipelines using 3 datasets. Our approach revealed the highest failure rates when prompts combine multiple questions: 91% for questions when spanning multiple documents and 78% for questions from a single document; indicating a need for developers to prioritise handling these combined questions. 60% failure rate was observed in academic domain dataset and 53% and 62% failure rates were observed in open-domain datasets. Our automated approach outperforms the existing state-of-the-art methods, by increasing the failure rate by 51% on average per dataset. Our work presents an automated approach for continuously monitoring the health of RAG pipelines, which can be integrated into existing CI/CD pipelines, allowing for improved quality.",
        "authors": [
            "Shangeetha Sivasothy",
            "Scott Barnett",
            "Stefanus Kurniawan",
            "Zafaryab Rasool",
            "Rajesh Vasa"
        ],
        "submitted_date": "2024-09-24T23:33:07+00:00",
        "pdf_url": "https://arxiv.org/pdf/2409.19019v1",
        "similarity_score": 0.620959997177124
    },
    {
        "arxiv_id": "2204.01075v1",
        "title": "Data Cards: Purposeful and Transparent Dataset Documentation for Responsible AI",
        "abstract": "As research and industry moves towards large-scale models capable of numerous downstream tasks, the complexity of understanding multi-modal datasets that give nuance to models rapidly increases. A clear and thorough understanding of a dataset's origins, development, intent, ethical considerations and evolution becomes a necessary step for the responsible and informed deployment of models, especially those in people-facing contexts and high-risk domains. However, the burden of this understanding often falls on the intelligibility, conciseness, and comprehensiveness of the documentation. It requires consistency and comparability across the documentation of all datasets involved, and as such documentation must be treated as a user-centric product in and of itself. In this paper, we propose Data Cards for fostering transparent, purposeful and human-centered documentation of datasets within the practical contexts of industry and research. Data Cards are structured summaries of essential facts about various aspects of ML datasets needed by stakeholders across a dataset's lifecycle for responsible AI development. These summaries provide explanations of processes and rationales that shape the data and consequently the models, such as upstream sources, data collection and annotation methods; training and evaluation methods, intended use; or decisions affecting model performance. We also present frameworks that ground Data Cards in real-world utility and human-centricity. Using two case studies, we report on desirable characteristics that support adoption across domains, organizational structures, and audience groups. Finally, we present lessons learned from deploying over 20 Data Cards.",
        "authors": [
            "Mahima Pushkarna",
            "Andrew Zaldivar",
            "Oddur Kjartansson"
        ],
        "submitted_date": "2022-04-03T13:49:36+00:00",
        "pdf_url": "https://arxiv.org/pdf/2204.01075v1",
        "similarity_score": 0.6193760633468628
    },
    {
        "arxiv_id": "2501.16945v1",
        "title": "ToolFactory: Automating Tool Generation by Leveraging LLM to Understand REST API Documentations",
        "abstract": "LLM-based tool agents offer natural language interfaces, enabling users to seamlessly interact with computing services. While REST APIs are valuable resources for building such agents, they must first be transformed into AI-compatible tools. Automatically generating AI-compatible tools from REST API documents can greatly streamline tool agent development and minimize user learning curves. However, API documentation often suffers from a lack of standardization, inconsistent schemas, and incomplete information. To address these issues, we developed \\textbf{ToolFactory}, an open-source pipeline for automating tool generation from unstructured API documents. To enhance the reliability of the developed tools, we implemented an evaluation method to diagnose errors. Furthermore, we built a knowledge base of verified tools, which we leveraged to infer missing information from poorly documented APIs. We developed the API Extraction Benchmark, comprising 167 API documents and 744 endpoints in various formats, and designed a JSON schema to annotate them. This annotated dataset was utilized to train and validate ToolFactory. The experimental results highlight the effectiveness of ToolFactory. We also demonstrated ToolFactory by creating a domain-specific AI agent for glycomaterials research. ToolFactory exhibits significant potential for facilitating the seamless integration of scientific REST APIs into AI workflows.",
        "authors": [
            "Xinyi Ni",
            "Qiuyang Wang",
            "Yukun Zhang",
            "Pengyu Hong"
        ],
        "submitted_date": "2025-01-28T13:42:33+00:00",
        "pdf_url": "https://arxiv.org/pdf/2501.16945v1",
        "similarity_score": 0.6180301904678345
    },
    {
        "arxiv_id": "2402.05160v1",
        "title": "What's documented in AI? Systematic Analysis of 32K AI Model Cards",
        "abstract": "The rapid proliferation of AI models has underscored the importance of thorough documentation, as it enables users to understand, trust, and effectively utilize these models in various applications. Although developers are encouraged to produce model cards, it's not clear how much information or what information these cards contain. In this study, we conduct a comprehensive analysis of 32,111 AI model documentations on Hugging Face, a leading platform for distributing and deploying AI models. Our investigation sheds light on the prevailing model card documentation practices. Most of the AI models with substantial downloads provide model cards, though the cards have uneven informativeness. We find that sections addressing environmental impact, limitations, and evaluation exhibit the lowest filled-out rates, while the training section is the most consistently filled-out. We analyze the content of each section to characterize practitioners' priorities. Interestingly, there are substantial discussions of data, sometimes with equal or even greater emphasis than the model itself. To evaluate the impact of model cards, we conducted an intervention study by adding detailed model cards to 42 popular models which had no or sparse model cards previously. We find that adding model cards is moderately correlated with an increase weekly download rates. Our study opens up a new perspective for analyzing community norms and practices for model documentation through large-scale data science and linguistics analysis.",
        "authors": [
            "Weixin Liang",
            "Nazneen Rajani",
            "Xinyu Yang",
            "Ezinwanne Ozoani",
            "Eric Wu",
            "Yiqun Chen",
            "Daniel Scott Smith",
            "James Zou"
        ],
        "submitted_date": "2024-02-07T18:04:32+00:00",
        "pdf_url": "https://arxiv.org/pdf/2402.05160v1",
        "similarity_score": 0.6159766316413879
    },
    {
        "arxiv_id": "2509.26100v1",
        "title": "SafeEvalAgent: Toward Agentic and Self-Evolving Safety Evaluation of LLMs",
        "abstract": "The rapid integration of Large Language Models (LLMs) into high-stakes domains necessitates reliable safety and compliance evaluation. However, existing static benchmarks are ill-equipped to address the dynamic nature of AI risks and evolving regulations, creating a critical safety gap. This paper introduces a new paradigm of agentic safety evaluation, reframing evaluation as a continuous and self-evolving process rather than a one-time audit. We then propose a novel multi-agent framework SafeEvalAgent, which autonomously ingests unstructured policy documents to generate and perpetually evolve a comprehensive safety benchmark. SafeEvalAgent leverages a synergistic pipeline of specialized agents and incorporates a Self-evolving Evaluation loop, where the system learns from evaluation results to craft progressively more sophisticated and targeted test cases. Our experiments demonstrate the effectiveness of SafeEvalAgent, showing a consistent decline in model safety as the evaluation hardens. For instance, GPT-5's safety rate on the EU AI Act drops from 72.50% to 36.36% over successive iterations. These findings reveal the limitations of static assessments and highlight our framework's ability to uncover deep vulnerabilities missed by traditional methods, underscoring the urgent need for dynamic evaluation ecosystems to ensure the safe and responsible deployment of advanced AI.",
        "authors": [
            "Yixu Wang",
            "Xin Wang",
            "Yang Yao",
            "Xinyuan Li",
            "Yan Teng",
            "Xingjun Ma",
            "Yingchun Wang"
        ],
        "submitted_date": "2025-09-30T11:20:41+00:00",
        "pdf_url": "https://arxiv.org/pdf/2509.26100v1",
        "similarity_score": 0.6157177686691284
    },
    {
        "arxiv_id": "2409.03759v1",
        "title": "VERA: Validation and Evaluation of Retrieval-Augmented Systems",
        "abstract": "The increasing use of Retrieval-Augmented Generation (RAG) systems in various applications necessitates stringent protocols to ensure RAG systems accuracy, safety, and alignment with user intentions. In this paper, we introduce VERA (Validation and Evaluation of Retrieval-Augmented Systems), a framework designed to enhance the transparency and reliability of outputs from large language models (LLMs) that utilize retrieved information. VERA improves the way we evaluate RAG systems in two important ways: (1) it introduces a cross-encoder based mechanism that encompasses a set of multidimensional metrics into a single comprehensive ranking score, addressing the challenge of prioritizing individual metrics, and (2) it employs Bootstrap statistics on LLM-based metrics across the document repository to establish confidence bounds, ensuring the repositorys topical coverage and improving the overall reliability of retrieval systems. Through several use cases, we demonstrate how VERA can strengthen decision-making processes and trust in AI applications. Our findings not only contribute to the theoretical understanding of LLM-based RAG evaluation metric but also promote the practical implementation of responsible AI systems, marking a significant advancement in the development of reliable and transparent generative AI technologies.",
        "authors": [
            "Tianyu Ding",
            "Adi Banerjee",
            "Laurent Mombaerts",
            "Yunhong Li",
            "Tarik Borogovac",
            "Juan Pablo De la Cruz Weinstein"
        ],
        "submitted_date": "2024-08-16T21:59:59+00:00",
        "pdf_url": "https://arxiv.org/pdf/2409.03759v1",
        "similarity_score": 0.6144626140594482
    },
    {
        "arxiv_id": "2506.22485v1",
        "title": "AI Agents-as-Judge: Automated Assessment of Accuracy, Consistency, Completeness and Clarity for Enterprise Documents",
        "abstract": "This study presents a modular, multi-agent system for the automated review of highly structured enterprise business documents using AI agents. Unlike prior solutions focused on unstructured texts or limited compliance checks, this framework leverages modern orchestration tools such as LangChain, CrewAI, TruLens, and Guidance to enable section-by-section evaluation of documents for accuracy, consistency, completeness, and clarity. Specialized agents, each responsible for discrete review criteria such as template compliance or factual correctness, operate in parallel or sequence as required. Evaluation outputs are enforced to a standardized, machine-readable schema, supporting downstream analytics and auditability. Continuous monitoring and a feedback loop with human reviewers allow for iterative system improvement and bias mitigation.   Quantitative evaluation demonstrates that the AI Agent-as-Judge system approaches or exceeds human performance in key areas: achieving 99% information consistency (vs. 92% for humans), halving error and bias rates, and reducing average review time from 30 to 2.5 minutes per document, with a 95% agreement rate between AI and expert human judgment. While promising for a wide range of industries, the study also discusses current limitations, including the need for human oversight in highly specialized domains and the operational cost of large-scale LLM usage. The proposed system serves as a flexible, auditable, and scalable foundation for AI-driven document quality assurance in the enterprise context.",
        "authors": [
            "Sudip Dasgupta",
            "Himanshu Shankar"
        ],
        "submitted_date": "2025-06-23T17:46:15+00:00",
        "pdf_url": "https://arxiv.org/pdf/2506.22485v1",
        "similarity_score": 0.6139779090881348
    },
    {
        "arxiv_id": "2409.08960v2",
        "title": "Improving governance outcomes through AI documentation: Bridging theory and practice",
        "abstract": "Documentation plays a crucial role in both external accountability and internal governance of AI systems. Although there are many proposals for documenting AI data, models, systems, and methods, the ways these practices enhance governance as well as the challenges practitioners and organizations face with documentation remain underexplored. In this paper, we analyze 37 proposed documentation frameworks and 22 empirical studies evaluating their use. We identify several pathways or \"theories of change\" through which documentation can enhance governance, including informing stakeholders about AI risks and applications, facilitating collaboration, encouraging ethical deliberation, and supporting best practices. However, empirical findings reveal significant challenges for practitioners, such as insufficient incentives and resources, structural and organizational communication barriers, interpersonal and organizational constraints to ethical action, and poor integration with existing workflows. These challenges often hinder the realization of the possible benefits of documentation. We also highlight key considerations for organizations when designing documentation, such as determining the appropriate level of detail and balancing automation in the process. We conclude by discussing how future research can expand on our findings such as by exploring documentation approaches that support governance of general-purpose models and how multiple transparency and documentation methods can collectively improve governance outcomes.",
        "authors": [
            "Amy A. Winecoff",
            "Miranda Bogen"
        ],
        "submitted_date": "2024-09-13T16:25:11+00:00",
        "pdf_url": "https://arxiv.org/pdf/2409.08960v2",
        "similarity_score": 0.6138582229614258
    },
    {
        "arxiv_id": "2504.07971v1",
        "title": "SPHERE: An Evaluation Card for Human-AI Systems",
        "abstract": "In the era of Large Language Models (LLMs), establishing effective evaluation methods and standards for diverse human-AI interaction systems is increasingly challenging. To encourage more transparent documentation and facilitate discussion on human-AI system evaluation design options, we present an evaluation card SPHERE, which encompasses five key dimensions: 1) What is being evaluated?; 2) How is the evaluation conducted?; 3) Who is participating in the evaluation?; 4) When is evaluation conducted?; 5) How is evaluation validated? We conduct a review of 39 human-AI systems using SPHERE, outlining current evaluation practices and areas for improvement. We provide three recommendations for improving the validity and rigor of evaluation practices.",
        "authors": [
            "Qianou Ma",
            "Dora Zhao",
            "Xinran Zhao",
            "Chenglei Si",
            "Chenyang Yang",
            "Ryan Louie",
            "Ehud Reiter",
            "Diyi Yang",
            "Tongshuang Wu"
        ],
        "submitted_date": "2025-03-24T20:17:20+00:00",
        "pdf_url": "https://arxiv.org/pdf/2504.07971v1",
        "similarity_score": 0.6098504066467285
    },
    {
        "arxiv_id": "2506.13776v1",
        "title": "Recommendations and Reporting Checklist for Rigorous & Transparent Human Baselines in Model Evaluations",
        "abstract": "In this position paper, we argue that human baselines in foundation model evaluations must be more rigorous and more transparent to enable meaningful comparisons of human vs. AI performance, and we provide recommendations and a reporting checklist towards this end. Human performance baselines are vital for the machine learning community, downstream users, and policymakers to interpret AI evaluations. Models are often claimed to achieve \"super-human\" performance, but existing baselining methods are neither sufficiently rigorous nor sufficiently well-documented to robustly measure and assess performance differences. Based on a meta-review of the measurement theory and AI evaluation literatures, we derive a framework with recommendations for designing, executing, and reporting human baselines. We synthesize our recommendations into a checklist that we use to systematically review 115 human baselines (studies) in foundation model evaluations and thus identify shortcomings in existing baselining methods; our checklist can also assist researchers in conducting human baselines and reporting results. We hope our work can advance more rigorous AI evaluation practices that can better serve both the research community and policymakers. Data is available at: https://github.com/kevinlwei/human-baselines",
        "authors": [
            "Kevin L. Wei",
            "Patricia Paskov",
            "Sunishchal Dev",
            "Michael J. Byun",
            "Anka Reuel",
            "Xavier Roberts-Gaal",
            "Rachel Calcott",
            "Evie Coxon",
            "Chinmay Deshpande"
        ],
        "submitted_date": "2025-06-09T04:08:16+00:00",
        "pdf_url": "https://arxiv.org/pdf/2506.13776v1",
        "similarity_score": 0.6088738441467285
    },
    {
        "arxiv_id": "2312.06153v2",
        "title": "Open Datasheets: Machine-readable Documentation for Open Datasets and Responsible AI Assessments",
        "abstract": "This paper introduces a no-code, machine-readable documentation framework for open datasets, with a focus on responsible AI (RAI) considerations. The framework aims to improve comprehensibility, and usability of open datasets, facilitating easier discovery and use, better understanding of content and context, and evaluation of dataset quality and accuracy. The proposed framework is designed to streamline the evaluation of datasets, helping researchers, data scientists, and other open data users quickly identify datasets that meet their needs and organizational policies or regulations. The paper also discusses the implementation of the framework and provides recommendations to maximize its potential. The framework is expected to enhance the quality and reliability of data used in research and decision-making, fostering the development of more responsible and trustworthy AI systems.",
        "authors": [
            "Anthony Cintron Roman",
            "Jennifer Wortman Vaughan",
            "Valerie See",
            "Steph Ballard",
            "Jehu Torres",
            "Caleb Robinson",
            "Juan M. Lavista Ferres"
        ],
        "submitted_date": "2023-12-11T06:41:14+00:00",
        "pdf_url": "https://arxiv.org/pdf/2312.06153v2",
        "similarity_score": 0.6088225841522217
    },
    {
        "arxiv_id": "2511.10049v1",
        "title": "Continuous Benchmark Generation for Evaluating Enterprise-scale LLM Agents",
        "abstract": "The rapid adoption of AI agents across domains has made systematic evaluation crucial for ensuring their usefulness and successful production deployment. Evaluation of AI agents typically involves using a fixed set of benchmarks and computing multiple evaluation metrics for the agent. While sufficient for simple coding tasks, these benchmarks fall short for enterprise-scale agents, where services and requirements evolve continuously and ground-truth examples are sparse. We propose a process of benchmark generation that helps evolve the benchmarks as the requirements change and perform robust evaluation of evolving AI agents. We instantiate this approach for a case study of service migration from one deployment platform to another at a large public enterprise. Our approach relies on semi-structured documents where developers express the high-level intent, and uses state-of-the-art LLMs to generate benchmarks from just a small number of such documents. Overall, this process results in a maintainable evaluation framework, enabling rapid feedback on agent performance and facilitating targeted improvements.",
        "authors": [
            "Divyanshu Saxena",
            "Rishikesh Maurya",
            "Xiaoxuan Ou",
            "Gagan Somashekar",
            "Shachee Mishra Gupta",
            "Arun Iyer",
            "Yu Kang",
            "Chetan Bansal",
            "Aditya Akella",
            "Saravan Rajmohan"
        ],
        "submitted_date": "2025-11-13T07:48:22+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.10049v1",
        "similarity_score": 0.6055800914764404
    },
    {
        "arxiv_id": "2408.14007v3",
        "title": "On the Quality of AI-Generated Source Code Comments: A Comprehensive Evaluation",
        "abstract": "This paper investigates the quality of source code comments automatically generated by Large Language Models (LLMs). While AI-based comment generation has emerged as a promising solution to reduce developers' documentation effort, prior studies have been limited by small datasets or by relying solely on traditional Information Retrieval (IR) metrics, which are insufficient to capture documentation quality. To address these limitations, we conducted a large-scale empirical study on 142 classes and 273 methods created after the training cut-off of the evaluated models. For each code element, we generated Javadoc comments using three LLMs (GPT-3.5 Turbo, GPT-4o, and DeepSeek-V3). A qualitative assessment of the comments-performed independently by two experts-showed that 58.8% were equivalent to, and 27.7% superior to, the original comments. A quantitative analysis using BLEU, ROUGE-L, and METEOR confirmed that IR-based metrics do not reliably reflect human evaluations, revealing the need for new documentation-specific metrics. Finally, correlation analyses indicated slightly positive relationships between code properties (size, complexity, coupling) and comment quality, confirming that LLMs benefit from richer contextual information.",
        "authors": [
            "Ian Guelman",
            "Arthur Greg√≥rio Leal",
            "Laerte Xavier",
            "Marco Tulio Valente"
        ],
        "submitted_date": "2024-08-26T04:27:25+00:00",
        "pdf_url": "https://arxiv.org/pdf/2408.14007v3",
        "similarity_score": 0.604292631149292
    },
    {
        "arxiv_id": "2006.16984v2",
        "title": "Mining Documentation to Extract Hyperparameter Schemas",
        "abstract": "AI automation tools need machine-readable hyperparameter schemas to define their search spaces. At the same time, AI libraries often come with good human-readable documentation. While such documentation contains most of the necessary information, it is unfortunately not ready to consume by tools. This paper describes how to automatically mine Python docstrings in AI libraries to extract JSON Schemas for their hyperparameters. We evaluate our approach on 119 transformers and estimators from three different libraries and find that it is effective at extracting machine-readable schemas. Our vision is to reduce the burden to manually create and maintain such schemas for AI automation tools and broaden the reach of automation to larger libraries and richer schemas.",
        "authors": [
            "Guillaume Baudart",
            "Peter D. Kirchner",
            "Martin Hirzel",
            "Kiran Kate"
        ],
        "submitted_date": "2020-06-30T17:32:47+00:00",
        "pdf_url": "https://arxiv.org/pdf/2006.16984v2",
        "similarity_score": 0.601906955242157
    },
    {
        "arxiv_id": "2404.07917v2",
        "title": "DesignQA: A Multimodal Benchmark for Evaluating Large Language Models' Understanding of Engineering Documentation",
        "abstract": "This research introduces DesignQA, a novel benchmark aimed at evaluating the proficiency of multimodal large language models (MLLMs) in comprehending and applying engineering requirements in technical documentation. Developed with a focus on real-world engineering challenges, DesignQA uniquely combines multimodal data-including textual design requirements, CAD images, and engineering drawings-derived from the Formula SAE student competition. Different from many existing MLLM benchmarks, DesignQA contains document-grounded visual questions where the input image and input document come from different sources. The benchmark features automatic evaluation metrics and is divided into segments-Rule Comprehension, Rule Compliance, and Rule Extraction-based on tasks that engineers perform when designing according to requirements. We evaluate state-of-the-art models (at the time of writing) like GPT-4o, GPT-4, Claude-Opus, Gemini-1.0, and LLaVA-1.5 against the benchmark, and our study uncovers the existing gaps in MLLMs' abilities to interpret complex engineering documentation. The MLLMs tested, while promising, struggle to reliably retrieve relevant rules from the Formula SAE documentation, face challenges in recognizing technical components in CAD images, and encounter difficulty in analyzing engineering drawings. These findings underscore the need for multimodal models that can better handle the multifaceted questions characteristic of design according to technical documentation. This benchmark sets a foundation for future advancements in AI-supported engineering design processes. DesignQA is publicly available at: https://github.com/anniedoris/design_qa/.",
        "authors": [
            "Anna C. Doris",
            "Daniele Grandi",
            "Ryan Tomich",
            "Md Ferdous Alam",
            "Mohammadmehdi Ataei",
            "Hyunmin Cheong",
            "Faez Ahmed"
        ],
        "submitted_date": "2024-04-11T16:59:54+00:00",
        "pdf_url": "https://arxiv.org/pdf/2404.07917v2",
        "similarity_score": 0.6018043160438538
    },
    {
        "arxiv_id": "2408.11800v3",
        "title": "WeQA: A Benchmark for Retrieval Augmented Generation in Wind Energy Domain",
        "abstract": "Wind energy project assessments present significant challenges for decision-makers, who must navigate and synthesize hundreds of pages of environmental and scientific documentation. These documents often span different regions and project scales, covering multiple domains of expertise. This process traditionally demands immense time and specialized knowledge from decision-makers. The advent of Large Language Models (LLM) and Retrieval Augmented Generation (RAG) approaches offer a transformative solution, enabling rapid, accurate cross-document information retrieval and synthesis. As the landscape of Natural Language Processing (NLP) and text generation continues to evolve, benchmarking becomes essential to evaluate and compare the performance of different RAG-based LLMs. In this paper, we present a comprehensive framework to generate a domain relevant RAG benchmark. Our framework is based on automatic question-answer generation with Human (domain experts)-AI (LLM) teaming. As a case study, we demonstrate the framework by introducing WeQA, a first-of-its-kind benchmark on the wind energy domain which comprises of multiple scientific documents/reports related to environmental aspects of wind energy projects. Our framework systematically evaluates RAG performance using diverse metrics and multiple question types with varying complexity level, providing a foundation for rigorous assessment of RAG-based systems in complex scientific domains and enabling researchers to identify areas for improvement in domain-specific applications.",
        "authors": [
            "Rounak Meyur",
            "Hung Phan",
            "Sridevi Wagle",
            "Jan Strube",
            "Mahantesh Halappanavar",
            "Sameera Horawalavithana",
            "Anurag Acharya",
            "Sai Munikoti"
        ],
        "submitted_date": "2024-08-21T17:43:11+00:00",
        "pdf_url": "https://arxiv.org/pdf/2408.11800v3",
        "similarity_score": 0.5997573137283325
    },
    {
        "arxiv_id": "2401.17486v1",
        "title": "A Scoping Study of Evaluation Practices for Responsible AI Tools: Steps Towards Effectiveness Evaluations",
        "abstract": "Responsible design of AI systems is a shared goal across HCI and AI communities. Responsible AI (RAI) tools have been developed to support practitioners to identify, assess, and mitigate ethical issues during AI development. These tools take many forms (e.g., design playbooks, software toolkits, documentation protocols). However, research suggests that use of RAI tools is shaped by organizational contexts, raising questions about how effective such tools are in practice. To better understand how RAI tools are -- and might be -- evaluated, we conducted a qualitative analysis of 37 publications that discuss evaluations of RAI tools. We find that most evaluations focus on usability, while questions of tools' effectiveness in changing AI development are sidelined. While usability evaluations are an important approach to evaluate RAI tools, we draw on evaluation approaches from other fields to highlight developer- and community-level steps to support evaluations of RAI tools' effectiveness in shaping AI development practices and outcomes.",
        "authors": [
            "Glen Berman",
            "Nitesh Goyal",
            "Michael Madaio"
        ],
        "submitted_date": "2024-01-30T22:44:18+00:00",
        "pdf_url": "https://arxiv.org/pdf/2401.17486v1",
        "similarity_score": 0.5958818197250366
    },
    {
        "arxiv_id": "2502.08650v5",
        "title": "Who is Responsible? The Data, Models, Users or Regulations? A Comprehensive Survey on Responsible Generative AI for a Sustainable Future",
        "abstract": "Generative AI is moving rapidly from research into real world deployment across sectors, which elevates the need for responsible development, deployment, evaluation, and governance. To address this pressing challenge, in this study, we synthesize the landscape of responsible generative AI across methods, benchmarks, and policies, and connects governance expectations to concrete engineering practice. We follow a prespecified search and screening protocol focused on post-ChatGPT era with selective inclusion of foundational work for definitions, and we conduct a narrative and thematic synthesis. Three findings emerge; First, benchmark and practice coverage is dense for bias and toxicity but relatively sparse for privacy and provenance, deepfake and media integrity risk, and system level failure in tool using and agentic settings. Second, many evaluations remain static and task local, which limits evidence portability for audit and lifecycle assurance. Third, documentation and metric validity are inconsistent, which complicates comparison across releases and domains. We outline a research and practice agenda that prioritizes adaptive and multimodal evaluation, privacy and provenance testing, deepfake risk assessment, calibration and uncertainty reporting, versioned and documented artifacts, and continuous monitoring. Limitations include reliance on public artifacts and the focus period, which may under represent capabilities reported later. The survey offers a path to align development and evaluation with governance needs and to support safe, transparent, and accountable deployment across domains. Project page: https://anas-zafar.github.io/responsible-ai.github.io , GitHub: https://github.com/anas-zafar/Responsible-AI",
        "authors": [
            "Shaina Raza",
            "Rizwan Qureshi",
            "Anam Zahid",
            "Safiullah Kamawal",
            "Ferhat Sadak",
            "Joseph Fioresi",
            "Muhammaed Saeed",
            "Ranjan Sapkota",
            "Aditya Jain",
            "Anas Zafar",
            "Muneeb Ul Hassan",
            "Aizan Zafar",
            "Hasan Maqbool",
            "Ashmal Vayani",
            "Jia Wu",
            "Maged Shoman"
        ],
        "submitted_date": "2025-01-15T20:59:42+00:00",
        "pdf_url": "https://arxiv.org/pdf/2502.08650v5",
        "similarity_score": 0.5936739444732666
    },
    {
        "arxiv_id": "2503.04479v3",
        "title": "ToolFuzz -- Automated Agent Tool Testing",
        "abstract": "Large Language Model (LLM) Agents leverage the advanced reasoning capabilities of LLMs in real-world applications. To interface with an environment, these agents often rely on tools, such as web search or database APIs. As the agent provides the LLM with tool documentation along the user query, the completeness and correctness of this documentation is critical. However, tool documentation is often over-, under-, or ill-specified, impeding the agent's accuracy. Standard software testing approaches struggle to identify these errors as they are expressed in natural language. Thus, despite its importance, there currently exists no automated method to test the tool documentation for agents. To address this issue, we present ToolFuzz, the first method for automated testing of tool documentations. ToolFuzz is designed to discover two types of errors: (1) user queries leading to tool runtime errors and (2) user queries that lead to incorrect agent responses. ToolFuzz can generate a large and diverse set of natural inputs, effectively finding tool description errors at a low false positive rate. Further, we present two straightforward prompt-engineering approaches. We evaluate all three tool testing approaches on 32 common LangChain tools and 35 newly created custom tools and 2 novel benchmarks to further strengthen the assessment. We find that many publicly available tools suffer from underspecification. Specifically, we show that ToolFuzz identifies 20x more erroneous inputs compared to the prompt-engineering approaches, making it a key component for building reliable AI agents.",
        "authors": [
            "Ivan Milev",
            "Mislav Balunoviƒá",
            "Maximilian Baader",
            "Martin Vechev"
        ],
        "submitted_date": "2025-03-06T14:29:52+00:00",
        "pdf_url": "https://arxiv.org/pdf/2503.04479v3",
        "similarity_score": 0.5929386615753174
    },
    {
        "arxiv_id": "2504.01028v1",
        "title": "Improving Applicability of Deep Learning based Token Classification models during Training",
        "abstract": "This paper shows that further evaluation metrics during model training are needed to decide about its applicability in inference. As an example, a LayoutLM-based model is trained for token classification in documents. The documents are German receipts. We show that conventional classification metrics, represented by the F1-Score in our experiments, are insufficient for evaluating the applicability of machine learning models in practice. To address this problem, we introduce a novel metric, Document Integrity Precision (DIP), as a solution for visual document understanding and the token classification task. To the best of our knowledge, nothing comparable has been introduced in this context. DIP is a rigorous metric, describing how many documents of the test dataset require manual interventions. It enables AI researchers and software developers to conduct an in-depth investigation of the level of process automation in business software. In order to validate DIP, we conduct experiments with our created models to highlight and analyze the impact and relevance of DIP to evaluate if the model should be deployed or not in different training settings. Our results demonstrate that existing metrics barely change for isolated model impairments, whereas DIP indicates that the model requires substantial human interventions in deployment. The larger the set of entities being predicted, the less sensitive conventional metrics are, entailing poor automation quality. DIP, in contrast, remains a single value to be interpreted for entire entity sets. This highlights the importance of having metrics that focus on the business task for model training in production. Since DIP is created for the token classification task, more research is needed to find suitable metrics for other training tasks.",
        "authors": [
            "Anket Mehra",
            "Malte Prie√ü",
            "Marian Himstedt"
        ],
        "submitted_date": "2025-03-28T17:01:19+00:00",
        "pdf_url": "https://arxiv.org/pdf/2504.01028v1",
        "similarity_score": 0.5925624370574951
    },
    {
        "arxiv_id": "2406.16746v4",
        "title": "The Responsible Foundation Model Development Cheatsheet: A Review of Tools & Resources",
        "abstract": "Foundation model development attracts a rapidly expanding body of contributors, scientists, and applications. To help shape responsible development practices, we introduce the Foundation Model Development Cheatsheet: a growing collection of 250+ tools and resources spanning text, vision, and speech modalities. We draw on a large body of prior work to survey resources (e.g. software, documentation, frameworks, guides, and practical tools) that support informed data selection, processing, and understanding, precise and limitation-aware artifact documentation, efficient model training, advance awareness of the environmental impact from training, careful model evaluation of capabilities, risks, and claims, as well as responsible model release, licensing and deployment practices. We hope this curated collection of resources helps guide more responsible development. The process of curating this list, enabled us to review the AI development ecosystem, revealing what tools are critically missing, misused, or over-used in existing practices. We find that (i) tools for data sourcing, model evaluation, and monitoring are critically under-serving ethical and real-world needs, (ii) evaluations for model safety, capabilities, and environmental impact all lack reproducibility and transparency, (iii) text and particularly English-centric analyses continue to dominate over multilingual and multi-modal analyses, and (iv) evaluation of systems, rather than just models, is needed so that capabilities and impact are assessed in context.",
        "authors": [
            "Shayne Longpre",
            "Stella Biderman",
            "Alon Albalak",
            "Hailey Schoelkopf",
            "Daniel McDuff",
            "Sayash Kapoor",
            "Kevin Klyman",
            "Kyle Lo",
            "Gabriel Ilharco",
            "Nay San",
            "Maribeth Rauh",
            "Aviya Skowron",
            "Bertie Vidgen",
            "Laura Weidinger",
            "Arvind Narayanan",
            "Victor Sanh",
            "David Adelani",
            "Percy Liang",
            "Rishi Bommasani",
            "Peter Henderson",
            "Sasha Luccioni",
            "Yacine Jernite",
            "Luca Soldaini"
        ],
        "submitted_date": "2024-06-24T15:55:49+00:00",
        "pdf_url": "https://arxiv.org/pdf/2406.16746v4",
        "similarity_score": 0.5914368629455566
    },
    {
        "arxiv_id": "2309.13701v2",
        "title": "ALLURE: Auditing and Improving LLM-based Evaluation of Text using Iterative In-Context-Learning",
        "abstract": "From grading papers to summarizing medical documents, large language models (LLMs) are evermore used for evaluation of text generated by humans and AI alike. However, despite their extensive utility, LLMs exhibit distinct failure modes, necessitating a thorough audit and improvement of their text evaluation capabilities. Here we introduce ALLURE, a systematic approach to Auditing Large Language Models Understanding and Reasoning Errors. ALLURE involves comparing LLM-generated evaluations with annotated data, and iteratively incorporating instances of significant deviation into the evaluator, which leverages in-context learning (ICL) to enhance and improve robust evaluation of text by LLMs. Through this iterative process, we refine the performance of the evaluator LLM, ultimately reducing reliance on human annotators in the evaluation process. We anticipate ALLURE to serve diverse applications of LLMs in various domains related to evaluation of textual data, such as medical summarization, education, and and productivity.",
        "authors": [
            "Hosein Hasanbeig",
            "Hiteshi Sharma",
            "Leo Betthauser",
            "Felipe Vieira Frujeri",
            "Ida Momennejad"
        ],
        "submitted_date": "2023-09-24T17:15:58+00:00",
        "pdf_url": "https://arxiv.org/pdf/2309.13701v2",
        "similarity_score": 0.5913906097412109
    },
    {
        "arxiv_id": "2501.14094v1",
        "title": "Datasheets for AI and medical datasets (DAIMS): a data validation and documentation framework before machine learning analysis in medical research",
        "abstract": "Despite progresses in data engineering, there are areas with limited consistencies across data validation and documentation procedures causing confusions and technical problems in research involving machine learning. There have been progresses by introducing frameworks like \"Datasheets for Datasets\", however there are areas for improvements to prepare datasets, ready for ML pipelines. Here, we extend the framework to \"Datasheets for AI and medical datasets - DAIMS.\" Our publicly available solution, DAIMS, provides a checklist including data standardization requirements, a software tool to assist the process of the data preparation, an extended form for data documentation and pose research questions, a table as data dictionary, and a flowchart to suggest ML analyses to address the research questions. The checklist consists of 24 common data standardization requirements, where the tool checks and validate a subset of them. In addition, we provided a flowchart mapping research questions to suggested ML methods. DAIMS can serve as a reference for standardizing datasets and a roadmap for researchers aiming to apply effective ML techniques in their medical research endeavors. DAIMS is available on GitHub and as an online app to automate key aspects of dataset evaluation, facilitating efficient preparation of datasets for ML studies.",
        "authors": [
            "Ramtin Zargari Marandi",
            "Anne Svane Frahm",
            "Maja Milojevic"
        ],
        "submitted_date": "2025-01-23T21:02:56+00:00",
        "pdf_url": "https://arxiv.org/pdf/2501.14094v1",
        "similarity_score": 0.5907407999038696
    },
    {
        "arxiv_id": "2404.15320v2",
        "title": "Using Large Language Models to Enrich the Documentation of Datasets for Machine Learning",
        "abstract": "Recent regulatory initiatives like the European AI Act and relevant voices in the Machine Learning (ML) community stress the need to describe datasets along several key dimensions for trustworthy AI, such as the provenance processes and social concerns. However, this information is typically presented as unstructured text in accompanying documentation, hampering their automated analysis and processing. In this work, we explore using large language models (LLM) and a set of prompting strategies to automatically extract these dimensions from documents and enrich the dataset description with them. Our approach could aid data publishers and practitioners in creating machine-readable documentation to improve the discoverability of their datasets, assess their compliance with current AI regulations, and improve the overall quality of ML models trained on them.   In this paper, we evaluate the approach on 12 scientific dataset papers published in two scientific journals (Nature's Scientific Data and Elsevier's Data in Brief) using two different LLMs (GPT3.5 and Flan-UL2). Results show good accuracy with our prompt extraction strategies. Concrete results vary depending on the dimensions, but overall, GPT3.5 shows slightly better accuracy (81,21%) than FLAN-UL2 (69,13%) although it is more prone to hallucinations. We have released an open-source tool implementing our approach and a replication package, including the experiments' code and results, in an open-source repository.",
        "authors": [
            "Joan Giner-Miguelez",
            "Abel G√≥mez",
            "Jordi Cabot"
        ],
        "submitted_date": "2024-04-04T10:09:28+00:00",
        "pdf_url": "https://arxiv.org/pdf/2404.15320v2",
        "similarity_score": 0.5875066518783569
    },
    {
        "arxiv_id": "2404.18496v1",
        "title": "AI-powered Code Review with LLMs: Early Results",
        "abstract": "In this paper, we present a novel approach to improving software quality and efficiency through a Large Language Model (LLM)-based model designed to review code and identify potential issues. Our proposed LLM-based AI agent model is trained on large code repositories. This training includes code reviews, bug reports, and documentation of best practices. It aims to detect code smells, identify potential bugs, provide suggestions for improvement, and optimize the code. Unlike traditional static code analysis tools, our LLM-based AI agent has the ability to predict future potential risks in the code. This supports a dual goal of improving code quality and enhancing developer education by encouraging a deeper understanding of best practices and efficient coding techniques. Furthermore, we explore the model's effectiveness in suggesting improvements that significantly reduce post-release bugs and enhance code review processes, as evidenced by an analysis of developer sentiment toward LLM feedback. For future work, we aim to assess the accuracy and efficiency of LLM-generated documentation updates in comparison to manual methods. This will involve an empirical study focusing on manually conducted code reviews to identify code smells and bugs, alongside an evaluation of best practice documentation, augmented by insights from developer discussions and code reviews. Our goal is to not only refine the accuracy of our LLM-based tool but also to underscore its potential in streamlining the software development lifecycle through proactive code improvement and education.",
        "authors": [
            "Zeeshan Rasheed",
            "Malik Abdul Sami",
            "Muhammad Waseem",
            "Kai-Kristian Kemell",
            "Xiaofeng Wang",
            "Anh Nguyen",
            "Kari Syst√§",
            "Pekka Abrahamsson"
        ],
        "submitted_date": "2024-04-29T08:27:50+00:00",
        "pdf_url": "https://arxiv.org/pdf/2404.18496v1",
        "similarity_score": 0.5832613706588745
    },
    {
        "arxiv_id": "2311.09476v2",
        "title": "ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems",
        "abstract": "Evaluating retrieval-augmented generation (RAG) systems traditionally relies on hand annotations for input queries, passages to retrieve, and responses to generate. We introduce ARES, an Automated RAG Evaluation System, for evaluating RAG systems along the dimensions of context relevance, answer faithfulness, and answer relevance. By creating its own synthetic training data, ARES finetunes lightweight LM judges to assess the quality of individual RAG components. To mitigate potential prediction errors, ARES utilizes a small set of human-annotated datapoints for prediction-powered inference (PPI). Across eight different knowledge-intensive tasks in KILT, SuperGLUE, and AIS, ARES accurately evaluates RAG systems while using only a few hundred human annotations during evaluation. Furthermore, ARES judges remain effective across domain shifts, proving accurate even after changing the type of queries and/or documents used in the evaluated RAG systems. We make our code and datasets publicly available on Github.",
        "authors": [
            "Jon Saad-Falcon",
            "Omar Khattab",
            "Christopher Potts",
            "Matei Zaharia"
        ],
        "submitted_date": "2023-11-16T00:39:39+00:00",
        "pdf_url": "https://arxiv.org/pdf/2311.09476v2",
        "similarity_score": 0.5822509527206421
    },
    {
        "arxiv_id": "2502.10398v2",
        "title": "Practical Application and Limitations of AI Certification Catalogues in the Light of the AI Act",
        "abstract": "In this work-in-progress, we investigate the certification of AI systems, focusing on the practical application and limitations of existing certification catalogues in the light of the AI Act by attempting to certify a publicly available AI system. We aim to evaluate how well current approaches work to effectively certify an AI system, and how publicly accessible AI systems, that might not be actively maintained or initially intended for certification, can be selected and used for a sample certification process. Our methodology involves leveraging the Fraunhofer AI Assessment Catalogue as a comprehensive tool to systematically assess an AI model's compliance with certification standards. We find that while the catalogue effectively structures the evaluation process, it can also be cumbersome and time-consuming to use. We observe the limitations of an AI system that has no active development team anymore and highlighted the importance of complete system documentation. Finally, we identify some limitations of the certification catalogues used and proposed ideas on how to streamline the certification process.",
        "authors": [
            "Gregor Autischer",
            "Kerstin Waxnegger",
            "Dominik Kowald"
        ],
        "submitted_date": "2025-01-20T15:54:57+00:00",
        "pdf_url": "https://arxiv.org/pdf/2502.10398v2",
        "similarity_score": 0.5811111330986023
    },
    {
        "arxiv_id": "2406.06657v1",
        "title": "Harnessing AI for efficient analysis of complex policy documents: a case study of Executive Order 14110",
        "abstract": "Policy documents, such as legislation, regulations, and executive orders, are crucial in shaping society. However, their length and complexity make interpretation and application challenging and time-consuming. Artificial intelligence (AI), particularly large language models (LLMs), has the potential to automate the process of analyzing these documents, improving accuracy and efficiency. This study aims to evaluate the potential of AI in streamlining policy analysis and to identify the strengths and limitations of current AI approaches. The research focuses on question answering and tasks involving content extraction from policy documents. A case study was conducted using Executive Order 14110 on \"Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence\" as a test case. Four commercial AI systems were used to analyze the document and answer a set of representative policy questions. The performance of the AI systems was compared to manual analysis conducted by human experts. The study found that two AI systems, Gemini 1.5 Pro and Claude 3 Opus, demonstrated significant potential for supporting policy analysis, providing accurate and reliable information extraction from complex documents. They performed comparably to human analysts but with significantly higher efficiency. However, achieving reproducibility remains a challenge, necessitating further research and development.",
        "authors": [
            "Mark A. Kramer",
            "Allen Leavens",
            "Alexander Scarlat"
        ],
        "submitted_date": "2024-06-10T11:19:28+00:00",
        "pdf_url": "https://arxiv.org/pdf/2406.06657v1",
        "similarity_score": 0.5796852111816406
    },
    {
        "arxiv_id": "2506.09230v1",
        "title": "Formal Methods Meets Readability: Auto-Documenting JML Java Code",
        "abstract": "This paper investigates whether formal specifications using Java Modeling Language (JML) can enhance the quality of Large Language Model (LLM)-generated Javadocs. While LLMs excel at producing documentation from code alone, we hypothesize that incorporating formally verified invariants yields more complete and accurate results. We present a systematic comparison of documentation generated from JML-annotated and non-annotated Java classes, evaluating quality through both automated metrics and expert analysis. Our findings demonstrate that JML significantly improves class-level documentation completeness, with more moderate gains at the method level. Formal specifications prove particularly effective in capturing complex class invariants and design contracts that are frequently overlooked in code-only documentation. A threshold effect emerges, where the benefits of JML become more pronounced for classes with richer sets of invariants. While JML enhances specification coverage, its impact on core descriptive quality is limited, suggesting that formal specifications primarily ensure comprehensive coverage rather than fundamentally altering implementation descriptions. These results offer actionable insights for software teams adopting formal methods in documentation workflows, highlighting scenarios where JML provides clear advantages. The study contributes to AI-assisted software documentation research by demonstrating how formal methods and LLMs can synergistically improve documentation quality.",
        "authors": [
            "Juan Carlos Recio Abad",
            "Ruben Saborido",
            "Francisco Chicano"
        ],
        "submitted_date": "2025-06-10T20:39:29+00:00",
        "pdf_url": "https://arxiv.org/pdf/2506.09230v1",
        "similarity_score": 0.579314112663269
    },
    {
        "arxiv_id": "2510.01474v2",
        "title": "AIReg-Bench: Benchmarking Language Models That Assess AI Regulation Compliance",
        "abstract": "As governments move to regulate AI, there is growing interest in using Large Language Models (LLMs) to assess whether or not an AI system complies with a given AI Regulation (AIR). However, there is presently no way to benchmark the performance of LLMs at this task. To fill this void, we introduce AIReg-Bench: the first benchmark dataset designed to test how well LLMs can assess compliance with the EU AI Act (AIA). We created this dataset through a two-step process: (1) by prompting an LLM with carefully structured instructions, we generated 120 technical documentation excerpts (samples), each depicting a fictional, albeit plausible, AI system - of the kind an AI provider might produce to demonstrate their compliance with AIR; (2) legal experts then reviewed and annotated each sample to indicate whether, and in what way, the AI system described therein violates specific Articles of the AIA. The resulting dataset, together with our evaluation of whether frontier LLMs can reproduce the experts' compliance labels, provides a starting point to understand the opportunities and limitations of LLM-based AIR compliance assessment tools and establishes a benchmark against which subsequent LLMs can be compared. The dataset and evaluation code are available at https://github.com/camlsys/aireg-bench.",
        "authors": [
            "Bill Marino",
            "Rosco Hunter",
            "Zubair Jamali",
            "Marinos Emmanouil Kalpakos",
            "Mudra Kashyap",
            "Isaiah Hinton",
            "Alexa Hanson",
            "Maahum Nazir",
            "Christoph Schnabl",
            "Felix Steffek",
            "Hongkai Wen",
            "Nicholas D. Lane"
        ],
        "submitted_date": "2025-10-01T21:33:33+00:00",
        "pdf_url": "https://arxiv.org/pdf/2510.01474v2",
        "similarity_score": 0.5736469030380249
    },
    {
        "arxiv_id": "2502.17943v1",
        "title": "CaseGen: A Benchmark for Multi-Stage Legal Case Documents Generation",
        "abstract": "Legal case documents play a critical role in judicial proceedings. As the number of cases continues to rise, the reliance on manual drafting of legal case documents is facing increasing pressure and challenges. The development of large language models (LLMs) offers a promising solution for automating document generation. However, existing benchmarks fail to fully capture the complexities involved in drafting legal case documents in real-world scenarios. To address this gap, we introduce CaseGen, the benchmark for multi-stage legal case documents generation in the Chinese legal domain. CaseGen is based on 500 real case samples annotated by legal experts and covers seven essential case sections. It supports four key tasks: drafting defense statements, writing trial facts, composing legal reasoning, and generating judgment results. To the best of our knowledge, CaseGen is the first benchmark designed to evaluate LLMs in the context of legal case document generation. To ensure an accurate and comprehensive evaluation, we design the LLM-as-a-judge evaluation framework and validate its effectiveness through human annotations. We evaluate several widely used general-domain LLMs and legal-specific LLMs, highlighting their limitations in case document generation and pinpointing areas for potential improvement. This work marks a step toward a more effective framework for automating legal case documents drafting, paving the way for the reliable application of AI in the legal field. The dataset and code are publicly available at https://github.com/CSHaitao/CaseGen.",
        "authors": [
            "Haitao Li",
            "Jiaying Ye",
            "Yiran Hu",
            "Jia Chen",
            "Qingyao Ai",
            "Yueyue Wu",
            "Junjie Chen",
            "Yifan Chen",
            "Cheng Luo",
            "Quan Zhou",
            "Yiqun Liu"
        ],
        "submitted_date": "2025-02-25T08:03:32+00:00",
        "pdf_url": "https://arxiv.org/pdf/2502.17943v1",
        "similarity_score": 0.572437047958374
    },
    {
        "arxiv_id": "2506.20608v2",
        "title": "AI Assistants to Enhance and Exploit the PETSc Knowledge Base",
        "abstract": "Generative AI, especially through large language models (LLMs), is transforming how technical knowledge can be accessed, reused, and extended. PETSc, a widely used numerical library for high-performance scientific computing, has accumulated a rich but fragmented knowledge base over its three decades of development, spanning source code, documentation, mailing lists, GitLab issues, Discord conversations, technical papers, and more. Much of this knowledge remains informal and inaccessible to users and new developers. To activate and utilize this knowledge base more effectively, the PETSc team has begun building an LLM-powered system that combines PETSc content with custom LLM tools -- including retrieval-augmented generation (RAG), reranking algorithms, and chatbots -- to assist users, support developers, and propose updates to formal documentation. This paper presents initial experiences designing and evaluating these tools, focusing on system architecture, using RAG and reranking for PETSc-specific information, evaluation methodologies for various LLMs and embedding models, and user interface design. Leveraging the Argonne Leadership Computing Facility resources, we analyze how LLM responses can enhance the development and use of numerical software, with an initial focus on scalable Krylov solvers. Our goal is to establish an extensible framework for knowledge-centered AI in scientific software, enabling scalable support, enriched documentation, and enhanced workflows for research and development. We conclude by outlining directions for expanding this system into a robust, evolving platform that advances software ecosystems to accelerate scientific discovery.",
        "authors": [
            "Barry Smith",
            "Junchao Zhang",
            "Hong Zhang",
            "Lois Curfman McInnes",
            "Murat Keceli",
            "Archit Vasan",
            "Satish Balay",
            "Toby Isaac",
            "Le Chen",
            "Venkatram Vishwanath"
        ],
        "submitted_date": "2025-06-25T17:00:05+00:00",
        "pdf_url": "https://arxiv.org/pdf/2506.20608v2",
        "similarity_score": 0.5706503391265869
    },
    {
        "arxiv_id": "2407.17374v2",
        "title": "Co-designing an AI Impact Assessment Report Template with AI Practitioners and AI Compliance Experts",
        "abstract": "In the evolving landscape of AI regulation, it is crucial for companies to conduct impact assessments and document their compliance through comprehensive reports. However, current reports lack grounding in regulations and often focus on specific aspects like privacy in relation to AI systems, without addressing the real-world uses of these systems. Moreover, there is no systematic effort to design and evaluate these reports with both AI practitioners and AI compliance experts. To address this gap, we conducted an iterative co-design process with 14 AI practitioners and 6 AI compliance experts and proposed a template for impact assessment reports grounded in the EU AI Act, NIST's AI Risk Management Framework, and ISO 42001 AI Management System. We evaluated the template by producing an impact assessment report for an AI-based meeting companion at a major tech company. A user study with 8 AI practitioners from the same company and 5 AI compliance experts from industry and academia revealed that our template effectively provides necessary information for impact assessments and documents the broad impacts of AI systems. Participants envisioned using the template not only at the pre-deployment stage for compliance but also as a tool to guide the design stage of AI uses.",
        "authors": [
            "Edyta Bogucka",
            "Marios Constantinides",
            "Sanja ≈†ƒáepanoviƒá",
            "Daniele Quercia"
        ],
        "submitted_date": "2024-07-24T15:53:04+00:00",
        "pdf_url": "https://arxiv.org/pdf/2407.17374v2",
        "similarity_score": 0.5700953602790833
    },
    {
        "arxiv_id": "2509.19209v1",
        "title": "A Knowledge Graph and a Tripartite Evaluation Framework Make Retrieval-Augmented Generation Scalable and Transparent",
        "abstract": "Large Language Models (LLMs) have significantly enhanced conversational Artificial Intelligence(AI) chatbots; however, domain-specific accuracy and the avoidance of factual inconsistencies remain pressing challenges, particularly for large datasets. Designing an effective chatbot with appropriate methods and evaluating its effectiveness is among the challenges in this domain. This study presents a Retrieval Augmented Generation (RAG) chatbot that harnesses a knowledge graph and vector search retrieval to deliver precise, context-rich responses in an exemplary use case from over high-volume engineering project-related emails, thereby minimising the need for document chunking. A central innovation of this work is the introduction of RAG Evaluation (RAG-Eval), a novel chain-of-thought LLM-based tripartite evaluation framework specifically developed to assess RAG applications. This framework operates in parallel with the chatbot, jointly assessing the user's query, the retrieved document, and the generated response, enabling a holistic evaluation across multiple quality metrics like query relevance, factual accuracy, coverage, coherence and fluency. The resulting scoring system is provided directly to users as a confidence score (1 to 100%), enabling quick identification of possible misaligned or incomplete answers. This proposed approach promotes transparency and rapid verification by incorporating metadata email IDs, timestamps into responses. Experimental comparisons against BERTScore and G-EVAL for summarisation evaluation tasks confirm its effectiveness, and empirical analysis also shows RAG-Eval reliably detects factual gaps and query mismatches, thereby fostering trust in high demand, data centric environments. These findings highlight a scalable path for developing accurate, user-verifiable chatbots that bridge the gap between high-level conversational fluency and factual accuracy.",
        "authors": [
            "Olalekan K. Akindele",
            "Bhupesh Kumar Mishra",
            "Kenneth Y. Wertheim"
        ],
        "submitted_date": "2025-09-23T16:29:22+00:00",
        "pdf_url": "https://arxiv.org/pdf/2509.19209v1",
        "similarity_score": 0.5684976577758789
    },
    {
        "arxiv_id": "2407.15353v2",
        "title": "Customized Retrieval Augmented Generation and Benchmarking for EDA Tool Documentation QA",
        "abstract": "Retrieval augmented generation (RAG) enhances the accuracy and reliability of generative AI models by sourcing factual information from external databases, which is extensively employed in document-grounded question-answering (QA) tasks. Off-the-shelf RAG flows are well pretrained on general-purpose documents, yet they encounter significant challenges when being applied to knowledge-intensive vertical domains, such as electronic design automation (EDA). This paper addresses such issue by proposing a customized RAG framework along with three domain-specific techniques for EDA tool documentation QA, including a contrastive learning scheme for text embedding model fine-tuning, a reranker distilled from proprietary LLM, and a generative LLM fine-tuned with high-quality domain corpus. Furthermore, we have developed and released a documentation QA evaluation benchmark, ORD-QA, for OpenROAD, an advanced RTL-to-GDSII design platform. Experimental results demonstrate that our proposed RAG flow and techniques have achieved superior performance on ORD-QA as well as on a commercial tool, compared with state-of-the-arts. The ORD-QA benchmark and the training dataset for our customized RAG flow are open-source at https://github.com/lesliepy99/RAG-EDA.",
        "authors": [
            "Yuan Pu",
            "Zhuolun He",
            "Tairu Qiu",
            "Haoyuan Wu",
            "Bei Yu"
        ],
        "submitted_date": "2024-07-22T03:44:27+00:00",
        "pdf_url": "https://arxiv.org/pdf/2407.15353v2",
        "similarity_score": 0.567781925201416
    },
    {
        "arxiv_id": "2510.20927v2",
        "title": "What do model reports say about their ChemBio benchmark evaluations? Comparing recent releases to the STREAM framework",
        "abstract": "Most frontier AI developers publicly document their safety evaluations of new AI models in model reports, including testing for chemical and biological (ChemBio) misuse risks. This practice provides a window into the methodology of these evaluations, helping to build public trust in AI systems, and enabling third party review in the still-emerging science of AI evaluation. But what aspects of evaluation methodology do developers currently include -- or omit -- in their reports? This paper examines three frontier AI model reports published in spring 2025 with among the most detailed documentation: OpenAI's o3, Anthropic's Claude 4, and Google DeepMind's Gemini 2.5 Pro. We compare these using the STREAM (v1) standard for reporting ChemBio benchmark evaluations. Each model report included some useful details that the others did not, and all model reports were found to have areas for development, suggesting that developers could benefit from adopting one another's best reporting practices. We identified several items where reporting was less well-developed across all model reports, such as providing examples of test material, and including a detailed list of elicitation conditions. Overall, we recommend that AI developers continue to strengthen the emerging science of evaluation by working towards greater transparency in areas where reporting currently remains limited.",
        "authors": [
            "Tom Reed",
            "Tegan McCaslin",
            "Luca Righetti"
        ],
        "submitted_date": "2025-10-23T18:35:15+00:00",
        "pdf_url": "https://arxiv.org/pdf/2510.20927v2",
        "similarity_score": 0.5670628547668457
    },
    {
        "arxiv_id": "2511.12668v1",
        "title": "AI Bill of Materials and Beyond: Systematizing Security Assurance through the AI Risk Scanning (AIRS) Framework",
        "abstract": "Assurance for artificial intelligence (AI) systems remains fragmented across software supply-chain security, adversarial machine learning, and governance documentation. Existing transparency mechanisms - including Model Cards, Datasheets, and Software Bills of Materials (SBOMs) - advance provenance reporting but rarely provide verifiable, machine-readable evidence of model security. This paper introduces the AI Risk Scanning (AIRS) Framework, a threat-model-based, evidence-generating framework designed to operationalize AI assurance. The AIRS Framework evolved through three progressive pilot studies - Smurf (AIBOM schema design), OPAL (operational validation), and Pilot C (AIRS) - that reframed AI documentation from descriptive disclosure toward measurable, evidence-bound verification. The framework aligns its assurance fields to the MITRE ATLAS adversarial ML taxonomy and automatically produces structured artifacts capturing model integrity, packaging and serialization safety, structural adapters, and runtime behaviors. Currently, the AIRS Framework is scoped to provide model-level assurances for LLMs, but it could be expanded to include other modalities and cover system-level threats (e.g. application-layer abuses, tool-calling). A proof-of-concept on a quantized GPT-OSS-20B model demonstrates enforcement of safe loader policies, per-shard hash verification, and contamination and backdoor probes executed under controlled runtime conditions. Comparative analysis with SBOM standards of SPDX 3.0 and CycloneDX 1.6 reveals alignment on identity and evaluation metadata, but identifies critical gaps in representing AI-specific assurance fields. The AIRS Framework thus extends SBOM practice to the AI domain by coupling threat modeling with automated, auditable evidence generation, providing a principled foundation for standardized, trustworthy, and machine-verifiable AI risk documentation.",
        "authors": [
            "Samuel Nathanson",
            "Alexander Lee",
            "Catherine Chen Kieffer",
            "Jared Junkin",
            "Jessica Ye",
            "Amir Saeed",
            "Melanie Lockhart",
            "Russ Fink",
            "Elisha Peterson",
            "Lanier Watkins"
        ],
        "submitted_date": "2025-11-16T16:10:38+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.12668v1",
        "similarity_score": 0.5659767389297485
    },
    {
        "arxiv_id": "2305.08455v3",
        "title": "Document Understanding Dataset and Evaluation (DUDE)",
        "abstract": "We call on the Document AI (DocAI) community to reevaluate current methodologies and embrace the challenge of creating more practically-oriented benchmarks. Document Understanding Dataset and Evaluation (DUDE) seeks to remediate the halted research progress in understanding visually-rich documents (VRDs). We present a new dataset with novelties related to types of questions, answers, and document layouts based on multi-industry, multi-domain, and multi-page VRDs of various origins, and dates. Moreover, we are pushing the boundaries of current methods by creating multi-task and multi-domain evaluation setups that more accurately simulate real-world situations where powerful generalization and adaptation under low-resource settings are desired. DUDE aims to set a new standard as a more practical, long-standing benchmark for the community, and we hope that it will lead to future extensions and contributions that address real-world challenges. Finally, our work illustrates the importance of finding more efficient ways to model language, images, and layout in DocAI.",
        "authors": [
            "Jordy Van Landeghem",
            "Rub√©n Tito",
            "≈Åukasz Borchmann",
            "Micha≈Ç Pietruszka",
            "Pawe≈Ç J√≥ziak",
            "Rafa≈Ç Powalski",
            "Dawid Jurkiewicz",
            "Micka√´l Coustaty",
            "Bertrand Ackaert",
            "Ernest Valveny",
            "Matthew Blaschko",
            "Sien Moens",
            "Tomasz Stanis≈Çawek"
        ],
        "submitted_date": "2023-05-15T08:54:32+00:00",
        "pdf_url": "https://arxiv.org/pdf/2305.08455v3",
        "similarity_score": 0.5658098459243774
    },
    {
        "arxiv_id": "2505.03214v1",
        "title": "DocSpiral: A Platform for Integrated Assistive Document Annotation through Human-in-the-Spiral",
        "abstract": "Acquiring structured data from domain-specific, image-based documents such as scanned reports is crucial for many downstream tasks but remains challenging due to document variability. Many of these documents exist as images rather than as machine-readable text, which requires human annotation to train automated extraction systems. We present DocSpiral, the first Human-in-the-Spiral assistive document annotation platform, designed to address the challenge of extracting structured information from domain-specific, image-based document collections. Our spiral design establishes an iterative cycle in which human annotations train models that progressively require less manual intervention. DocSpiral integrates document format normalization, comprehensive annotation interfaces, evaluation metrics dashboard, and API endpoints for the development of AI / ML models into a unified workflow. Experiments demonstrate that our framework reduces annotation time by at least 41\\% while showing consistent performance gains across three iterations during model training. By making this annotation platform freely accessible, we aim to lower barriers to AI/ML models development in document processing, facilitating the adoption of large language models in image-based, document-intensive fields such as geoscience and healthcare. The system is freely available at: https://app.ai4wa.com. The demonstration video is available: https://app.ai4wa.com/docs/docspiral/demo.",
        "authors": [
            "Qiang Sun",
            "Sirui Li",
            "Tingting Bi",
            "Du Huynh",
            "Mark Reynolds",
            "Yuanyi Luo",
            "Wei Liu"
        ],
        "submitted_date": "2025-05-06T06:02:42+00:00",
        "pdf_url": "https://arxiv.org/pdf/2505.03214v1",
        "similarity_score": 0.5637474060058594
    },
    {
        "arxiv_id": "2504.08725v3",
        "title": "DocAgent: A Multi-Agent System for Automated Code Documentation Generation",
        "abstract": "High-quality code documentation is crucial for software development especially in the era of AI. However, generating it automatically using Large Language Models (LLMs) remains challenging, as existing approaches often produce incomplete, unhelpful, or factually incorrect outputs. We introduce DocAgent, a novel multi-agent collaborative system using topological code processing for incremental context building. Specialized agents (Reader, Searcher, Writer, Verifier, Orchestrator) then collaboratively generate documentation. We also propose a multi-faceted evaluation framework assessing Completeness, Helpfulness, and Truthfulness. Comprehensive experiments show DocAgent significantly outperforms baselines consistently. Our ablation study confirms the vital role of the topological processing order. DocAgent offers a robust approach for reliable code documentation generation in complex and proprietary repositories.",
        "authors": [
            "Dayu Yang",
            "Antoine Simoulin",
            "Xin Qian",
            "Xiaoyi Liu",
            "Yuwei Cao",
            "Zhaopu Teng",
            "Grey Yang"
        ],
        "submitted_date": "2025-04-11T17:50:08+00:00",
        "pdf_url": "https://arxiv.org/pdf/2504.08725v3",
        "similarity_score": 0.563311755657196
    },
    {
        "arxiv_id": "2503.23574v1",
        "title": "Navigating Uncertainties: Understanding How GenAI Developers Document Their Models on Open-Source Platforms",
        "abstract": "Model documentation plays a crucial role in promoting transparency and responsible development of AI systems. With the rise of Generative AI (GenAI), open-source platforms have increasingly become hubs for hosting and distributing these models, prompting platforms like Hugging Face to develop dedicated model documentation guidelines that align with responsible AI principles. Despite these growing efforts, there remains a lack of understanding of how developers document their GenAI models on open-source platforms. Through interviews with 13 GenAI developers active on open-source platforms, we provide empirical insights into their documentation practices and challenges. Our analysis reveals that despite existing resources, developers of GenAI models still face multiple layers of uncertainties in their model documentation: (1) uncertainties about what specific content should be included; (2) uncertainties about how to effectively report key components of their models; and (3) uncertainties in deciding who should take responsibilities for various aspects of model documentation. Based on our findings, we discuss the implications for policymakers, open-source platforms, and the research community to support meaningful, effective and actionable model documentation in the GenAI era, including cultivating better community norms, building robust evaluation infrastructures, and clarifying roles and responsibilities.",
        "authors": [
            "Ningjing Tang",
            "Megan Li",
            "Amy Winecoff",
            "Michael Madaio",
            "Hoda Heidari",
            "Hong Shen"
        ],
        "submitted_date": "2025-03-30T19:46:46+00:00",
        "pdf_url": "https://arxiv.org/pdf/2503.23574v1",
        "similarity_score": 0.5621252059936523
    },
    {
        "arxiv_id": "2411.11410v2",
        "title": "Detecting Multi-Parameter Constraint Inconsistencies in Python Data Science Libraries",
        "abstract": "Modern AI- and Data-intensive software systems rely heavily on data science and machine learning libraries that provide essential algorithmic implementations and computational frameworks. These libraries expose complex APIs whose correct usage has to follow constraints among multiple interdependent parameters. Developers using these APIs are expected to learn about the constraints through the provided documentations and any discrepancy may lead to unexpected behaviors. However, maintaining correct and consistent multi-parameter constraints in API documentations remains a significant challenge for API compatibility and reliability. To address this challenge, we propose MPDetector, for detecting inconsistencies between code and documentation, specifically focusing on multi-parameter constraints. MPDetector identifies these constraints at the code level by exploring execution paths through symbolic execution and further extracts corresponding constraints from documentation using large language models (LLMs). We propose a customized fuzzy constraint logic to reconcile the unpredictability of LLM outputs and detects logical inconsistencies between the code and documentation constraints. We collected and constructed two datasets from four popular data science libraries and evaluated MPDetector on them. The results demonstrate that MPDetector can effectively detect inconsistency issues with the precision of 92.8%. We further reported 14 detected inconsistency issues to the library developers, who have confirmed 11 issues at the time of writing.",
        "authors": [
            "Xiufeng Xu",
            "Fuman Xie",
            "Chenguang Zhu",
            "Guangdong Bai",
            "Sarfraz Khurshid",
            "Yi Li"
        ],
        "submitted_date": "2024-11-18T09:30:14+00:00",
        "pdf_url": "https://arxiv.org/pdf/2411.11410v2",
        "similarity_score": 0.5604971647262573
    },
    {
        "arxiv_id": "2302.07872v1",
        "title": "Data-Centric Governance",
        "abstract": "Artificial intelligence (AI) governance is the body of standards and practices used to ensure that AI systems are deployed responsibly. Current AI governance approaches consist mainly of manual review and documentation processes. While such reviews are necessary for many systems, they are not sufficient to systematically address all potential harms, as they do not operationalize governance requirements for system engineering, behavior, and outcomes in a way that facilitates rigorous and reproducible evaluation. Modern AI systems are data-centric: they act on data, produce data, and are built through data engineering. The assurance of governance requirements must also be carried out in terms of data. This work explores the systematization of governance requirements via datasets and algorithmic evaluations. When applied throughout the product lifecycle, data-centric governance decreases time to deployment, increases solution quality, decreases deployment risks, and places the system in a continuous state of assured compliance with governance requirements.",
        "authors": [
            "Sean McGregor",
            "Jesse Hostetler"
        ],
        "submitted_date": "2023-02-14T07:22:32+00:00",
        "pdf_url": "https://arxiv.org/pdf/2302.07872v1",
        "similarity_score": 0.5578557848930359
    },
    {
        "arxiv_id": "2406.04383v2",
        "title": "Exploring the Latest LLMs for Leaderboard Extraction",
        "abstract": "The rapid advancements in Large Language Models (LLMs) have opened new avenues for automating complex tasks in AI research. This paper investigates the efficacy of different LLMs-Mistral 7B, Llama-2, GPT-4-Turbo and GPT-4.o in extracting leaderboard information from empirical AI research articles. We explore three types of contextual inputs to the models: DocTAET (Document Title, Abstract, Experimental Setup, and Tabular Information), DocREC (Results, Experiments, and Conclusions), and DocFULL (entire document). Our comprehensive study evaluates the performance of these models in generating (Task, Dataset, Metric, Score) quadruples from research papers. The findings reveal significant insights into the strengths and limitations of each model and context type, providing valuable guidance for future AI research automation efforts.",
        "authors": [
            "Salomon Kabongo",
            "Jennifer D'Souza",
            "S√∂ren Auer"
        ],
        "submitted_date": "2024-06-06T05:54:45+00:00",
        "pdf_url": "https://arxiv.org/pdf/2406.04383v2",
        "similarity_score": 0.5563209056854248
    },
    {
        "arxiv_id": "2503.16504v1",
        "title": "Open-Source Tool for Evaluating Human-Generated vs. AI-Generated Medical Notes Using the PDQI-9 Framework",
        "abstract": "Background: The increasing use of artificial intelligence (AI) in healthcare documentation necessitates robust methods for evaluating the quality of AI-generated medical notes compared to those written by humans. This paper introduces an open-source tool, the Human Notes Evaluator, designed to assess clinical note quality and differentiate between human and AI authorship. Methods: The Human Notes Evaluator is a Flask-based web application implemented on Hugging Face Spaces. It employs the Physician Documentation Quality Instrument (PDQI-9), a validated 9-item rubric, to evaluate notes across dimensions such as accuracy, thoroughness, clarity, and more. The tool allows users to upload clinical notes in CSV format and systematically score each note against the PDQI-9 criteria, as well as assess the perceived origin (human, AI, or undetermined). Results: The Human Notes Evaluator provides a user-friendly interface for standardized note assessment. It outputs comprehensive results, including individual PDQI-9 scores for each criterion, origin assessments, and overall quality metrics. Exportable data facilitates comparative analyses between human and AI-generated notes, identification of quality trends, and areas for documentation improvement. The tool is available online at https://huggingface.co/spaces/iyadsultan/human_evaluator . Discussion: This open-source tool offers a valuable resource for researchers, healthcare professionals, and AI developers to rigorously evaluate and compare the quality of medical notes. By leveraging the PDQI-9 framework, it provides a structured and reliable approach to assess clinical documentation, contributing to the responsible integration of AI in healthcare. The tool's availability on Hugging Face promotes accessibility and collaborative development in the field of AI-driven medical documentation.",
        "authors": [
            "Iyad Sultan"
        ],
        "submitted_date": "2025-03-13T05:22:02+00:00",
        "pdf_url": "https://arxiv.org/pdf/2503.16504v1",
        "similarity_score": 0.5549765825271606
    },
    {
        "arxiv_id": "2009.11352v1",
        "title": "ConvAI3: Generating Clarifying Questions for Open-Domain Dialogue Systems (ClariQ)",
        "abstract": "This document presents a detailed description of the challenge on clarifying questions for dialogue systems (ClariQ). The challenge is organized as part of the Conversational AI challenge series (ConvAI3) at Search Oriented Conversational AI (SCAI) EMNLP workshop in 2020. The main aim of the conversational systems is to return an appropriate answer in response to the user requests. However, some user requests might be ambiguous. In IR settings such a situation is handled mainly thought the diversification of the search result page. It is however much more challenging in dialogue settings with limited bandwidth. Therefore, in this challenge, we provide a common evaluation framework to evaluate mixed-initiative conversations. Participants are asked to rank clarifying questions in an information-seeking conversations. The challenge is organized in two stages where in Stage 1 we evaluate the submissions in an offline setting and single-turn conversations. Top participants of Stage 1 get the chance to have their model tested by human annotators.",
        "authors": [
            "Mohammad Aliannejadi",
            "Julia Kiseleva",
            "Aleksandr Chuklin",
            "Jeff Dalton",
            "Mikhail Burtsev"
        ],
        "submitted_date": "2020-09-23T19:48:02+00:00",
        "pdf_url": "https://arxiv.org/pdf/2009.11352v1",
        "similarity_score": 0.5545199513435364
    },
    {
        "arxiv_id": "2502.15237v1",
        "title": "From Documents to Dialogue: Building KG-RAG Enhanced AI Assistants",
        "abstract": "The Adobe Experience Platform AI Assistant is a conversational tool that enables organizations to interact seamlessly with proprietary enterprise data through a chatbot. However, due to access restrictions, Large Language Models (LLMs) cannot retrieve these internal documents, limiting their ability to generate accurate zero-shot responses. To overcome this limitation, we use a Retrieval-Augmented Generation (RAG) framework powered by a Knowledge Graph (KG) to retrieve relevant information from external knowledge sources, enabling LLMs to answer questions over private or previously unseen document collections. In this paper, we propose a novel approach for building a high-quality, low-noise KG. We apply several techniques, including incremental entity resolution using seed concepts, similarity-based filtering to deduplicate entries, assigning confidence scores to entity-relation pairs to filter for high-confidence pairs, and linking facts to source documents for provenance. Our KG-RAG system retrieves relevant tuples, which are added to the user prompts context before being sent to the LLM generating the response. Our evaluation demonstrates that this approach significantly enhances response relevance, reducing irrelevant answers by over 50% and increasing fully relevant answers by 88% compared to the existing production system.",
        "authors": [
            "Manisha Mukherjee",
            "Sungchul Kim",
            "Xiang Chen",
            "Dan Luo",
            "Tong Yu",
            "Tung Mai"
        ],
        "submitted_date": "2025-02-21T06:22:12+00:00",
        "pdf_url": "https://arxiv.org/pdf/2502.15237v1",
        "similarity_score": 0.5533900856971741
    },
    {
        "arxiv_id": "2205.02894v1",
        "title": "Interactive Model Cards: A Human-Centered Approach to Model Documentation",
        "abstract": "Deep learning models for natural language processing (NLP) are increasingly adopted and deployed by analysts without formal training in NLP or machine learning (ML). However, the documentation intended to convey the model's details and appropriate use is tailored primarily to individuals with ML or NLP expertise. To address this gap, we conduct a design inquiry into interactive model cards, which augment traditionally static model cards with affordances for exploring model documentation and interacting with the models themselves. Our investigation consists of an initial conceptual study with experts in ML, NLP, and AI Ethics, followed by a separate evaluative study with non-expert analysts who use ML models in their work. Using a semi-structured interview format coupled with a think-aloud protocol, we collected feedback from a total of 30 participants who engaged with different versions of standard and interactive model cards. Through a thematic analysis of the collected data, we identified several conceptual dimensions that summarize the strengths and limitations of standard and interactive model cards, including: stakeholders; design; guidance; understandability & interpretability; sensemaking & skepticism; and trust & safety. Our findings demonstrate the importance of carefully considered design and interactivity for orienting and supporting non-expert analysts using deep learning models, along with a need for consideration of broader sociotechnical contexts and organizational dynamics. We have also identified design elements, such as language, visual cues, and warnings, among others, that support interactivity and make non-interactive content accessible. We summarize our findings as design guidelines and discuss their implications for a human-centered approach towards AI/ML documentation.",
        "authors": [
            "Anamaria Crisan",
            "Margaret Drouhard",
            "Jesse Vig",
            "Nazneen Rajani"
        ],
        "submitted_date": "2022-05-05T19:19:28+00:00",
        "pdf_url": "https://arxiv.org/pdf/2205.02894v1",
        "similarity_score": 0.5522114634513855
    },
    {
        "arxiv_id": "2103.06076v2",
        "title": "Designing Disaggregated Evaluations of AI Systems: Choices, Considerations, and Tradeoffs",
        "abstract": "Disaggregated evaluations of AI systems, in which system performance is assessed and reported separately for different groups of people, are conceptually simple. However, their design involves a variety of choices. Some of these choices influence the results that will be obtained, and thus the conclusions that can be drawn; others influence the impacts -- both beneficial and harmful -- that a disaggregated evaluation will have on people, including the people whose data is used to conduct the evaluation. We argue that a deeper understanding of these choices will enable researchers and practitioners to design careful and conclusive disaggregated evaluations. We also argue that better documentation of these choices, along with the underlying considerations and tradeoffs that have been made, will help others when interpreting an evaluation's results and conclusions.",
        "authors": [
            "Solon Barocas",
            "Anhong Guo",
            "Ece Kamar",
            "Jacquelyn Krones",
            "Meredith Ringel Morris",
            "Jennifer Wortman Vaughan",
            "Duncan Wadsworth",
            "Hanna Wallach"
        ],
        "submitted_date": "2021-03-10T14:26:14+00:00",
        "pdf_url": "https://arxiv.org/pdf/2103.06076v2",
        "similarity_score": 0.5510806441307068
    },
    {
        "arxiv_id": "2503.15526v1",
        "title": "Assessment of AI-Generated Pediatric Rehabilitation SOAP-Note Quality",
        "abstract": "This study explores the integration of artificial intelligence (AI) or large language models (LLMs) into pediatric rehabilitation clinical documentation, focusing on the generation of SOAP (Subjective, Objective, Assessment, Plan) notes, which are essential for patient care. Creating complex documentation is time-consuming in pediatric settings. We evaluate the effectiveness of two AI tools; Copilot, a commercial LLM, and KAUWbot, a fine-tuned LLM developed for KidsAbility Centre for Child Development (an Ontario pediatric rehabilitation facility), in simplifying and automating this process. We focus on two key questions: (i) How does the quality of AI-generated SOAP notes based on short clinician summaries compare to human-authored notes, and (ii) To what extent is human editing necessary for improving AI-generated SOAP notes? We found no evidence of prior work assessing the quality of AI-generated clinical notes in pediatric rehabilitation.   We used a sample of 432 SOAP notes, evenly divided among human-authored, Copilot-generated, and KAUWbot-generated notes. We employ a blind evaluation by experienced clinicians based on a custom rubric. Statistical analysis is conducted to assess the quality of the notes and the impact of human editing. The results suggest that AI tools such as KAUWbot and Copilot can generate SOAP notes with quality comparable to those authored by humans. We highlight the potential for combining AI with human expertise to enhance clinical documentation and offer insights for the future integration of AI into pediatric rehabilitation practice and other settings for the management of clinical conditions.",
        "authors": [
            "Solomon Amenyo",
            "Maura R. Grossman",
            "Daniel G. Brown",
            "Brendan Wylie-Toal"
        ],
        "submitted_date": "2025-02-04T02:37:36+00:00",
        "pdf_url": "https://arxiv.org/pdf/2503.15526v1",
        "similarity_score": 0.5487031936645508
    },
    {
        "arxiv_id": "2406.13264v2",
        "title": "WONDERBREAD: A Benchmark for Evaluating Multimodal Foundation Models on Business Process Management Tasks",
        "abstract": "Existing ML benchmarks lack the depth and diversity of annotations needed for evaluating models on business process management (BPM) tasks. BPM is the practice of documenting, measuring, improving, and automating enterprise workflows. However, research has focused almost exclusively on one task - full end-to-end automation using agents based on multimodal foundation models (FMs) like GPT-4. This focus on automation ignores the reality of how most BPM tools are applied today - simply documenting the relevant workflow takes 60% of the time of the typical process optimization project. To address this gap we present WONDERBREAD, the first benchmark for evaluating multimodal FMs on BPM tasks beyond automation. Our contributions are: (1) a dataset containing 2928 documented workflow demonstrations; (2) 6 novel BPM tasks sourced from real-world applications ranging from workflow documentation to knowledge transfer to process improvement; and (3) an automated evaluation harness. Our benchmark shows that while state-of-the-art FMs can automatically generate documentation (e.g. recalling 88% of the steps taken in a video demonstration of a workflow), they struggle to re-apply that knowledge towards finer-grained validation of workflow completion (F1 < 0.3). We hope WONDERBREAD encourages the development of more \"human-centered\" AI tooling for enterprise applications and furthers the exploration of multimodal FMs for the broader universe of BPM tasks. We publish our dataset and experiments here: https://github.com/HazyResearch/wonderbread",
        "authors": [
            "Michael Wornow",
            "Avanika Narayan",
            "Ben Viggiano",
            "Ishan S. Khare",
            "Tathagat Verma",
            "Tibor Thompson",
            "Miguel Angel Fuentes Hernandez",
            "Sudharsan Sundar",
            "Chloe Trujillo",
            "Krrish Chawla",
            "Rongfei Lu",
            "Justin Shen",
            "Divya Nagaraj",
            "Joshua Martinez",
            "Vardhan Agrawal",
            "Althea Hudson",
            "Nigam H. Shah",
            "Christopher Re"
        ],
        "submitted_date": "2024-06-19T06:50:15+00:00",
        "pdf_url": "https://arxiv.org/pdf/2406.13264v2",
        "similarity_score": 0.547547459602356
    },
    {
        "arxiv_id": "2502.01635v1",
        "title": "The AI Agent Index",
        "abstract": "Leading AI developers and startups are increasingly deploying agentic AI systems that can plan and execute complex tasks with limited human involvement. However, there is currently no structured framework for documenting the technical components, intended uses, and safety features of agentic systems. To fill this gap, we introduce the AI Agent Index, the first public database to document information about currently deployed agentic AI systems. For each system that meets the criteria for inclusion in the index, we document the system's components (e.g., base model, reasoning implementation, tool use), application domains (e.g., computer use, software engineering), and risk management practices (e.g., evaluation results, guardrails), based on publicly available information and correspondence with developers. We find that while developers generally provide ample information regarding the capabilities and applications of agentic systems, they currently provide limited information regarding safety and risk management practices. The AI Agent Index is available online at https://aiagentindex.mit.edu/",
        "authors": [
            "Stephen Casper",
            "Luke Bailey",
            "Rosco Hunter",
            "Carson Ezell",
            "Emma Cabal√©",
            "Michael Gerovitch",
            "Stewart Slocum",
            "Kevin Wei",
            "Nikola Jurkovic",
            "Ariba Khan",
            "Phillip J. K. Christoffersen",
            "A. Pinar Ozisik",
            "Rakshit Trivedi",
            "Dylan Hadfield-Menell",
            "Noam Kolt"
        ],
        "submitted_date": "2025-02-03T18:59:13+00:00",
        "pdf_url": "https://arxiv.org/pdf/2502.01635v1",
        "similarity_score": 0.539310097694397
    },
    {
        "arxiv_id": "2510.00762v1",
        "title": "AI Where It Matters: Where, Why, and How Developers Want AI Support in Daily Work",
        "abstract": "Generative AI is reshaping software work, yet we lack clear guidance on where developers most need and want support, and how to design it responsibly. We report a large-scale, mixed-methods study of N=860 developers that examines where, why, and how they seek or limit AI help, providing the first task-aware, empirically validated mapping from developers' perceptions of their tasks to AI adoption patterns and responsible AI priorities. Using cognitive appraisal theory, we show that task evaluations predict openness to and use of AI, revealing distinct patterns: strong current use and a desire for improvement in core work (e.g., coding, testing); high demand to reduce toil (e.g., documentation, operations); and clear limits for identity- and relationship-centric work (e.g., mentoring). Priorities for responsible AI support vary by context: reliability and security for systems-facing tasks; transparency, alignment, and steerability to maintain control; and fairness and inclusiveness for human-facing work. Our results offer concrete, contextual guidance for delivering AI where it matters to developers and their work.",
        "authors": [
            "Rudrajit Choudhuri",
            "Carmen Badea",
            "Christian Bird",
            "Jenna Butler",
            "Rob DeLine",
            "Brian Houck"
        ],
        "submitted_date": "2025-10-01T10:51:03+00:00",
        "pdf_url": "https://arxiv.org/pdf/2510.00762v1",
        "similarity_score": 0.5384294390678406
    },
    {
        "arxiv_id": "2410.15528v1",
        "title": "Improving Clinical Documentation with AI: A Comparative Study of Sporo AI Scribe and GPT-4o mini",
        "abstract": "AI-powered medical scribes have emerged as a promising solution to alleviate the documentation burden in healthcare. Ambient AI scribes provide real-time transcription and automated data entry into Electronic Health Records (EHRs), with the potential to improve efficiency, reduce costs, and enhance scalability. Despite early success, the accuracy of AI scribes remains critical, as errors can lead to significant clinical consequences. Additionally, AI scribes face challenges in handling the complexity and variability of medical language and ensuring the privacy of sensitive patient data. This case study aims to evaluate Sporo Health's AI scribe, a multi-agent system leveraging fine-tuned medical LLMs, by comparing its performance with OpenAI's GPT-4o Mini on multiple performance metrics. Using a dataset of de-identified patient conversation transcripts, AI-generated summaries were compared to clinician-generated notes (the ground truth) based on clinical content recall, precision, and F1 scores. Evaluations were further supplemented by clinician satisfaction assessments using a modified Physician Documentation Quality Instrument revision 9 (PDQI-9), rated by both a medical student and a physician. The results show that Sporo AI consistently outperformed GPT-4o Mini, achieving higher recall, precision, and overall F1 scores. Moreover, the AI generated summaries provided by Sporo were rated more favorably in terms of accuracy, comprehensiveness, and relevance, with fewer hallucinations. These findings demonstrate that Sporo AI Scribe is an effective and reliable tool for clinical documentation, enhancing clinician workflows while maintaining high standards of privacy and security.",
        "authors": [
            "Chanseo Lee",
            "Sonu Kumar",
            "Kimon A. Vogt",
            "Sam Meraj"
        ],
        "submitted_date": "2024-10-20T22:48:40+00:00",
        "pdf_url": "https://arxiv.org/pdf/2410.15528v1",
        "similarity_score": 0.5370432138442993
    },
    {
        "arxiv_id": "2401.02986v1",
        "title": "Identification of Regulatory Requirements Relevant to Business Processes: A Comparative Study on Generative AI, Embedding-based Ranking, Crowd and Expert-driven Methods",
        "abstract": "Organizations face the challenge of ensuring compliance with an increasing amount of requirements from various regulatory documents. Which requirements are relevant depends on aspects such as the geographic location of the organization, its domain, size, and business processes. Considering these contextual factors, as a first step, relevant documents (e.g., laws, regulations, directives, policies) are identified, followed by a more detailed analysis of which parts of the identified documents are relevant for which step of a given business process. Nowadays the identification of regulatory requirements relevant to business processes is mostly done manually by domain and legal experts, posing a tremendous effort on them, especially for a large number of regulatory documents which might frequently change. Hence, this work examines how legal and domain experts can be assisted in the assessment of relevant requirements. For this, we compare an embedding-based NLP ranking method, a generative AI method using GPT-4, and a crowdsourced method with the purely manual method of creating relevancy labels by experts. The proposed methods are evaluated based on two case studies: an Australian insurance case created with domain experts and a global banking use case, adapted from SAP Signavio's workflow example of an international guideline. A gold standard is created for both BPMN2.0 processes and matched to real-world textual requirements from multiple regulatory documents. The evaluation and discussion provide insights into strengths and weaknesses of each method regarding applicability, automation, transparency, and reproducibility and provide guidelines on which method combinations will maximize benefits for given characteristics such as process usage, impact, and dynamics of an application scenario.",
        "authors": [
            "Catherine Sai",
            "Shazia Sadiq",
            "Lei Han",
            "Gianluca Demartini",
            "Stefanie Rinderle-Ma"
        ],
        "submitted_date": "2024-01-02T12:08:31+00:00",
        "pdf_url": "https://arxiv.org/pdf/2401.02986v1",
        "similarity_score": 0.5314769744873047
    },
    {
        "arxiv_id": "2504.01349v1",
        "title": "Tasks and Roles in Legal AI: Data Curation, Annotation, and Verification",
        "abstract": "The application of AI tools to the legal field feels natural: large legal document collections could be used with specialized AI to improve workflow efficiency for lawyers and ameliorate the \"justice gap\" for underserved clients. However, legal documents differ from the web-based text that underlies most AI systems. The challenges of legal AI are both specific to the legal domain, and confounded with the expectation of AI's high performance in high-stakes settings. We identify three areas of special relevance to practitioners: data curation, data annotation, and output verification. First, it is difficult to obtain usable legal texts. Legal collections are inconsistent, analog, and scattered for reasons technical, economic, and jurisdictional. AI tools can assist document curation efforts, but the lack of existing data also limits AI performance. Second, legal data annotation typically requires significant expertise to identify complex phenomena such as modes of judicial reasoning or controlling precedents. We describe case studies of AI systems that have been developed to improve the efficiency of human annotation in legal contexts and identify areas of underperformance. Finally, AI-supported work in the law is valuable only if results are verifiable and trustworthy. We describe both the abilities of AI systems to support evaluation of their outputs, as well as new approaches to systematic evaluation of computational systems in complex domains. We call on both legal and AI practitioners to collaborate across disciplines and to release open access materials to support the development of novel, high-performing, and reliable AI tools for legal applications.",
        "authors": [
            "Allison Koenecke",
            "Jed Stiglitz",
            "David Mimno",
            "Matthew Wilkens"
        ],
        "submitted_date": "2025-04-02T04:34:58+00:00",
        "pdf_url": "https://arxiv.org/pdf/2504.01349v1",
        "similarity_score": 0.5297403335571289
    },
    {
        "arxiv_id": "2508.00875v1",
        "title": "Preliminary suggestions for rigorous GPAI model evaluations",
        "abstract": "This document presents a preliminary compilation of general-purpose AI (GPAI) evaluation practices that may promote internal validity, external validity and reproducibility. It includes suggestions for human uplift studies and benchmark evaluations, as well as cross-cutting suggestions that may apply to many different evaluation types. Suggestions are organised across four stages in the evaluation life cycle: design, implementation, execution and documentation. Drawing from established practices in machine learning, statistics, psychology, economics, biology and other fields recognised to have important lessons for AI evaluation, these suggestions seek to contribute to the conversation on the nascent and evolving field of the science of GPAI evaluations. The intended audience of this document includes providers of GPAI models presenting systemic risk (GPAISR), for whom the EU AI Act lays out specific evaluation requirements; third-party evaluators; policymakers assessing the rigour of evaluations; and academic researchers developing or conducting GPAI evaluations.",
        "authors": [
            "Patricia Paskov",
            "Michael J. Byun",
            "Kevin Wei",
            "Toby Webster"
        ],
        "submitted_date": "2025-07-22T03:27:42+00:00",
        "pdf_url": "https://arxiv.org/pdf/2508.00875v1",
        "similarity_score": 0.528466522693634
    },
    {
        "arxiv_id": "2507.18932v2",
        "title": "MMESGBench: Pioneering Multimodal Understanding and Complex Reasoning Benchmark for ESG Tasks",
        "abstract": "Environmental, Social, and Governance (ESG) reports are essential for evaluating sustainability practices, ensuring regulatory compliance, and promoting financial transparency. However, these documents are often lengthy, structurally diverse, and multimodal, comprising dense text, structured tables, complex figures, and layout-dependent semantics. Existing AI systems often struggle to perform reliable document-level reasoning in such settings, and no dedicated benchmark currently exists in ESG domain. To fill the gap, we introduce \\textbf{MMESGBench}, a first-of-its-kind benchmark dataset targeted to evaluate multimodal understanding and complex reasoning across structurally diverse and multi-source ESG documents. This dataset is constructed via a human-AI collaborative, multi-stage pipeline. First, a multimodal LLM generates candidate question-answer (QA) pairs by jointly interpreting rich textual, tabular, and visual information from layout-aware document pages. Second, an LLM verifies the semantic accuracy, completeness, and reasoning complexity of each QA pair. This automated process is followed by an expert-in-the-loop validation, where domain specialists validate and calibrate QA pairs to ensure quality, relevance, and diversity. MMESGBench comprises 933 validated QA pairs derived from 45 ESG documents, spanning across seven distinct document types and three major ESG source categories. Questions are categorized as single-page, cross-page, or unanswerable, with each accompanied by fine-grained multimodal evidence. Initial experiments validate that multimodal and retrieval-augmented models substantially outperform text-only baselines, particularly on visually grounded and cross-page tasks. MMESGBench is publicly available as an open-source dataset at https://github.com/Zhanglei1103/MMESGBench.",
        "authors": [
            "Lei Zhang",
            "Xin Zhou",
            "Chaoyue He",
            "Di Wang",
            "Yi Wu",
            "Hong Xu",
            "Wei Liu",
            "Chunyan Miao"
        ],
        "submitted_date": "2025-07-25T03:58:07+00:00",
        "pdf_url": "https://arxiv.org/pdf/2507.18932v2",
        "similarity_score": 0.5262155532836914
    },
    {
        "arxiv_id": "2503.13786v2",
        "title": "Evaluating the Application of SOLID Principles in Modern AI Framework Architectures",
        "abstract": "This research evaluates the extent to which modern AI frameworks, specifically TensorFlow and scikit-learn, adhere to the SOLID design principles - Single Responsibility, Open/Closed, Liskov Substitution, Interface Segregation, and Dependency Inversion. Analyzing the frameworks architectural documentation and design philosophies, this research investigates architectural trade-offs when balancing software engineering best practices with AI-specific needs. I examined each frameworks documentation, source code, and architectural components to evaluate their adherence to these principles. The results show that both frameworks adopt certain aspects of SOLID design principles but make intentional trade-offs to address performance, scalability, and the experimental nature of AI development. TensorFlow focuses on performance and scalability, sometimes sacrificing strict adherence to principles like Single Responsibility and Interface Segregation. While scikit-learns design philosophy aligns more closely with SOLID principles through consistent interfaces and composition principles, sticking closer to SOLID guidelines but with occasional deviations for performance optimizations and scalability. This research discovered that applying SOLID principles in AI frameworks depends on context, as performance, scalability, and flexibility often require deviations from traditional software engineering principles. This research contributes to understanding how domain-specific constraints influence architectural decisions in modern AI frameworks and how these frameworks strategically adapted design choices to effectively balance these contradicting requirements.",
        "authors": [
            "Jonesh Shrestha"
        ],
        "submitted_date": "2025-03-18T00:37:23+00:00",
        "pdf_url": "https://arxiv.org/pdf/2503.13786v2",
        "similarity_score": 0.5254625082015991
    },
    {
        "arxiv_id": "2501.06497v2",
        "title": "PASS: Presentation Automation for Slide Generation and Speech",
        "abstract": "In today's fast-paced world, effective presentations have become an essential tool for communication in both online and offline meetings. The crafting of a compelling presentation requires significant time and effort, from gathering key insights to designing slides that convey information clearly and concisely. However, despite the wealth of resources available, people often find themselves manually extracting crucial points, analyzing data, and organizing content in a way that ensures clarity and impact. Furthermore, a successful presentation goes beyond just the slides; it demands rehearsal and the ability to weave a captivating narrative to fully engage the audience. Although there has been some exploration of automating document-to-slide generation, existing research is largely centered on converting research papers. In addition, automation of the delivery of these presentations has yet to be addressed. We introduce PASS, a pipeline used to generate slides from general Word documents, going beyond just research papers, which also automates the oral delivery of the generated slides. PASS analyzes user documents to create a dynamic, engaging presentation with an AI-generated voice. Additionally, we developed an LLM-based evaluation metric to assess our pipeline across three critical dimensions of presentations: relevance, coherence, and redundancy. The data and codes are available at https://github.com/AggarwalTushar/PASS.",
        "authors": [
            "Tushar Aggarwal",
            "Aarohi Bhand"
        ],
        "submitted_date": "2025-01-11T10:22:04+00:00",
        "pdf_url": "https://arxiv.org/pdf/2501.06497v2",
        "similarity_score": 0.5244666337966919
    },
    {
        "arxiv_id": "2509.16449v2",
        "title": "PersonaMatrix: A Recipe for Persona-Aware Evaluation of Legal Summarization",
        "abstract": "Legal documents are often long, dense, and difficult to comprehend, not only for laypeople but also for legal experts. While automated document summarization has great potential to improve access to legal knowledge, prevailing task-based evaluators overlook divergent user and stakeholder needs. Tool development is needed to encompass the technicality of a case summary for a litigator yet be accessible for a self-help public researching for their lawsuit. We introduce PersonaMatrix, a persona-by-criterion evaluation framework that scores summaries through the lens of six personas, including legal and non-legal users. We also introduce a controlled dimension-shifted pilot dataset of U.S. civil rights case summaries that varies along depth, accessibility, and procedural detail as well as Diversity-Coverage Index (DCI) to expose divergent optima of legal summary between persona-aware and persona-agnostic judges. This work enables refinement of legal AI summarization systems for both expert and non-expert users, with the potential to increase access to legal knowledge. The code base and data are publicly available in GitHub.",
        "authors": [
            "Tsz Fung Pang",
            "Maryam Berijanian",
            "Thomas Orth",
            "Breanna Shi",
            "Charlotte S. Alexander"
        ],
        "submitted_date": "2025-09-19T22:03:08+00:00",
        "pdf_url": "https://arxiv.org/pdf/2509.16449v2",
        "similarity_score": 0.5233919024467468
    },
    {
        "arxiv_id": "2410.18218v1",
        "title": "Optimizing the role of human evaluation in LLM-based spoken document summarization systems",
        "abstract": "The emergence of powerful LLMs has led to a paradigm shift in abstractive summarization of spoken documents. The properties that make LLMs so valuable for this task -- creativity, ability to produce fluent speech, and ability to abstract information from large corpora -- also present new challenges to evaluating their content. Quick, cost-effective automatic evaluations such as ROUGE and BERTScore offer promise, but do not yet show competitive performance when compared to human evaluations. We draw on methodologies from the social sciences to propose an evaluation paradigm for spoken document summarization explicitly tailored for generative AI content. We provide detailed evaluation criteria and best practices guidelines to ensure robustness in the experimental design, replicability, and trustworthiness of human evaluation studies. We additionally include two case studies that show how these human-in-the-loop evaluation methods have been implemented at a major U.S. technology company.",
        "authors": [
            "Margaret Kroll",
            "Kelsey Kraus"
        ],
        "submitted_date": "2024-10-23T18:37:14+00:00",
        "pdf_url": "https://arxiv.org/pdf/2410.18218v1",
        "similarity_score": 0.5209532976150513
    },
    {
        "arxiv_id": "2502.09479v1",
        "title": "Assessing Generative AI value in a public sector context: evidence from a field experiment",
        "abstract": "The emergence of Generative AI (Gen AI) has motivated an interest in understanding how it could be used to enhance productivity across various tasks. We add to research results for the performance impact of Gen AI on complex knowledge-based tasks in a public sector setting. In a pre-registered experiment, after establishing a baseline level of performance, we find mixed evidence for two types of composite tasks related to document understanding and data analysis. For the Documents task, the treatment group using Gen AI had a 17% improvement in answer quality scores (as judged by human evaluators) and a 34% improvement in task completion time compared to a control group. For the Data task, we find the Gen AI treatment group experienced a 12% reduction in quality scores and no significant difference in mean completion time compared to the control group. These results suggest that the benefits of Gen AI may be task and potentially respondent dependent. We also discuss field notes and lessons learned, as well as supplementary insights from a post-trial survey and feedback workshop with participants.",
        "authors": [
            "Trevor Fitzpatrick",
            "Seamus Kelly",
            "Patrick Carey",
            "David Walsh",
            "Ruairi Nugent"
        ],
        "submitted_date": "2025-02-13T16:43:32+00:00",
        "pdf_url": "https://arxiv.org/pdf/2502.09479v1",
        "similarity_score": 0.5194908976554871
    },
    {
        "arxiv_id": "2507.20046v1",
        "title": "Infogen: Generating Complex Statistical Infographics from Documents",
        "abstract": "Statistical infographics are powerful tools that simplify complex data into visually engaging and easy-to-understand formats. Despite advancements in AI, particularly with LLMs, existing efforts have been limited to generating simple charts, with no prior work addressing the creation of complex infographics from text-heavy documents that demand a deep understanding of the content. We address this gap by introducing the task of generating statistical infographics composed of multiple sub-charts (e.g., line, bar, pie) that are contextually accurate, insightful, and visually aligned. To achieve this, we define infographic metadata that includes its title and textual insights, along with sub-chart-specific details such as their corresponding data and alignment. We also present Infodat, the first benchmark dataset for text-to-infographic metadata generation, where each sample links a document to its metadata. We propose Infogen, a two-stage framework where fine-tuned LLMs first generate metadata, which is then converted into infographic code. Extensive evaluations on Infodat demonstrate that Infogen achieves state-of-the-art performance, outperforming both closed and open-source LLMs in text-to-statistical infographic generation.",
        "authors": [
            "Akash Ghosh",
            "Aparna Garimella",
            "Pritika Ramu",
            "Sambaran Bandyopadhyay",
            "Sriparna Saha"
        ],
        "submitted_date": "2025-07-26T19:38:46+00:00",
        "pdf_url": "https://arxiv.org/pdf/2507.20046v1",
        "similarity_score": 0.5194286108016968
    },
    {
        "arxiv_id": "2409.16307v1",
        "title": "DeepScore: A Comprehensive Approach to Measuring Quality in AI-Generated Clinical Documentation",
        "abstract": "Medical practitioners are rapidly adopting generative AI solutions for clinical documentation, leading to significant time savings and reduced stress. However, evaluating the quality of AI-generated documentation is a complex and ongoing challenge. This paper presents an overview of DeepScribe's methodologies for assessing and managing note quality, focusing on various metrics and the composite \"DeepScore\", an overall index of quality and accuracy. These methodologies aim to enhance the quality of patient care documentation through accountability and continuous improvement.",
        "authors": [
            "Jon Oleson"
        ],
        "submitted_date": "2024-09-10T23:06:48+00:00",
        "pdf_url": "https://arxiv.org/pdf/2409.16307v1",
        "similarity_score": 0.517078697681427
    },
    {
        "arxiv_id": "2409.15076v1",
        "title": "Enhancing Scientific Reproducibility Through Automated BioCompute Object Creation Using Retrieval-Augmented Generation from Publications",
        "abstract": "The exponential growth in computational power and accessibility has transformed the complexity and scale of bioinformatics research, necessitating standardized documentation for transparency, reproducibility, and regulatory compliance. The IEEE BioCompute Object (BCO) standard addresses this need but faces adoption challenges due to the overhead of creating compliant documentation, especially for legacy research. This paper presents a novel approach to automate the creation of BCOs from scientific papers using Retrieval-Augmented Generation (RAG) and Large Language Models (LLMs). We describe the development of the BCO assistant tool that leverages RAG to extract relevant information from source papers and associated code repositories, addressing key challenges such as LLM hallucination and long-context understanding. The implementation incorporates optimized retrieval processes, including a two-pass retrieval with re-ranking, and employs carefully engineered prompts for each BCO domain. We discuss the tool's architecture, extensibility, and evaluation methods, including automated and manual assessment approaches. The BCO assistant demonstrates the potential to significantly reduce the time and effort required for retroactive documentation of bioinformatics research while maintaining compliance with the standard. This approach opens avenues for AI-assisted scientific documentation and knowledge extraction from publications thereby enhancing scientific reproducibility. The BCO assistant tool and documentation is available at https://biocompute-objects.github.io/bco-rag/.",
        "authors": [
            "Sean Kim",
            "Raja Mazumder"
        ],
        "submitted_date": "2024-09-23T14:51:22+00:00",
        "pdf_url": "https://arxiv.org/pdf/2409.15076v1",
        "similarity_score": 0.5153661966323853
    },
    {
        "arxiv_id": "2404.02929v2",
        "title": "Using Large Language Models to Understand Telecom Standards",
        "abstract": "The Third Generation Partnership Project (3GPP) has successfully introduced standards for global mobility. However, the volume and complexity of these standards has increased over time, thus complicating access to relevant information for vendors and service providers. Use of Generative Artificial Intelligence (AI) and in particular Large Language Models (LLMs), may provide faster access to relevant information. In this paper, we evaluate the capability of state-of-art LLMs to be used as Question Answering (QA) assistants for 3GPP document reference. Our contribution is threefold. First, we provide a benchmark and measuring methods for evaluating performance of LLMs. Second, we do data preprocessing and fine-tuning for one of these LLMs and provide guidelines to increase accuracy of the responses that apply to all LLMs. Third, we provide a model of our own, TeleRoBERTa, that performs on-par with foundation LLMs but with an order of magnitude less number of parameters. Results show that LLMs can be used as a credible reference tool on telecom technical documents, and thus have potential for a number of different applications from troubleshooting and maintenance, to network operations and software product development.",
        "authors": [
            "Athanasios Karapantelakis",
            "Mukesh Thakur",
            "Alexandros Nikou",
            "Farnaz Moradi",
            "Christian Orlog",
            "Fitsum Gaim",
            "Henrik Holm",
            "Doumitrou Daniil Nimara",
            "Vincent Huang"
        ],
        "submitted_date": "2024-04-02T09:54:51+00:00",
        "pdf_url": "https://arxiv.org/pdf/2404.02929v2",
        "similarity_score": 0.5150704383850098
    },
    {
        "arxiv_id": "2506.21604v1",
        "title": "Evaluating VisualRAG: Quantifying Cross-Modal Performance in Enterprise Document Understanding",
        "abstract": "Current evaluation frameworks for multimodal generative AI struggle to establish trustworthiness, hindering enterprise adoption where reliability is paramount. We introduce a systematic, quantitative benchmarking framework to measure the trustworthiness of progressively integrating cross-modal inputs such as text, images, captions, and OCR within VisualRAG systems for enterprise document intelligence. Our approach establishes quantitative relationships between technical metrics and user-centric trust measures. Evaluation reveals that optimal modality weighting with weights of 30% text, 15% image, 25% caption, and 30% OCR improves performance by 57.3% over text-only baselines while maintaining computational efficiency. We provide comparative assessments of foundation models, demonstrating their differential impact on trustworthiness in caption generation and OCR extraction-a vital consideration for reliable enterprise AI. This work advances responsible AI deployment by providing a rigorous framework for quantifying and enhancing trustworthiness in multimodal RAG for critical enterprise applications.",
        "authors": [
            "Varun Mannam",
            "Fang Wang",
            "Xin Chen"
        ],
        "submitted_date": "2025-06-19T18:05:00+00:00",
        "pdf_url": "https://arxiv.org/pdf/2506.21604v1",
        "similarity_score": 0.5126066207885742
    },
    {
        "arxiv_id": "2404.05144v1",
        "title": "Enhancing Clinical Efficiency through LLM: Discharge Note Generation for Cardiac Patients",
        "abstract": "Medical documentation, including discharge notes, is crucial for ensuring patient care quality, continuity, and effective medical communication. However, the manual creation of these documents is not only time-consuming but also prone to inconsistencies and potential errors. The automation of this documentation process using artificial intelligence (AI) represents a promising area of innovation in healthcare. This study directly addresses the inefficiencies and inaccuracies in creating discharge notes manually, particularly for cardiac patients, by employing AI techniques, specifically large language model (LLM). Utilizing a substantial dataset from a cardiology center, encompassing wide-ranging medical records and physician assessments, our research evaluates the capability of LLM to enhance the documentation process. Among the various models assessed, Mistral-7B distinguished itself by accurately generating discharge notes that significantly improve both documentation efficiency and the continuity of care for patients. These notes underwent rigorous qualitative evaluation by medical expert, receiving high marks for their clinical relevance, completeness, readability, and contribution to informed decision-making and care planning. Coupled with quantitative analyses, these results confirm Mistral-7B's efficacy in distilling complex medical information into concise, coherent summaries. Overall, our findings illuminate the considerable promise of specialized LLM, such as Mistral-7B, in refining healthcare documentation workflows and advancing patient care. This study lays the groundwork for further integrating advanced AI technologies in healthcare, demonstrating their potential to revolutionize patient documentation and support better care outcomes.",
        "authors": [
            "HyoJe Jung",
            "Yunha Kim",
            "Heejung Choi",
            "Hyeram Seo",
            "Minkyoung Kim",
            "JiYe Han",
            "Gaeun Kee",
            "Seohyun Park",
            "Soyoung Ko",
            "Byeolhee Kim",
            "Suyeon Kim",
            "Tae Joon Jun",
            "Young-Hak Kim"
        ],
        "submitted_date": "2024-04-08T01:55:28+00:00",
        "pdf_url": "https://arxiv.org/pdf/2404.05144v1",
        "similarity_score": 0.5102524757385254
    },
    {
        "arxiv_id": "2107.11913v2",
        "title": "Measuring Ethics in AI with AI: A Methodology and Dataset Construction",
        "abstract": "Recently, the use of sound measures and metrics in Artificial Intelligence has become the subject of interest of academia, government, and industry. Efforts towards measuring different phenomena have gained traction in the AI community, as illustrated by the publication of several influential field reports and policy documents. These metrics are designed to help decision takers to inform themselves about the fast-moving and impacting influences of key advances in Artificial Intelligence in general and Machine Learning in particular. In this paper we propose to use such newfound capabilities of AI technologies to augment our AI measuring capabilities. We do so by training a model to classify publications related to ethical issues and concerns. In our methodology we use an expert, manually curated dataset as the training set and then evaluate a large set of research papers. Finally, we highlight the implications of AI metrics, in particular their contribution towards developing trustful and fair AI-based tools and technologies. Keywords: AI Ethics; AI Fairness; AI Measurement. Ethics in Computer Science.",
        "authors": [
            "Pedro H. C. Avelar",
            "Rafael B. Audibert",
            "Anderson R. Tavares",
            "Lu√≠s C. Lamb"
        ],
        "submitted_date": "2021-07-26T00:26:12+00:00",
        "pdf_url": "https://arxiv.org/pdf/2107.11913v2",
        "similarity_score": 0.5099092721939087
    },
    {
        "arxiv_id": "2505.01077v2",
        "title": "Zero-Shot Document-Level Biomedical Relation Extraction via Scenario-based Prompt Design in Two-Stage with LLM",
        "abstract": "With the advent of artificial intelligence (AI), many researchers are attempting to extract structured information from document-level biomedical literature by fine-tuning large language models (LLMs). However, they face significant challenges such as the need for expensive hardware, like high-performance GPUs and the high labor costs associated with annotating training datasets, especially in biomedical realm. Recent research on LLMs, such as GPT-4 and Llama3, has shown promising performance in zero-shot settings, inspiring us to explore a novel approach to achieve the same results from unannotated full documents using general LLMs with lower hardware and labor costs. Our approach combines two major stages: named entity recognition (NER) and relation extraction (RE). NER identifies chemical, disease and gene entities from the document with synonym and hypernym extraction using an LLM with a crafted prompt. RE extracts relations between entities based on predefined relation schemas and prompts. To enhance the effectiveness of prompt, we propose a five-part template structure and a scenario-based prompt design principles, along with evaluation method to systematically assess the prompts. Finally, we evaluated our approach against fine-tuning and pre-trained models on two biomedical datasets: ChemDisGene and CDR. The experimental results indicate that our proposed method can achieve comparable accuracy levels to fine-tuning and pre-trained models but with reduced human and hardware expenses.",
        "authors": [
            "Lei Zhao",
            "Ling Kang",
            "Quan Guo"
        ],
        "submitted_date": "2025-05-02T07:33:20+00:00",
        "pdf_url": "https://arxiv.org/pdf/2505.01077v2",
        "similarity_score": 0.5097483396530151
    },
    {
        "arxiv_id": "1904.01721v1",
        "title": "Explainable Text Classification in Legal Document Review A Case Study of Explainable Predictive Coding",
        "abstract": "In today's legal environment, lawsuits and regulatory investigations require companies to embark upon increasingly intensive data-focused engagements to identify, collect and analyze large quantities of data. When documents are staged for review the process can require companies to dedicate an extraordinary level of resources, both with respect to human resources, but also with respect to the use of technology-based techniques to intelligently sift through data. For several years, attorneys have been using a variety of tools to conduct this exercise, and most recently, they are accepting the use of machine learning techniques like text classification to efficiently cull massive volumes of data to identify responsive documents for use in these matters. In recent years, a group of AI and Machine Learning researchers have been actively researching Explainable AI. In an explainable AI system, actions or decisions are human understandable. In typical legal `document review' scenarios, a document can be identified as responsive, as long as one or more of the text snippets in a document are deemed responsive. In these scenarios, if predictive coding can be used to locate these responsive snippets, then attorneys could easily evaluate the model's document classification decision. When deployed with defined and explainable results, predictive coding can drastically enhance the overall quality and speed of the document review process by reducing the time it takes to review documents. The authors of this paper propose the concept of explainable predictive coding and simple explainable predictive coding methods to locate responsive snippets within responsive documents. We also report our preliminary experimental results using the data from an actual legal matter that entailed this type of document review.",
        "authors": [
            "Rishi Chhatwal",
            "Peter Gronvall",
            "Nathaniel Huber-Fliflet",
            "Robert Keeling",
            "Jianping Zhang",
            "Haozhen Zhao"
        ],
        "submitted_date": "2019-04-03T00:57:32+00:00",
        "pdf_url": "https://arxiv.org/pdf/1904.01721v1",
        "similarity_score": 0.508275032043457
    },
    {
        "arxiv_id": "2506.16990v1",
        "title": "TeXpert: A Multi-Level Benchmark for Evaluating LaTeX Code Generation by LLMs",
        "abstract": "LaTeX's precision and flexibility in typesetting have made it the gold standard for the preparation of scientific documentation. Large Language Models (LLMs) present a promising opportunity for researchers to produce publication-ready material using LaTeX with natural language instructions, yet current benchmarks completely lack evaluation of this ability. By introducing TeXpert, our benchmark dataset with natural language prompts for generating LaTeX code focused on components of scientific documents across multiple difficulty levels, we conduct an in-depth analysis of LLM performance in this regard and identify frequent error types. Our evaluation across open and closed-source LLMs highlights multiple key findings: LLMs excelling on standard benchmarks perform poorly in LaTeX generation with a significant accuracy drop-off as the complexity of tasks increases; open-source models like DeepSeek v3 and DeepSeek Coder strongly rival closed-source counterparts in LaTeX tasks; and formatting and package errors are unexpectedly prevalent, suggesting a lack of diverse LaTeX examples in the training datasets of most LLMs. Our dataset, code, and model evaluations are available at https://github.com/knowledge-verse-ai/TeXpert.",
        "authors": [
            "Sahil Kale",
            "Vijaykant Nadadur"
        ],
        "submitted_date": "2025-06-20T13:39:16+00:00",
        "pdf_url": "https://arxiv.org/pdf/2506.16990v1",
        "similarity_score": 0.506439208984375
    },
    {
        "arxiv_id": "2510.01244v1",
        "title": "Feasibility of Structuring Stress Documentation Using an Ontology-Guided Large Language Model",
        "abstract": "Stress, arising from the dynamic interaction between external stressors, individual appraisals, and physiological or psychological responses, significantly impacts health yet is often underreported and inconsistently documented, typically captured as unstructured free-text in electronic health records. Ambient AI technologies offer promise in reducing documentation burden, but predominantly generate unstructured narratives, limiting downstream clinical utility.   This study aimed to develop an ontology for mental stress and evaluate the feasibility of using a Large Language Model (LLM) to extract ontology-guided stress-related information from narrative text. The Mental Stress Ontology (MeSO) was developed by integrating theoretical models like the Transactional Model of Stress with concepts from 11 validated stress assessment tools. MeSO's structure and content were refined using Ontology Pitfall Scanner! and expert validation.   Using MeSO, six categories of stress-related information--stressor, stress response, coping strategy, duration, onset, and temporal profile--were extracted from 35 Reddit posts using Claude Sonnet 4. Human reviewers evaluated accuracy and ontology coverage. The final ontology included 181 concepts across eight top-level classes. Of 220 extractable stress-related items, the LLM correctly identified 172 (78.2%), misclassified 27 (12.3%), and missed 21 (9.5%). All correctly extracted items were accurately mapped to MeSO, although 24 relevant concepts were not yet represented in the ontology.   This study demonstrates the feasibility of using an ontology-guided LLM for structured extraction of stress-related information, offering potential to enhance the consistency and utility of stress documentation in ambient AI systems. Future work should involve clinical dialogue data and comparison across LLMs.",
        "authors": [
            "Hyeoneui Kim",
            "Jeongha Kim",
            "Huijing Xu",
            "Jinsun Jung",
            "Sunghoon Kang",
            "Sun Joo Jang"
        ],
        "submitted_date": "2025-09-24T04:10:57+00:00",
        "pdf_url": "https://arxiv.org/pdf/2510.01244v1",
        "similarity_score": 0.505082368850708
    },
    {
        "arxiv_id": "2311.09133v1",
        "title": "Explainable Text Classification Techniques in Legal Document Review: Locating Rationales without Using Human Annotated Training Text Snippets",
        "abstract": "US corporations regularly spend millions of dollars reviewing electronically-stored documents in legal matters. Recently, attorneys apply text classification to efficiently cull massive volumes of data to identify responsive documents for use in these matters. While text classification is regularly used to reduce the discovery costs of legal matters, it also faces a perception challenge: amongst lawyers, this technology is sometimes looked upon as a \"black box\". Put simply, no extra information is provided for attorneys to understand why documents are classified as responsive. In recent years, explainable machine learning has emerged as an active research area. In an explainable machine learning system, predictions or decisions made by a machine learning model are human understandable. In legal 'document review' scenarios, a document is responsive, because one or more of its small text snippets are deemed responsive. In these scenarios, if these responsive snippets can be located, then attorneys could easily evaluate the model's document classification decisions - this is especially important in the field of responsible AI. Our prior research identified that predictive models created using annotated training text snippets improved the precision of a model when compared to a model created using all of a set of documents' text as training. While interesting, manually annotating training text snippets is not generally practical during a legal document review. However, small increases in precision can drastically decrease the cost of large document reviews. Automating the identification of training text snippets without human review could then make the application of training text snippet-based models a practical approach.",
        "authors": [
            "Christian Mahoney",
            "Peter Gronvall",
            "Nathaniel Huber-Fliflet",
            "Jianping Zhang"
        ],
        "submitted_date": "2023-11-15T17:24:56+00:00",
        "pdf_url": "https://arxiv.org/pdf/2311.09133v1",
        "similarity_score": 0.502444326877594
    },
    {
        "arxiv_id": "2010.14202v3",
        "title": "A Clarifying Question Selection System from NTES_ALONG in Convai3 Challenge",
        "abstract": "This paper presents the participation of NetEase Game AI Lab team for the ClariQ challenge at Search-oriented Conversational AI (SCAI) EMNLP workshop in 2020. The challenge asks for a complete conversational information retrieval system that can understanding and generating clarification questions. We propose a clarifying question selection system which consists of response understanding, candidate question recalling and clarifying question ranking. We fine-tune a RoBERTa model to understand user's responses and use an enhanced BM25 model to recall the candidate questions. In clarifying question ranking stage, we reconstruct the training dataset and propose two models based on ELECTRA. Finally we ensemble the models by summing up their output probabilities and choose the question with the highest probability as the clarification question. Experiments show that our ensemble ranking model outperforms in the document relevance task and achieves the best recall@[20,30] metrics in question relevance task. And in multi-turn conversation evaluation in stage2, our system achieve the top score of all document relevance metrics.",
        "authors": [
            "Wenjie Ou",
            "Yue Lin"
        ],
        "submitted_date": "2020-10-27T11:22:53+00:00",
        "pdf_url": "https://arxiv.org/pdf/2010.14202v3",
        "similarity_score": 0.50204998254776
    },
    {
        "arxiv_id": "2408.12871v5",
        "title": "DeepDiveAI: Identifying AI Related Documents in Large Scale Literature Data",
        "abstract": "In this paper, we propose a method to automatically classify AI-related documents from large-scale literature databases, leading to the creation of an AI-related literature dataset, named DeepDiveAI. The dataset construction approach integrates expert knowledge with the capabilities of advanced models, structured across two global stages. In the first stage, expert-curated classification datasets are used to train an LSTM model, which classifies coarse AI related records from large-scale datasets. In the second stage, we use Qwen2.5 Plus to annotate a random 10% of the coarse AI-related records, which are then used to train a BERT binary classifier. This step further refines the coarse AI related record set to obtain the final DeepDiveAI dataset. Evaluation results demonstrate that the entire workflow can efficiently and accurately identify AI-related literature from large-scale datasets.",
        "authors": [
            "Zhou Xiaochen",
            "Liang Xingzhou",
            "Zou Hui",
            "Lu Yi",
            "Qu Jingjing"
        ],
        "submitted_date": "2024-08-23T07:05:12+00:00",
        "pdf_url": "https://arxiv.org/pdf/2408.12871v5",
        "similarity_score": 0.5006260871887207
    },
    {
        "arxiv_id": "2309.05447v2",
        "title": "DoG-Instruct: Towards Premium Instruction-Tuning Data via Text-Grounded Instruction Wrapping",
        "abstract": "The improvement of LLMs' instruction-following capabilities relies heavily on the availability of high-quality instruction-response pairs. Unfortunately, the current methods used to collect the pairs suffer from either unaffordable labor costs or severe hallucinations in the self-generation of LLM. To tackle these challenges, this paper proposes a scalable solution. It involves training LLMs to generate instruction-response pairs based on human-written documents, rather than relying solely on self-generation without context. Our proposed method not only exploits the advantages of human-written documents in reducing hallucinations but also utilizes an LLM to wrap the expression of documents, which enables us to bridge the gap between various document styles and the standard AI response. Experiments demonstrate that our method outperforms existing typical methods on multiple benchmarks. In particular, compared to the best-performing baseline, the LLM trained using our generated dataset exhibits a 10\\% relative improvement in performance on AlpacaEval, despite utilizing only 1/5 of its training data. Furthermore, a comprehensive manual evaluation validates the quality of the data we generated. Our trained wrapper is publicly available at https://github.com/Bahuia/Dog-Instruct.",
        "authors": [
            "Yongrui Chen",
            "Haiyun Jiang",
            "Xinting Huang",
            "Shuming Shi",
            "Guilin Qi"
        ],
        "submitted_date": "2023-09-11T13:41:18+00:00",
        "pdf_url": "https://arxiv.org/pdf/2309.05447v2",
        "similarity_score": 0.5005925297737122
    },
    {
        "arxiv_id": "2504.09283v1",
        "title": "Semantic Commit: Helping Users Update Intent Specifications for AI Memory at Scale",
        "abstract": "How do we update AI memory of user intent as intent changes? We consider how an AI interface may assist the integration of new information into a repository of natural language data. Inspired by software engineering concepts like impact analysis, we develop methods and a UI for managing semantic changes with non-local effects, which we call \"semantic conflict resolution.\" The user commits new intent to a project -- makes a \"semantic commit\" -- and the AI helps the user detect and resolve semantic conflicts within a store of existing information representing their intent (an \"intent specification\"). We develop an interface, SemanticCommit, to better understand how users resolve conflicts when updating intent specifications such as Cursor Rules and game design documents. A knowledge graph-based RAG pipeline drives conflict detection, while LLMs assist in suggesting resolutions. We evaluate our technique on an initial benchmark. Then, we report a 12 user within-subjects study of SemanticCommit for two task domains -- game design documents, and AI agent memory in the style of ChatGPT memories -- where users integrated new information into an existing list. Half of our participants adopted a workflow of impact analysis, where they would first flag conflicts without AI revisions then resolve conflicts locally, despite having access to a global revision feature. We argue that AI agent interfaces, such as software IDEs like Cursor and Windsurf, should provide affordances for impact analysis and help users validate AI retrieval independently from generation. Our work speaks to how AI agent designers should think about updating memory as a process that involves human feedback and decision-making.",
        "authors": [
            "Priyan Vaithilingam",
            "Munyeong Kim",
            "Frida-Cecilia Acosta-Parenteau",
            "Daniel Lee",
            "Amine Mhedhbi",
            "Elena L. Glassman",
            "Ian Arawjo"
        ],
        "submitted_date": "2025-04-12T17:07:08+00:00",
        "pdf_url": "https://arxiv.org/pdf/2504.09283v1",
        "similarity_score": 0.4986072778701782
    },
    {
        "arxiv_id": "2505.17047v1",
        "title": "Assessing the Quality of AI-Generated Clinical Notes: A Validated Evaluation of a Large Language Model Scribe",
        "abstract": "In medical practices across the United States, physicians have begun implementing generative artificial intelligence (AI) tools to perform the function of scribes in order to reduce the burden of documenting clinical encounters. Despite their widespread use, no established methods exist to gauge the quality of AI scribes. To address this gap, we developed a blinded study comparing the relative performance of large language model (LLM) generated clinical notes with those from field experts based on audio-recorded clinical encounters. Quantitative metrics from the Physician Documentation Quality Instrument (PDQI9) provided a framework to measure note quality, which we adapted to assess relative performance of AI generated notes. Clinical experts spanning 5 medical specialties used the PDQI9 tool to evaluate specialist-drafted Gold notes and LLM authored Ambient notes. Two evaluators from each specialty scored notes drafted from a total of 97 patient visits. We found uniformly high inter rater agreement (RWG greater than 0.7) between evaluators in general medicine, orthopedics, and obstetrics and gynecology, and moderate (RWG 0.5 to 0.7) to high inter rater agreement in pediatrics and cardiology. We found a modest yet significant difference in the overall note quality, wherein Gold notes achieved a score of 4.25 out of 5 and Ambient notes scored 4.20 out of 5 (p = 0.04). Our findings support the use of the PDQI9 instrument as a practical method to gauge the quality of LLM authored notes, as compared to human-authored notes.",
        "authors": [
            "Erin Palm",
            "Astrit Manikantan",
            "Mark E. Pepin",
            "Herprit Mahal",
            "Srikanth Subramanya Belwadi"
        ],
        "submitted_date": "2025-05-15T16:14:53+00:00",
        "pdf_url": "https://arxiv.org/pdf/2505.17047v1",
        "similarity_score": 0.49836885929107666
    },
    {
        "arxiv_id": "2510.05410v1",
        "title": "Aligning Language Models with Clinical Expertise: DPO for Heart Failure Nursing Documentation in Critical Care",
        "abstract": "Nursing documentation in intensive care units (ICUs) provides essential clinical intelligence but often suffers from inconsistent terminology, informal styles, and lack of standardization, challenges that are particularly critical in heart failure care. This study applies Direct Preference Optimization (DPO) to adapt Mistral-7B, a locally deployable language model, using 8,838 heart failure nursing notes from the MIMIC-III database and 21,210 preference pairs derived from expert-verified GPT outputs, model generations, and original notes. Evaluation across BLEU, ROUGE, BERTScore, Perplexity, and expert qualitative assessments demonstrates that DPO markedly enhances documentation quality. Specifically, BLEU increased by 84% (0.173 to 0.318), BERTScore improved by 7.6% (0.828 to 0.891), and expert ratings rose across accuracy (+14.4 points), completeness (+14.5 points), logical consistency (+14.1 points), readability (+11.1 points), and structural clarity (+6.0 points). These results indicate that DPO can align lightweight clinical language models with expert standards, supporting privacy-preserving, AI-assisted documentation within electronic health record systems to reduce administrative burden and improve ICU patient safety.",
        "authors": [
            "Junyi Fan",
            "Li Sun",
            "Negin Ashrafi",
            "Kamiar Alaei",
            "Maryam Pishgar"
        ],
        "submitted_date": "2025-10-06T22:04:37+00:00",
        "pdf_url": "https://arxiv.org/pdf/2510.05410v1",
        "similarity_score": 0.4982471466064453
    },
    {
        "arxiv_id": "2503.19217v2",
        "title": "LLM Benchmarking with LLaMA2: Evaluating Code Development Performance Across Multiple Programming Languages",
        "abstract": "The rapid evolution of large language models (LLMs) has opened new possibilities for automating various tasks in software development. This paper evaluates the capabilities of the Llama 2-70B model in automating these tasks for scientific applications written in commonly used programming languages. Using representative test problems, we assess the model's capacity to generate code, documentation, and unit tests, as well as its ability to translate existing code between commonly used programming languages. Our comprehensive analysis evaluates the compilation, runtime behavior, and correctness of the generated and translated code. Additionally, we assess the quality of automatically generated code, documentation and unit tests. Our results indicate that while Llama 2-70B frequently generates syntactically correct and functional code for simpler numerical tasks, it encounters substantial difficulties with more complex, parallelized, or distributed computations, requiring considerable manual corrections. We identify key limitations and suggest areas for future improvements to better leverage AI-driven automation in scientific computing workflows.",
        "authors": [
            "Patrick Diehl",
            "Nojoud Nader",
            "Maxim Moraru",
            "Steven R. Brandt"
        ],
        "submitted_date": "2025-03-24T23:46:14+00:00",
        "pdf_url": "https://arxiv.org/pdf/2503.19217v2",
        "similarity_score": 0.49804767966270447
    },
    {
        "arxiv_id": "2309.14805v1",
        "title": "Fine-tuning and aligning question answering models for complex information extraction tasks",
        "abstract": "The emergence of Large Language Models (LLMs) has boosted performance and possibilities in various NLP tasks. While the usage of generative AI models like ChatGPT opens up new opportunities for several business use cases, their current tendency to hallucinate fake content strongly limits their applicability to document analysis, such as information retrieval from documents. In contrast, extractive language models like question answering (QA) or passage retrieval models guarantee query results to be found within the boundaries of an according context document, which makes them candidates for more reliable information extraction in productive environments of companies. In this work we propose an approach that uses and integrates extractive QA models for improved feature extraction of German business documents such as insurance reports or medical leaflets into a document analysis solution. We further show that fine-tuning existing German QA models boosts performance for tailored extraction tasks of complex linguistic features like damage cause explanations or descriptions of medication appearance, even with using only a small set of annotated data. Finally, we discuss the relevance of scoring metrics for evaluating information extraction tasks and deduce a combined metric from Levenshtein distance, F1-Score, Exact Match and ROUGE-L to mimic the assessment criteria from human experts.",
        "authors": [
            "Matthias Engelbach",
            "Dennis Klau",
            "Felix Scheerer",
            "Jens Drawehn",
            "Maximilien Kintz"
        ],
        "submitted_date": "2023-09-26T10:02:21+00:00",
        "pdf_url": "https://arxiv.org/pdf/2309.14805v1",
        "similarity_score": 0.49280673265457153
    },
    {
        "arxiv_id": "2303.13216v1",
        "title": "A Case Study on AI Engineering Practices: Developing an Autonomous Stock Trading System",
        "abstract": "Today, many systems use artificial intelligence (AI) to solve complex problems. While this often increases system effectiveness, developing a production-ready AI-based system is a difficult task. Thus, solid AI engineering practices are required to ensure the quality of the resulting system and to improve the development process. While several practices have already been proposed for the development of AI-based systems, detailed practical experiences of applying these practices are rare.   In this paper, we aim to address this gap by collecting such experiences during a case study, namely the development of an autonomous stock trading system that uses machine learning functionality to invest in stocks. We selected 10 AI engineering practices from the literature and systematically applied them during development, with the goal to collect evidence about their applicability and effectiveness. Using structured field notes, we documented our experiences. Furthermore, we also used field notes to document challenges that occurred during the development, and the solutions we applied to overcome them. Afterwards, we analyzed the collected field notes, and evaluated how each practice improved the development. Lastly, we compared our evidence with existing literature.   Most applied practices improved our system, albeit to varying extent, and we were able to overcome all major challenges. The qualitative results provide detailed accounts about 10 AI engineering practices, as well as challenges and solutions associated with such a project. Our experiences therefore enrich the emerging body of evidence in this field, which may be especially helpful for practitioner teams new to AI engineering.",
        "authors": [
            "Marcel Grote",
            "Justus Bogner"
        ],
        "submitted_date": "2023-03-23T12:27:27+00:00",
        "pdf_url": "https://arxiv.org/pdf/2303.13216v1",
        "similarity_score": 0.48854970932006836
    },
    {
        "arxiv_id": "2410.14567v4",
        "title": "ELOQ: Resources for Enhancing LLM Detection of Out-of-Scope Questions",
        "abstract": "Retrieval-augmented generation (RAG) has become integral to large language models (LLMs), particularly for conversational AI systems where user questions may reference knowledge beyond the LLMs' training cutoff. However, many natural user questions lack well-defined answers, either due to limited domain knowledge or because the retrieval system returns documents that are relevant in appearance but uninformative in content. In such cases, LLMs often produce hallucinated answers without flagging them. While recent work has largely focused on questions with false premises, we study out-of-scope questions, where the retrieved document appears semantically similar to the question but lacks the necessary information to answer it. In this paper, we propose a guided hallucination-based approach ELOQ to automatically generate a diverse set of out-of-scope questions from post-cutoff documents, followed by human verification to ensure quality. We use this dataset to evaluate several LLMs on their ability to detect out-of-scope questions and generate appropriate responses. Finally, we introduce an improved detection method that enhances the reliability of LLM-based question-answering systems in handling out-of-scope questions.",
        "authors": [
            "Zhiyuan Peng",
            "Jinming Nian",
            "Alexandre Evfimievski",
            "Yi Fang"
        ],
        "submitted_date": "2024-10-18T16:11:29+00:00",
        "pdf_url": "https://arxiv.org/pdf/2410.14567v4",
        "similarity_score": 0.4868575930595398
    },
    {
        "arxiv_id": "2505.14661v2",
        "title": "Abacus: A Cost-Based Optimizer for Semantic Operator Systems",
        "abstract": "LLMs enable an exciting new class of data processing applications over large collections of unstructured documents. Several new programming frameworks have enabled developers to build these applications by composing them out of semantic operators: a declarative set of AI-powered data transformations with natural language specifications. These include LLM-powered maps, filters, joins, etc. used for document processing tasks such as information extraction, summarization, and more. While systems of semantic operators have achieved strong performance on benchmarks, they can be difficult to optimize. An optimizer for this setting must determine how to physically implement each semantic operator in a way that optimizes the system globally. Existing optimizers are limited in the number of optimizations they can apply, and most (if not all) cannot optimize system quality, cost, or latency subject to constraint(s) on the other dimensions. In this paper we present Abacus, an extensible, cost-based optimizer which searches for the best implementation of a semantic operator system given a (possibly constrained) optimization objective. Abacus estimates operator performance by leveraging a minimal set of validation examples and, if available, prior beliefs about operator performance. We evaluate Abacus on document processing workloads in the biomedical and legal domains (BioDEX; CUAD) and multi-modal question answering (MMQA). We demonstrate that systems optimized by Abacus achieve 18.7%-39.2% better quality and up to 23.6x lower cost and 4.2x lower latency than the next best system.",
        "authors": [
            "Matthew Russo",
            "Sivaprasad Sudhir",
            "Gerardo Vitagliano",
            "Chunwei Liu",
            "Tim Kraska",
            "Samuel Madden",
            "Michael Cafarella"
        ],
        "submitted_date": "2025-05-20T17:49:46+00:00",
        "pdf_url": "https://arxiv.org/pdf/2505.14661v2",
        "similarity_score": 0.485567182302475
    },
    {
        "arxiv_id": "2506.06921v1",
        "title": "Teaching Astronomy with Large Language Models",
        "abstract": "We present a study of LLM integration in final-year undergraduate astronomy education, examining how students develop AI literacy through structured guidance and documentation requirements. We developed AstroTutor, a domain-specific astronomy tutoring system enhanced with curated arXiv content, and deployed it alongside general-purpose LLMs in the course. Students documented their AI usage through homework reflections and post-course surveys. We analyzed student evolution in AI interaction strategies and conducted experimental comparisons of LLM-assisted versus traditional grading methods. LLM grading showed strong correlation with human evaluation while providing more detailed and consistent feedback. We also piloted LLM-facilitated interview-based examinations as a scalable alternative to traditional assessments, demonstrating potential for individualized evaluation that addresses common testing limitations. Students experienced decreased rather than increased reliance on LLMs over the semester, developing critical evaluation skills and strategic tool selection. They evolved from basic assistance-seeking to verification workflows, with documentation requirements fostering metacognitive awareness. Students developed effective prompting strategies, contextual enrichment techniques, and cross-verification practices. Our findings suggest that structured LLM integration with transparency requirements and domain-specific tools can enhance astronomy education while building essential AI literacy skills. We provide implementation guidelines for educators and make our AstroTutor repository freely available.",
        "authors": [
            "Yuan-Sen Ting",
            "Teaghan O'Briain"
        ],
        "submitted_date": "2025-06-07T21:00:01+00:00",
        "pdf_url": "https://arxiv.org/pdf/2506.06921v1",
        "similarity_score": 0.4855539798736572
    },
    {
        "arxiv_id": "2508.13774v1",
        "title": "Agentic DraCor and the Art of Docstring Engineering: Evaluating MCP-empowered LLM Usage of the DraCor API",
        "abstract": "This paper reports on the implementation and evaluation of a Model Context Protocol (MCP) server for DraCor, enabling Large Language Models (LLM) to autonomously interact with the DraCor API. We conducted experiments focusing on tool selection and application by the LLM, employing a qualitative approach that includes systematic observation of prompts to understand how LLMs behave when using MCP tools, evaluating \"Tool Correctness\", \"Tool-Calling Efficiency\", and \"Tool-Use Reliability\". Our findings highlight the importance of \"Docstring Engineering\", defined as reflexively crafting tool documentation to optimize LLM-tool interaction. Our experiments demonstrate both the promise of agentic AI for research in Computational Literary Studies and the essential infrastructure development needs for reliable Digital Humanities infrastructures.",
        "authors": [
            "Peer Trilcke",
            "Ingo B√∂rner",
            "Henny Sluyter-G√§thje",
            "Daniil Skorinkin",
            "Frank Fischer",
            "Carsten Milling"
        ],
        "submitted_date": "2025-08-19T12:21:21+00:00",
        "pdf_url": "https://arxiv.org/pdf/2508.13774v1",
        "similarity_score": 0.4854455888271332
    },
    {
        "arxiv_id": "1810.03993v2",
        "title": "Model Cards for Model Reporting",
        "abstract": "Trained machine learning models are increasingly used to perform high-impact tasks in areas such as law enforcement, medicine, education, and employment. In order to clarify the intended use cases of machine learning models and minimize their usage in contexts for which they are not well suited, we recommend that released models be accompanied by documentation detailing their performance characteristics. In this paper, we propose a framework that we call model cards, to encourage such transparent model reporting. Model cards are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phenotypic groups (e.g., race, geographic location, sex, Fitzpatrick skin type) and intersectional groups (e.g., age and race, or sex and Fitzpatrick skin type) that are relevant to the intended application domains. Model cards also disclose the context in which models are intended to be used, details of the performance evaluation procedures, and other relevant information. While we focus primarily on human-centered machine learning models in the application fields of computer vision and natural language processing, this framework can be used to document any trained machine learning model. To solidify the concept, we provide cards for two supervised models: One trained to detect smiling faces in images, and one trained to detect toxic comments in text. We propose model cards as a step towards the responsible democratization of machine learning and related AI technology, increasing transparency into how well AI technology works. We hope this work encourages those releasing trained machine learning models to accompany model releases with similar detailed evaluation numbers and other relevant documentation.",
        "authors": [
            "Margaret Mitchell",
            "Simone Wu",
            "Andrew Zaldivar",
            "Parker Barnes",
            "Lucy Vasserman",
            "Ben Hutchinson",
            "Elena Spitzer",
            "Inioluwa Deborah Raji",
            "Timnit Gebru"
        ],
        "submitted_date": "2018-10-05T22:33:43+00:00",
        "pdf_url": "https://arxiv.org/pdf/1810.03993v2",
        "similarity_score": 0.4840606153011322
    },
    {
        "arxiv_id": "2403.15394v1",
        "title": "\"Model Cards for Model Reporting\" in 2024: Reclassifying Category of Ethical Considerations in Terms of Trustworthiness and Risk Management",
        "abstract": "In 2019, the paper entitled \"Model Cards for Model Reporting\" introduced a new tool for documenting model performance and encouraged the practice of transparent reporting for a defined list of categories. One of the categories detailed in that paper is ethical considerations, which includes the subcategories of data, human life, mitigations, risks and harms, and use cases. We propose to reclassify this category in the original model card due to the recent maturing of the field known as trustworthy AI, a term which analyzes whether the algorithmic properties of the model indicate that the AI system is deserving of trust from its stakeholders. In our examination of trustworthy AI, we highlight three respected organizations - the European Commission's High-Level Expert Group on AI, the OECD, and the U.S.-based NIST - that have written guidelines on various aspects of trustworthy AI. These recent publications converge on numerous characteristics of the term, including accountability, explainability, fairness, privacy, reliability, robustness, safety, security, and transparency, while recognizing that the implementation of trustworthy AI varies by context. Our reclassification of the original model-card category known as ethical considerations involves a two-step process: 1) adding a new category known as trustworthiness, where the subcategories will be derived from the discussion of trustworthy AI in our paper, and 2) maintaining the subcategories of ethical considerations under a renamed category known as risk environment and risk management, a title which we believe better captures today's understanding of the essence of these topics. We hope that this reclassification will further the goals of the original paper and continue to prompt those releasing trained models to accompany these models with documentation that will assist in the evaluation of their algorithmic properties.",
        "authors": [
            "DeBrae Kennedy-Mayo",
            "Jake Gord"
        ],
        "submitted_date": "2024-02-15T14:56:00+00:00",
        "pdf_url": "https://arxiv.org/pdf/2403.15394v1",
        "similarity_score": 0.4824810028076172
    },
    {
        "arxiv_id": "1912.09501v1",
        "title": "A Framework for Explainable Text Classification in Legal Document Review",
        "abstract": "Companies regularly spend millions of dollars producing electronically-stored documents in legal matters. Recently, parties on both sides of the 'legal aisle' are accepting the use of machine learning techniques like text classification to cull massive volumes of data and to identify responsive documents for use in these matters. While text classification is regularly used to reduce the discovery costs in legal matters, it also faces a peculiar perception challenge: amongst lawyers, this technology is sometimes looked upon as a \"black box\", little information provided for attorneys to understand why documents are classified as responsive. In recent years, a group of AI and ML researchers have been actively researching Explainable AI, in which actions or decisions are human understandable. In legal document review scenarios, a document can be identified as responsive, if one or more of its text snippets are deemed responsive. In these scenarios, if text classification can be used to locate these snippets, then attorneys could easily evaluate the model's classification decision. When deployed with defined and explainable results, text classification can drastically enhance overall quality and speed of the review process by reducing the review time. Moreover, explainable predictive coding provides lawyers with greater confidence in the results of that supervised learning task. This paper describes a framework for explainable text classification as a valuable tool in legal services: for enhancing the quality and efficiency of legal document review and for assisting in locating responsive snippets within responsive documents. This framework has been implemented in our legal analytics product, which has been used in hundreds of legal matters. We also report our experimental results using the data from an actual legal matter that used this type of document review.",
        "authors": [
            "Christian J. Mahoney",
            "Jianping Zhang",
            "Nathaniel Huber-Fliflet",
            "Peter Gronvall",
            "Haozhen Zhao"
        ],
        "submitted_date": "2019-12-19T19:07:23+00:00",
        "pdf_url": "https://arxiv.org/pdf/1912.09501v1",
        "similarity_score": 0.47830674052238464
    },
    {
        "arxiv_id": "2508.16713v1",
        "title": "CelloAI: Leveraging Large Language Models for HPC Software Development in High Energy Physics",
        "abstract": "Next-generation High Energy Physics (HEP) experiments will generate unprecedented data volumes, necessitating High Performance Computing (HPC) integration alongside traditional high-throughput computing. However, HPC adoption in HEP is hindered by the challenge of porting legacy software to heterogeneous architectures and the sparse documentation of these complex scientific codebases. We present CelloAI, a locally hosted coding assistant that leverages Large Language Models (LLMs) with retrieval-augmented generation (RAG) to support HEP code documentation and generation. This local deployment ensures data privacy, eliminates recurring costs and provides access to large context windows without external dependencies. CelloAI addresses two primary use cases, code documentation and code generation, through specialized components. For code documentation, the assistant provides: (a) Doxygen style comment generation for all functions and classes by retrieving relevant information from RAG sources (papers, posters, presentations), (b) file-level summary generation, and (c) an interactive chatbot for code comprehension queries. For code generation, CelloAI employs syntax-aware chunking strategies that preserve syntactic boundaries during embedding, improving retrieval accuracy in large codebases. The system integrates callgraph knowledge to maintain dependency awareness during code modifications and provides AI-generated suggestions for performance optimization and accurate refactoring. We evaluate CelloAI using real-world HEP applications from ATLAS, CMS, and DUNE experiments, comparing different embedding models for code retrieval effectiveness. Our results demonstrate the AI assistant's capability to enhance code understanding and support reliable code generation while maintaining the transparency and safety requirements essential for scientific computing environments.",
        "authors": [
            "Mohammad Atif",
            "Kriti Chopra",
            "Ozgur Kilic",
            "Tianle Wang",
            "Zhihua Dong",
            "Charles Leggett",
            "Meifeng Lin",
            "Paolo Calafiura",
            "Salman Habib"
        ],
        "submitted_date": "2025-08-22T15:17:44+00:00",
        "pdf_url": "https://arxiv.org/pdf/2508.16713v1",
        "similarity_score": 0.4770837426185608
    },
    {
        "arxiv_id": "2502.20616v2",
        "title": "PersonaBench: Evaluating AI Models on Understanding Personal Information through Accessing (Synthetic) Private User Data",
        "abstract": "Personalization is critical in AI assistants, particularly in the context of private AI models that work with individual users. A key scenario in this domain involves enabling AI models to access and interpret a user's private data (e.g., conversation history, user-AI interactions, app usage) to understand personal details such as biographical information, preferences, and social connections. However, due to the sensitive nature of such data, there are no publicly available datasets that allow us to assess an AI model's ability to understand users through direct access to personal information.   To address this gap, we introduce a synthetic data generation pipeline that creates diverse, realistic user profiles and private documents simulating human activities. Leveraging this synthetic data, we present PersonaBench, a benchmark designed to evaluate AI models' performance in understanding personal information derived from simulated private user data.   We evaluate Retrieval-Augmented Generation (RAG) pipelines using questions directly related to a user's personal information, supported by the relevant private documents provided to the models. Our results reveal that current retrieval-augmented AI models struggle to answer private questions by extracting personal information from user documents, highlighting the need for improved methodologies to enhance personalization capabilities in AI.",
        "authors": [
            "Juntao Tan",
            "Liangwei Yang",
            "Zuxin Liu",
            "Zhiwei Liu",
            "Rithesh Murthy",
            "Tulika Manoj Awalgaonkar",
            "Jianguo Zhang",
            "Weiran Yao",
            "Ming Zhu",
            "Shirley Kokane",
            "Silvio Savarese",
            "Huan Wang",
            "Caiming Xiong",
            "Shelby Heinecke"
        ],
        "submitted_date": "2025-02-28T00:43:35+00:00",
        "pdf_url": "https://arxiv.org/pdf/2502.20616v2",
        "similarity_score": 0.47388869524002075
    },
    {
        "arxiv_id": "2509.02241v1",
        "title": "LLMs for LLMs: A Structured Prompting Methodology for Long Legal Documents",
        "abstract": "The rise of Large Language Models (LLMs) has had a profoundly transformative effect on a number of fields and domains. However, their uptake in Law has proven more challenging due to the important issues of reliability and transparency. In this study, we present a structured prompting methodology as a viable alternative to the often expensive fine-tuning, with the capability of tacking long legal documents from the CUAD dataset on the task of information retrieval. Each document is first split into chunks via a system of chunking and augmentation, addressing the long document problem. Then, alongside an engineered prompt, the input is fed into QWEN-2 to produce a set of answers for each question. Finally, we tackle the resulting candidate selection problem with the introduction of the Distribution-based Localisation and Inverse Cardinality Weighting heuristics. This approach leverages a general purpose model to promote long term scalability, prompt engineering to increase reliability and the two heuristic strategies to reduce the impact of the black box effect. Whilst our model performs up to 9\\% better than the previously presented method, reaching state-of-the-art performance, it also highlights the limiting factor of current automatic evaluation metrics for question answering, serving as a call to action for future research. However, the chief aim of this work is to underscore the potential of structured prompt engineering as a useful, yet under-explored, tool in ensuring accountability and responsibility of AI in the legal domain, and beyond.",
        "authors": [
            "Strahinja Klem",
            "Noura Al Moubayed"
        ],
        "submitted_date": "2025-09-02T12:09:49+00:00",
        "pdf_url": "https://arxiv.org/pdf/2509.02241v1",
        "similarity_score": 0.4730006456375122
    },
    {
        "arxiv_id": "2409.17815v2",
        "title": "DREAMS: A python framework for Training Deep Learning Models on EEG Data with Model Card Reporting for Medical Applications",
        "abstract": "Electroencephalography (EEG) provides a non-invasive way to observe brain activity in real time. Deep learning has enhanced EEG analysis, enabling meaningful pattern detection for clinical and research purposes. However, most existing frameworks for EEG data analysis are either focused on preprocessing techniques or deep learning model development, often overlooking the crucial need for structured documentation and model interpretability. In this paper, we introduce DREAMS (Deep REport for AI ModelS), a Python-based framework designed to generate automated model cards for deep learning models applied to EEG data. Unlike generic model reporting tools, DREAMS is specifically tailored for EEG-based deep learning applications, incorporating domain-specific metadata, preprocessing details, performance metrics, and uncertainty quantification. The framework seamlessly integrates with deep learning pipelines, providing structured YAML-based documentation. We evaluate DREAMS through two case studies: an EEG emotion classification task using the FACED dataset and a abnormal EEG classification task using the Temple Univeristy Hospital (TUH) Abnormal dataset. These evaluations demonstrate how the generated model card enhances transparency by documenting model performance, dataset biases, and interpretability limitations. Unlike existing model documentation approaches, DREAMS provides visualized performance metrics, dataset alignment details, and model uncertainty estimations, making it a valuable tool for researchers and clinicians working with EEG-based AI. The source code for DREAMS is open-source, facilitating broad adoption in healthcare AI, research, and ethical AI development.",
        "authors": [
            "Rabindra Khadka",
            "Pedro G Lind",
            "Anis Yazidi",
            "Asma Belhadi"
        ],
        "submitted_date": "2024-09-26T13:12:13+00:00",
        "pdf_url": "https://arxiv.org/pdf/2409.17815v2",
        "similarity_score": 0.47116878628730774
    },
    {
        "arxiv_id": "2509.24854v1",
        "title": "Interface Design to Support Legal Reading and Writing: Insights from Interviews with Legal Experts",
        "abstract": "Legal professionals spend significant time reading, writing, and interpreting complex documents, yet research has not fully captured how they approach these tasks or what they expect from skimming and writing-support tools. To examine practices and views on emerging tools, we interviewed 22 legal professionals about workflows, challenges, and technology use. In each session, we leveraged prior HCI-based skimming and writing prototypes that surface emergent cross-document relationships and support AI-resilient interaction (noticing, judging, and recovering from model errors or unexpected behavior); participants completed a contextual fit evaluation to assess whether and how they would use the tools, which document types, and at what stages in their work. Our analysis details limitations and challenges in workflows, domain-specific feedback on AI-resilient interfaces, and expert insights on legal tech design. These findings offer actionable guidance for technology designers developing reading and writing-support for legal professionals, and for legal professionals seeking peer-informed tool integration strategies.",
        "authors": [
            "Chelse Swoopes",
            "Ziwei Gu",
            "Elena L. Glassman"
        ],
        "submitted_date": "2025-09-29T14:40:09+00:00",
        "pdf_url": "https://arxiv.org/pdf/2509.24854v1",
        "similarity_score": 0.4710312783718109
    },
    {
        "arxiv_id": "2412.10487v2",
        "title": "HyperGraphOS: A Modern Meta-Operating System for the Scientific and Engineering Domains",
        "abstract": "This paper presents HyperGraphOS, a significant innovation in the domain of operating systems, specifically designed to address the needs of scientific and engineering domains. This platform aims to combine model-based engineering, graph modeling, data containers, and documents, along with tools for handling computational elements. HyperGraphOS functions as an Operating System offering to users an infinite workspace for creating and managing complex models represented as graphs with customizable semantics. By leveraging a web-based architecture, it requires only a modern web browser for access, allowing organization of knowledge, documents, and content into models represented in a network of workspaces. Elements of the workspace are defined in terms of domain-specific languages (DSLs). These DSLs are pivotal for navigating workspaces, generating code, triggering AI components, and organizing information and processes. The models' dual nature as both visual drawings and data structures allows dynamic modifications and inspections both interactively as well as programaticaly. We evaluated HyperGraphOS's efficiency and applicability across a large set of diverse domains, including the design and development of a virtual Avatar dialog system, a robotic task planner based on large language models (LLMs), a new meta-model for feature-based code development and many others. Our findings show that HyperGraphOS offers substantial benefits in the interaction with a computer as information system, as platoform for experiments and data analysis, as streamlined engineering processes, demonstrating enhanced flexibility in managing data, computation and documents, showing an innovative approaches to persistent desktop environments.",
        "authors": [
            "Antonello Ceravola",
            "Frank Joublin"
        ],
        "submitted_date": "2024-12-13T15:18:39+00:00",
        "pdf_url": "https://arxiv.org/pdf/2412.10487v2",
        "similarity_score": 0.469346821308136
    },
    {
        "arxiv_id": "2510.01453v1",
        "title": "The Command Line GUIde: Graphical Interfaces from Man Pages via AI",
        "abstract": "Although birthed in the era of teletypes, the command line shell survived the graphical interface revolution of the 1980's and lives on in modern desktop operating systems. The command line provides access to powerful functionality not otherwise exposed on the computer, but requires users to recall textual syntax and carefully scour documentation. In contrast, graphical interfaces let users organically discover and invoke possible actions through widgets and menus. To better expose the power of the command line, we demonstrate a mechanism for automatically creating graphical interfaces for command line tools by translating their documentation (in the form of man pages) into interface specifications via AI. Using these specifications, our user-facing system, called GUIde, presents the command options to the user graphically. We evaluate the generated interfaces on a corpus of commands to show to what degree GUIde offers thorough graphical interfaces for users' real-world command line tasks.",
        "authors": [
            "Saketh Ram Kasibatla",
            "Kiran Medleri Hiremath",
            "Raven Rothkopf",
            "Sorin Lerner",
            "Haijun Xia",
            "Brian Hempel"
        ],
        "submitted_date": "2025-10-01T20:46:53+00:00",
        "pdf_url": "https://arxiv.org/pdf/2510.01453v1",
        "similarity_score": 0.46907520294189453
    },
    {
        "arxiv_id": "2508.10004v1",
        "title": "User Perception of Attention Visualizations: Effects on Interpretability Across Evidence-Based Medical Documents",
        "abstract": "The attention mechanism is a core component of the Transformer architecture. Beyond improving performance, attention has been proposed as a mechanism for explainability via attention weights, which are associated with input features (e.g., tokens in a document). In this context, larger attention weights may imply more relevant features for the model's prediction. In evidence-based medicine, such explanations could support physicians' understanding and interaction with AI systems used to categorize biomedical literature. However, there is still no consensus on whether attention weights provide helpful explanations. Moreover, little research has explored how visualizing attention affects its usefulness as an explanation aid. To bridge this gap, we conducted a user study to evaluate whether attention-based explanations support users in biomedical document classification and whether there is a preferred way to visualize them. The study involved medical experts from various disciplines who classified articles based on study design (e.g., systematic reviews, broad synthesis, randomized and non-randomized trials). Our findings show that the Transformer model (XLNet) classified documents accurately; however, the attention weights were not perceived as particularly helpful for explaining the predictions. However, this perception varied significantly depending on how attention was visualized. Contrary to Munzner's principle of visual effectiveness, which favors precise encodings like bar length, users preferred more intuitive formats, such as text brightness or background color. While our results do not confirm the overall utility of attention weights for explanation, they suggest that their perceived helpfulness is influenced by how they are visually presented.",
        "authors": [
            "Andr√©s Carvallo",
            "Denis Parra",
            "Peter Brusilovsky",
            "Hernan Valdivieso",
            "Gabriel Rada",
            "Ivania Donoso",
            "Vladimir Araujo"
        ],
        "submitted_date": "2025-08-05T13:24:52+00:00",
        "pdf_url": "https://arxiv.org/pdf/2508.10004v1",
        "similarity_score": 0.46684402227401733
    },
    {
        "arxiv_id": "2306.00007v1",
        "title": "Datasets for Portuguese Legal Semantic Textual Similarity: Comparing weak supervision and an annotation process approaches",
        "abstract": "The Brazilian judiciary has a large workload, resulting in a long time to finish legal proceedings. Brazilian National Council of Justice has established in Resolution 469/2022 formal guidance for document and process digitalization opening up the possibility of using automatic techniques to help with everyday tasks in the legal field, particularly in a large number of texts yielded on the routine of law procedures. Notably, Artificial Intelligence (AI) techniques allow for processing and extracting useful information from textual data, potentially speeding up the process. However, datasets from the legal domain required by several AI techniques are scarce and difficult to obtain as they need labels from experts. To address this challenge, this article contributes with four datasets from the legal domain, two with documents and metadata but unlabeled, and another two labeled with a heuristic aiming at its use in textual semantic similarity tasks. Also, to evaluate the effectiveness of the proposed heuristic label process, this article presents a small ground truth dataset generated from domain expert annotations. The analysis of ground truth labels highlights that semantic analysis of domain text can be challenging even for domain experts. Also, the comparison between ground truth and heuristic labels shows that heuristic labels are useful.",
        "authors": [
            "Daniel da Silva Junior",
            "Paulo Roberto dos S. Corval",
            "Aline Paes",
            "Daniel de Oliveira"
        ],
        "submitted_date": "2023-05-29T18:27:10+00:00",
        "pdf_url": "https://arxiv.org/pdf/2306.00007v1",
        "similarity_score": 0.4667677581310272
    },
    {
        "arxiv_id": "2504.03486v1",
        "title": "Structured Legal Document Generation in India: A Model-Agnostic Wrapper Approach with VidhikDastaavej",
        "abstract": "Automating legal document drafting can significantly enhance efficiency, reduce manual effort, and streamline legal workflows. While prior research has explored tasks such as judgment prediction and case summarization, the structured generation of private legal documents in the Indian legal domain remains largely unaddressed. To bridge this gap, we introduce VidhikDastaavej, a novel, anonymized dataset of private legal documents, and develop NyayaShilp, a fine-tuned legal document generation model specifically adapted to Indian legal texts. We propose a Model-Agnostic Wrapper (MAW), a two-step framework that first generates structured section titles and then iteratively produces content while leveraging retrieval-based mechanisms to ensure coherence and factual accuracy. We benchmark multiple open-source LLMs, including instruction-tuned and domain-adapted versions, alongside proprietary models for comparison. Our findings indicate that while direct fine-tuning on small datasets does not always yield improvements, our structured wrapper significantly enhances coherence, factual adherence, and overall document quality while mitigating hallucinations. To ensure real-world applicability, we developed a Human-in-the-Loop (HITL) Document Generation System, an interactive user interface that enables users to specify document types, refine section details, and generate structured legal drafts. This tool allows legal professionals and researchers to generate, validate, and refine AI-generated legal documents efficiently. Extensive evaluations, including expert assessments, confirm that our framework achieves high reliability in structured legal drafting. This research establishes a scalable and adaptable foundation for AI-assisted legal drafting in India, offering an effective approach to structured legal document generation.",
        "authors": [
            "Shubham Kumar Nigam",
            "Balaramamahanthi Deepak Patnaik",
            "Ajay Varghese Thomas",
            "Noel Shallum",
            "Kripabandhu Ghosh",
            "Arnab Bhattacharya"
        ],
        "submitted_date": "2025-04-04T14:41:50+00:00",
        "pdf_url": "https://arxiv.org/pdf/2504.03486v1",
        "similarity_score": 0.46650534868240356
    },
    {
        "arxiv_id": "2407.05786v1",
        "title": "Large Language Models for Judicial Entity Extraction: A Comparative Study",
        "abstract": "Domain-specific Entity Recognition holds significant importance in legal contexts, serving as a fundamental task that supports various applications such as question-answering systems, text summarization, machine translation, sentiment analysis, and information retrieval specifically within case law documents. Recent advancements have highlighted the efficacy of Large Language Models in natural language processing tasks, demonstrating their capability to accurately detect and classify domain-specific facts (entities) from specialized texts like clinical and financial documents. This research investigates the application of Large Language Models in identifying domain-specific entities (e.g., courts, petitioner, judge, lawyer, respondents, FIR nos.) within case law documents, with a specific focus on their aptitude for handling domain-specific language complexity and contextual variations. The study evaluates the performance of state-of-the-art Large Language Model architectures, including Large Language Model Meta AI 3, Mistral, and Gemma, in the context of extracting judicial facts tailored to Indian judicial texts. Mistral and Gemma emerged as the top-performing models, showcasing balanced precision and recall crucial for accurate entity identification. These findings confirm the value of Large Language Models in judicial documents and demonstrate how they can facilitate and quicken scientific research by producing precise, organised data outputs that are appropriate for in-depth examination.",
        "authors": [
            "Atin Sakkeer Hussain",
            "Anu Thomas"
        ],
        "submitted_date": "2024-07-08T09:49:03+00:00",
        "pdf_url": "https://arxiv.org/pdf/2407.05786v1",
        "similarity_score": 0.4659309685230255
    },
    {
        "arxiv_id": "2510.21717v1",
        "title": "AI-Enhanced Operator Assistance for UNICOS Applications",
        "abstract": "This project explores the development of an AI-enhanced operator assistant for UNICOS, CERN's UNified Industrial Control System. While powerful, UNICOS presents a number of challenges, including the cognitive burden of decoding widgets, manual effort required for root cause analysis, and difficulties maintainers face in tracing datapoint elements (DPEs) across a complex codebase. In situations where timely responses are critical, these challenges can increase cognitive load and slow down diagnostics. To address these issues, a multi-agent system was designed and implemented. The solution is supported by a modular architecture comprising a UNICOS-side extension written in CTRL code, a Python-based multi-agent system deployed on a virtual machine, and a vector database storing both operator documentation and widget animation code. Preliminary evaluations suggest that the system is capable of decoding widgets, performing root cause analysis by leveraging live device data and documentation, and tracing DPEs across a complex codebase. Together, these capabilities reduce the manual workload of operators and maintainers, enhance situational awareness in operations, and accelerate responses to alarms and anomalies. Beyond these immediate gains, this work highlights the potential of introducing multi-modal reasoning and retrieval augmented generation (RAG) into the domain of industrial control. Ultimately, this work represents more than a proof of concept: it provides a basis for advancing intelligent operator interfaces at CERN. By combining modular design, extensibility, and practical AI integration, this project not only alleviates current operator pain points but also points toward broader opportunities for assistive AI in accelerator operations.",
        "authors": [
            "Bernard Tam",
            "Jean-Charles Tournier",
            "Fernando Varela Rodriguez"
        ],
        "submitted_date": "2025-09-16T03:43:54+00:00",
        "pdf_url": "https://arxiv.org/pdf/2510.21717v1",
        "similarity_score": 0.46527671813964844
    },
    {
        "arxiv_id": "2412.07794v1",
        "title": "Automatic answering of scientific questions using the FACTS-V1 framework: New methods in research to increase efficiency through the use of AI",
        "abstract": "The use of artificial intelligence (AI) offers various possibilities to expand and support educational research. Specifically, the implementation of AI can be used to develop new frameworks to establish new research tools that accelerate and meaningfully expand the efficiency of data evaluation and interpretation (Buckingham Shum et al., 2023). This article presents the prototype of the FACTS-V1 (Filtering and Analysis of Content in Textual Sources) framework. With the help of the application, numerous scientific papers can be automatically extracted, analyzed and interpreted from open access document servers without having to rely on proprietary applications and their limitations. The FACTS-V1 prototype consists of three building blocks. The first part deals with the extraction of texts, the second with filtering and interpretation, and the last with the actual statistical evaluation (topic modeling) using an interactive overview. The aim of the framework is to provide recommendations for future scientific questions based on existing data. The functionality is illustrated by asking how the use of AI will change the education sector. The data used to answer the question comes from 82 scientific papers on the topic of AI from 2024. The papers are publicly available on the peDOCS document server of the Leibniz Institute for Educational Research and Educational Information.",
        "authors": [
            "Stefan Pietrusky"
        ],
        "submitted_date": "2024-12-01T18:55:39+00:00",
        "pdf_url": "https://arxiv.org/pdf/2412.07794v1",
        "similarity_score": 0.46401065587997437
    },
    {
        "arxiv_id": "2412.04626v2",
        "title": "BigDocs: An Open Dataset for Training Multimodal Models on Document and Code Tasks",
        "abstract": "Multimodal AI has the potential to significantly enhance document-understanding tasks, such as processing receipts, understanding workflows, extracting data from documents, and summarizing reports. Code generation tasks that require long-structured outputs can also be enhanced by multimodality. Despite this, their use in commercial applications is often limited due to limited access to training data and restrictive licensing, which hinders open access. To address these limitations, we introduce BigDocs-7.5M, a high-quality, open-access dataset comprising 7.5 million multimodal documents across 30 tasks. We use an efficient data curation process to ensure our data is high-quality and license-permissive. Our process emphasizes accountability, responsibility, and transparency through filtering rules, traceable metadata, and careful content analysis. Additionally, we introduce BigDocs-Bench, a benchmark suite with 10 novel tasks where we create datasets that reflect real-world use cases involving reasoning over Graphical User Interfaces (GUI) and code generation from images. Our experiments show that training with BigDocs-Bench improves average performance up to 25.8% over closed-source GPT-4o in document reasoning and structured output tasks such as Screenshot2HTML or Image2Latex generation. Finally, human evaluations showed a preference for outputs from models trained on BigDocs over GPT-4o. This suggests that BigDocs can help both academics and the open-source community utilize and improve AI tools to enhance multimodal capabilities and document reasoning. The project is hosted at https://bigdocs.github.io .",
        "authors": [
            "Juan Rodriguez",
            "Xiangru Jian",
            "Siba Smarak Panigrahi",
            "Tianyu Zhang",
            "Aarash Feizi",
            "Abhay Puri",
            "Akshay Kalkunte",
            "Fran√ßois Savard",
            "Ahmed Masry",
            "Shravan Nayak",
            "Rabiul Awal",
            "Mahsa Massoud",
            "Amirhossein Abaskohi",
            "Zichao Li",
            "Suyuchen Wang",
            "Pierre-Andr√© No√´l",
            "Mats Leon Richter",
            "Saverio Vadacchino",
            "Shubham Agarwal",
            "Sanket Biswas",
            "Sara Shanian",
            "Ying Zhang",
            "Noah Bolger",
            "Kurt MacDonald",
            "Simon Fauvel",
            "Sathwik Tejaswi",
            "Srinivas Sunkara",
            "Joao Monteiro",
            "Krishnamurthy DJ Dvijotham",
            "Torsten Scholak",
            "Nicolas Chapados",
            "Sepideh Kharagani",
            "Sean Hughes",
            "M. √ñzsu",
            "Siva Reddy",
            "Marco Pedersoli",
            "Yoshua Bengio",
            "Christopher Pal",
            "Issam Laradji",
            "Spandana Gella",
            "Perouz Taslakian",
            "David Vazquez",
            "Sai Rajeswar"
        ],
        "submitted_date": "2024-12-05T21:41:20+00:00",
        "pdf_url": "https://arxiv.org/pdf/2412.04626v2",
        "similarity_score": 0.4605723023414612
    },
    {
        "arxiv_id": "2401.15050v1",
        "title": "LongFin: A Multimodal Document Understanding Model for Long Financial Domain Documents",
        "abstract": "Document AI is a growing research field that focuses on the comprehension and extraction of information from scanned and digital documents to make everyday business operations more efficient. Numerous downstream tasks and datasets have been introduced to facilitate the training of AI models capable of parsing and extracting information from various document types such as receipts and scanned forms. Despite these advancements, both existing datasets and models fail to address critical challenges that arise in industrial contexts. Existing datasets primarily comprise short documents consisting of a single page, while existing models are constrained by a limited maximum length, often set at 512 tokens. Consequently, the practical application of these methods in financial services, where documents can span multiple pages, is severely impeded. To overcome these challenges, we introduce LongFin, a multimodal document AI model capable of encoding up to 4K tokens. We also propose the LongForms dataset, a comprehensive financial dataset that encapsulates several industrial challenges in financial documents. Through an extensive evaluation, we demonstrate the effectiveness of the LongFin model on the LongForms dataset, surpassing the performance of existing public models while maintaining comparable results on existing single-page benchmarks.",
        "authors": [
            "Ahmed Masry",
            "Amir Hajian"
        ],
        "submitted_date": "2024-01-26T18:23:45+00:00",
        "pdf_url": "https://arxiv.org/pdf/2401.15050v1",
        "similarity_score": 0.4600226879119873
    },
    {
        "arxiv_id": "2509.25494v1",
        "title": "On-Premise AI for the Newsroom: Evaluating Small Language Models for Investigative Document Search",
        "abstract": "Investigative journalists routinely confront large document collections. Large language models (LLMs) with retrieval-augmented generation (RAG) capabilities promise to accelerate the process of document discovery, but newsroom adoption remains limited due to hallucination risks, verification burden, and data privacy concerns. We present a journalist-centered approach to LLM-powered document search that prioritizes transparency and editorial control through a five-stage pipeline -- corpus summarization, search planning, parallel thread execution, quality evaluation, and synthesis -- using small, locally-deployable language models that preserve data security and maintain complete auditability through explicit citation chains. Evaluating three quantized models (Gemma 3 12B, Qwen 3 14B, and GPT-OSS 20B) on two corpora, we find substantial variation in reliability. All models achieved high citation validity and ran effectively on standard desktop hardware (e.g., 24 GB of memory), demonstrating feasibility for resource-constrained newsrooms. However, systematic challenges emerged, including error propagation through multi-stage synthesis and dramatic performance variation based on training data overlap with corpus content. These findings suggest that effective newsroom AI deployment requires careful model selection and system design, alongside human oversight for maintaining standards of accuracy and accountability.",
        "authors": [
            "Nick Hagar",
            "Nicholas Diakopoulos",
            "Jeremy Gilbert"
        ],
        "submitted_date": "2025-09-29T20:50:40+00:00",
        "pdf_url": "https://arxiv.org/pdf/2509.25494v1",
        "similarity_score": 0.4586344063282013
    },
    {
        "arxiv_id": "2409.08181v1",
        "title": "Enhancing Canine Musculoskeletal Diagnoses: Leveraging Synthetic Image Data for Pre-Training AI-Models on Visual Documentations",
        "abstract": "The examination of the musculoskeletal system in dogs is a challenging task in veterinary practice. In this work, a novel method has been developed that enables efficient documentation of a dog's condition through a visual representation. However, since the visual documentation is new, there is no existing training data. The objective of this work is therefore to mitigate the impact of data scarcity in order to develop an AI-based diagnostic support system. To this end, the potential of synthetic data that mimics realistic visual documentations of diseases for pre-training AI models is investigated. We propose a method for generating synthetic image data that mimics realistic visual documentations. Initially, a basic dataset containing three distinct classes is generated, followed by the creation of a more sophisticated dataset containing 36 different classes. Both datasets are used for the pre-training of an AI model. Subsequently, an evaluation dataset is created, consisting of 250 manually created visual documentations for five different diseases. This dataset, along with a subset containing 25 examples. The obtained results on the evaluation dataset containing 25 examples demonstrate a significant enhancement of approximately 10% in diagnosis accuracy when utilizing generated synthetic images that mimic real-world visual documentations. However, these results do not hold true for the larger evaluation dataset containing 250 examples, indicating that the advantages of using synthetic data for pre-training an AI model emerge primarily when dealing with few examples of visual documentations for a given disease. Overall, this work provides valuable insights into mitigating the limitations imposed by limited training data through the strategic use of generated synthetic data, presenting an approach applicable beyond the canine musculoskeletal assessment domain.",
        "authors": [
            "Martin Thi√üen",
            "Thi Ngoc Diep Tran",
            "Ben Joel Sch√∂nbein",
            "Ute Trapp",
            "Barbara Esteve Ratsch",
            "Beate Egner",
            "Romana Piat",
            "Elke Hergenr√∂ther"
        ],
        "submitted_date": "2024-09-12T16:13:07+00:00",
        "pdf_url": "https://arxiv.org/pdf/2409.08181v1",
        "similarity_score": 0.45726463198661804
    },
    {
        "arxiv_id": "2411.13518v1",
        "title": "Advancing Complex Medical Communication in Arabic with Sporo AraSum: Surpassing Existing Large Language Models",
        "abstract": "The increasing demand for multilingual capabilities in healthcare underscores the need for AI models adept at processing diverse languages, particularly in clinical documentation and decision-making. Arabic, with its complex morphology, syntax, and diglossia, poses unique challenges for natural language processing (NLP) in medical contexts. This case study evaluates Sporo AraSum, a language model tailored for Arabic clinical documentation, against JAIS, the leading Arabic NLP model. Using synthetic datasets and modified PDQI-9 metrics modified ourselves for the purposes of assessing model performances in a different language. The study assessed the models' performance in summarizing patient-physician interactions, focusing on accuracy, comprehensiveness, clinical utility, and linguistic-cultural competence.   Results indicate that Sporo AraSum significantly outperforms JAIS in AI-centric quantitative metrics and all qualitative attributes measured in our modified version of the PDQI-9. AraSum's architecture enables precise and culturally sensitive documentation, addressing the linguistic nuances of Arabic while mitigating risks of AI hallucinations. These findings suggest that Sporo AraSum is better suited to meet the demands of Arabic-speaking healthcare environments, offering a transformative solution for multilingual clinical workflows. Future research should incorporate real-world data to further validate these findings and explore broader integration into healthcare systems.",
        "authors": [
            "Chanseo Lee",
            "Sonu Kumar",
            "Kimon A. Vogt",
            "Sam Meraj",
            "Antonia Vogt"
        ],
        "submitted_date": "2024-11-20T18:10:19+00:00",
        "pdf_url": "https://arxiv.org/pdf/2411.13518v1",
        "similarity_score": 0.4570351243019104
    },
    {
        "arxiv_id": "2502.18209v2",
        "title": "League: Leaderboard Generation on Demand",
        "abstract": "This paper introduces Leaderboard Auto Generation (LAG), a novel and well-organized framework for automatic generation of leaderboards on a given research topic in rapidly evolving fields like Artificial Intelligence (AI). Faced with a large number of AI papers updated daily, it becomes difficult for researchers to track every paper's proposed methods, experimental results, and settings, prompting the need for efficient automatic leaderboard construction. While large language models (LLMs) offer promise in automating this process, challenges such as multi-document summarization, leaderboard generation, and experiment fair comparison still remain under exploration. LAG solves these challenges through a systematic approach that involves the paper collection, experiment results extraction and integration, leaderboard generation, and quality evaluation. Our contributions include a comprehensive solution to the leaderboard construction problem, a reliable evaluation method, and experimental results showing the high quality of leaderboards.",
        "authors": [
            "Jian Wu",
            "Jiayu Zhang",
            "Dongyuan Li",
            "Linyi Yang",
            "Aoxiao Zhong",
            "Renhe Jiang",
            "Qingsong Wen",
            "Yue Zhang"
        ],
        "submitted_date": "2025-02-25T13:54:03+00:00",
        "pdf_url": "https://arxiv.org/pdf/2502.18209v2",
        "similarity_score": 0.45682692527770996
    },
    {
        "arxiv_id": "2407.03651v2",
        "title": "Evaluating Language Model Context Windows: A \"Working Memory\" Test and Inference-time Correction",
        "abstract": "Large language models are prominently used in real-world applications, often tasked with reasoning over large volumes of documents. An exciting development in this space is models boasting extended context capabilities, with some accommodating over 2 million tokens. Such long context model capabilities remain uncertain in production systems, motivating the need to benchmark their performance on real world use cases. We address this challenge by proposing SWiM, an evaluation framework that addresses the limitations of standard tests. Testing the framework on eight long context models, we find that even strong models such as GPT-4 and Claude 3 Opus degrade in performance when information is present in the middle of the context window (lost-in-the-middle effect). Next, in addition to our benchmark, we propose medoid voting, a simple, but effective training-free approach that helps alleviate this effect, by generating responses a few times, each time randomly permuting documents in the context, and selecting the medoid answer. We evaluate medoid voting on single document QA tasks, achieving up to a 24% lift in accuracy. Our code is available at https://github.com/snorkel-ai/long-context-eval.",
        "authors": [
            "Amanda Dsouza",
            "Christopher Glaze",
            "Changho Shin",
            "Frederic Sala"
        ],
        "submitted_date": "2024-07-04T05:46:20+00:00",
        "pdf_url": "https://arxiv.org/pdf/2407.03651v2",
        "similarity_score": 0.45633548498153687
    },
    {
        "arxiv_id": "2505.19662v2",
        "title": "FieldWorkArena: Agentic AI Benchmark for Real Field Work Tasks",
        "abstract": "This paper proposes FieldWorkArena, a benchmark for agentic AI targeting real-world field work. With the recent increase in demand for agentic AI, they are required to monitor and report safety and health incidents, as well as manufacturing-related incidents, that may occur in real-world work environments. Existing agentic AI benchmarks have been limited to evaluating web tasks and are insufficient for evaluating agents in real-world work environments, where complexity increases significantly. In this paper, we define a new action space that agentic AI should possess for real world work environment benchmarks and improve the evaluation function from previous methods to assess the performance of agentic AI in diverse real-world tasks. The dataset consists of videos captured on-site and documents actually used in factories and warehouses, and tasks were created based on interviews with on-site workers and managers. Evaluation results confirmed that performance evaluation considering the characteristics of Multimodal LLM (MLLM) such as GPT-4o is feasible. Additionally, the effectiveness and limitations of the proposed new evaluation method were identified. The complete dataset (HuggingFace) and evaluation program (GitHub) can be downloaded from the following website: https://en-documents.research.global.fujitsu.com/fieldworkarena/.",
        "authors": [
            "Atsunori Moteki",
            "Shoichi Masui",
            "Fan Yang",
            "Yueqi Song",
            "Yonatan Bisk",
            "Graham Neubig",
            "Ikuo Kusajima",
            "Yasuto Watanabe",
            "Hiroyuki Ishida",
            "Jun Takahashi",
            "Shan Jiang"
        ],
        "submitted_date": "2025-05-26T08:21:46+00:00",
        "pdf_url": "https://arxiv.org/pdf/2505.19662v2",
        "similarity_score": 0.45572221279144287
    },
    {
        "arxiv_id": "2504.13879v1",
        "title": "Ambient Listening in Clinical Practice: Evaluating EPIC Signal Data Before and After Implementation and Its Impact on Physician Workload",
        "abstract": "The widespread adoption of EHRs following the HITECH Act has increased the clinician documentation burden, contributing to burnout. Emerging technologies, such as ambient listening tools powered by generative AI, offer real-time, scribe-like documentation capabilities to reduce physician workload. This study evaluates the impact of ambient listening tools implemented at UCI Health by analyzing EPIC Signal data to assess changes in note length and time spent on notes. Results show significant reductions in note-taking time and an increase in note length, particularly during the first-month post-implementation. Findings highlight the potential of AI-powered documentation tools to improve clinical efficiency. Future research should explore adoption barriers, long-term trends, and user experiences to enhance the scalability and sustainability of ambient listening technology in clinical practice.",
        "authors": [
            "Yawen Guo",
            "Di Hu",
            "Jiayuan Wang",
            "Kai Zheng",
            "Danielle Perret",
            "Deepti Pandita",
            "Steven Tam"
        ],
        "submitted_date": "2025-04-02T00:31:14+00:00",
        "pdf_url": "https://arxiv.org/pdf/2504.13879v1",
        "similarity_score": 0.45448705554008484
    },
    {
        "arxiv_id": "2509.07666v1",
        "title": "MoLoRAG: Bootstrapping Document Understanding via Multi-modal Logic-aware Retrieval",
        "abstract": "Document Understanding is a foundational AI capability with broad applications, and Document Question Answering (DocQA) is a key evaluation task. Traditional methods convert the document into text for processing by Large Language Models (LLMs), but this process strips away critical multi-modal information like figures. While Large Vision-Language Models (LVLMs) address this limitation, their constrained input size makes multi-page document comprehension infeasible. Retrieval-augmented generation (RAG) methods mitigate this by selecting relevant pages, but they rely solely on semantic relevance, ignoring logical connections between pages and the query, which is essential for reasoning.   To this end, we propose MoLoRAG, a logic-aware retrieval framework for multi-modal, multi-page document understanding. By constructing a page graph that captures contextual relationships between pages, a lightweight VLM performs graph traversal to retrieve relevant pages, including those with logical connections often overlooked. This approach combines semantic and logical relevance to deliver more accurate retrieval. After retrieval, the top-$K$ pages are fed into arbitrary LVLMs for question answering. To enhance flexibility, MoLoRAG offers two variants: a training-free solution for easy deployment and a fine-tuned version to improve logical relevance checking. Experiments on four DocQA datasets demonstrate average improvements of 9.68% in accuracy over LVLM direct inference and 7.44% in retrieval precision over baselines. Codes and datasets are released at https://github.com/WxxShirley/MoLoRAG.",
        "authors": [
            "Xixi Wu",
            "Yanchao Tan",
            "Nan Hou",
            "Ruiyang Zhang",
            "Hong Cheng"
        ],
        "submitted_date": "2025-09-06T00:59:28+00:00",
        "pdf_url": "https://arxiv.org/pdf/2509.07666v1",
        "similarity_score": 0.45272552967071533
    },
    {
        "arxiv_id": "2510.20061v1",
        "title": "Ask What Your Country Can Do For You: Towards a Public Red Teaming Model",
        "abstract": "AI systems have the potential to produce both benefits and harms, but without rigorous and ongoing adversarial evaluation, AI actors will struggle to assess the breadth and magnitude of the AI risk surface. Researchers from the field of systems design have developed several effective sociotechnical AI evaluation and red teaming techniques targeting bias, hate speech, mis/disinformation, and other documented harm classes. However, as increasingly sophisticated AI systems are released into high-stakes sectors (such as education, healthcare, and intelligence-gathering), our current evaluation and monitoring methods are proving less and less capable of delivering effective oversight.   In order to actually deliver responsible AI and to ensure AI's harms are fully understood and its security vulnerabilities mitigated, pioneering new approaches to close this \"responsibility gap\" are now more urgent than ever. In this paper, we propose one such approach, the cooperative public AI red-teaming exercise, and discuss early results of its prior pilot implementations. This approach is intertwined with CAMLIS itself: the first in-person public demonstrator exercise was held in conjunction with CAMLIS 2024. We review the operational design and results of this exercise, the prior National Institute of Standards and Technology (NIST)'s Assessing the Risks and Impacts of AI (ARIA) pilot exercise, and another similar exercise conducted with the Singapore Infocomm Media Development Authority (IMDA). Ultimately, we argue that this approach is both capable of delivering meaningful results and is also scalable to many AI developing jurisdictions.",
        "authors": [
            "Wm. Matthew Kennedy",
            "Cigdem Patlak",
            "Jayraj Dave",
            "Blake Chambers",
            "Aayush Dhanotiya",
            "Darshini Ramiah",
            "Reva Schwartz",
            "Jack Hagen",
            "Akash Kundu",
            "Mouni Pendharkar",
            "Liam Baisley",
            "Theodora Skeadas",
            "Rumman Chowdhury"
        ],
        "submitted_date": "2025-10-22T22:24:21+00:00",
        "pdf_url": "https://arxiv.org/pdf/2510.20061v1",
        "similarity_score": 0.45153382420539856
    },
    {
        "arxiv_id": "2403.15404v1",
        "title": "AI Sustainability in Practice Part Two: Sustainability Throughout the AI Workflow",
        "abstract": "The sustainability of AI systems depends on the capacity of project teams to proceed with a continuous sensitivity to their potential real-world impacts and transformative effects. Stakeholder Impact Assessments (SIAs) are governance mechanisms that enable this kind of responsiveness. They are tools that create a procedure for, and a means of documenting, the collaborative evaluation and reflective anticipation of the possible harms and benefits of AI innovation projects. SIAs are not one-off governance actions. They require project teams to pay continuous attention to the dynamic and changing character of AI production and use and to the shifting conditions of the real-world environments in which AI technologies are embedded. This workbook is part two of two workbooks on AI Sustainability. It provides a template of the SIA and activities that allow a deeper dive into crucial parts of it. It discusses methods for weighing values and considering trade-offs during the SIA. And, it highlights the need to treat the SIA as an end-to-end process of responsive evaluation and re-assessment.",
        "authors": [
            "David Leslie",
            "Cami Rincon",
            "Morgan Briggs",
            "Antonella Perini",
            "Smera Jayadeva",
            "Ann Borda",
            "SJ Bennett",
            "Christopher Burr",
            "Mhairi Aitken",
            "Michael Katell",
            "Claudia Fischer",
            "Janis Wong",
            "Ismael Kherroubi Garcia"
        ],
        "submitted_date": "2024-02-19T22:58:05+00:00",
        "pdf_url": "https://arxiv.org/pdf/2403.15404v1",
        "similarity_score": 0.45108762383461
    },
    {
        "arxiv_id": "2406.11845v2",
        "title": "Decoding the Digital Fine Print: Navigating the potholes in Terms of service/ use of GenAI tools against the emerging need for Transparent and Trustworthy Tech Futures",
        "abstract": "The research investigates the crucial role of clear and intelligible terms of service in cultivating user trust and facilitating informed decision-making in the context of AI, in specific GenAI. It highlights the obstacles presented by complex legal terminology and detailed fine print, which impede genuine user consent and recourse, particularly during instances of algorithmic malfunctions, hazards, damages, or inequities, while stressing the necessity of employing machine-readable terms for effective service licensing. The increasing reliance on General Artificial Intelligence (GenAI) tools necessitates transparent, comprehensible, and standardized terms of use, which facilitate informed decision-making while fostering trust among stakeholders. Despite recent efforts promoting transparency via system and model cards, existing documentation frequently falls short of providing adequate disclosures, leaving users ill-equipped to evaluate potential risks and harms. To address this gap, this research examines key considerations necessary in terms of use or terms of service for Generative AI tools, drawing insights from multiple studies. Subsequently, this research evaluates whether the terms of use or terms of service of prominent Generative AI tools against the identified considerations. Findings indicate inconsistencies and variability in document quality, signaling a pressing demand for uniformity in disclosure practices. Consequently, this study advocates for robust, enforceable standards ensuring complete and intelligible disclosures prior to the release of GenAI tools, thereby empowering end-users to make well-informed choices and enhancing overall accountability in the field.",
        "authors": [
            "Sundaraparipurnan Narayanan"
        ],
        "submitted_date": "2024-03-26T04:54:53+00:00",
        "pdf_url": "https://arxiv.org/pdf/2406.11845v2",
        "similarity_score": 0.45018115639686584
    },
    {
        "arxiv_id": "2503.21839v1",
        "title": "M-DocSum: Do LVLMs Genuinely Comprehend Interleaved Image-Text in Document Summarization?",
        "abstract": "We investigate a critical yet under-explored question in Large Vision-Language Models (LVLMs): Do LVLMs genuinely comprehend interleaved image-text in the document? Existing document understanding benchmarks often assess LVLMs using question-answer formats, which are information-sparse and difficult to guarantee the coverage of long-range dependencies. To address this issue, we introduce a novel and challenging Multimodal Document Summarization Benchmark (M-DocSum-Bench), which comprises 500 high-quality arXiv papers, along with interleaved multimodal summaries aligned with human preferences. M-DocSum-Bench is a reference-based generation task and necessitates the generation of interleaved image-text summaries using provided reference images, thereby simultaneously evaluating capabilities in understanding, reasoning, localization, and summarization within complex multimodal document scenarios. To facilitate this benchmark, we develop an automated framework to construct summaries and propose a fine-grained evaluation method called M-DocEval. Moreover, we further develop a robust summarization baseline, i.e., M-DocSum-7B, by progressive two-stage training with diverse instruction and preference data. The extensive results on our M-DocSum-Bench reveal that the leading LVLMs struggle to maintain coherence and accurately integrate information within long and interleaved contexts, often exhibiting confusion between similar images and a lack of robustness. Notably, M-DocSum-7B achieves state-of-the-art performance compared to larger and closed-source models (including GPT-4o, Gemini Pro, Claude-3.5-Sonnet and Qwen2.5-VL-72B, etc.), demonstrating the potential of LVLMs for improved interleaved image-text understanding. The code, data, and models are available at https://github.com/stepfun-ai/M-DocSum-Bench.",
        "authors": [
            "Haolong Yan",
            "Kaijun Tan",
            "Yeqing Shen",
            "Xin Huang",
            "Zheng Ge",
            "Xiangyu Zhang",
            "Si Li",
            "Daxin Jiang"
        ],
        "submitted_date": "2025-03-27T07:28:32+00:00",
        "pdf_url": "https://arxiv.org/pdf/2503.21839v1",
        "similarity_score": 0.4487510621547699
    },
    {
        "arxiv_id": "2405.20362v1",
        "title": "Hallucination-Free? Assessing the Reliability of Leading AI Legal Research Tools",
        "abstract": "Legal practice has witnessed a sharp rise in products incorporating artificial intelligence (AI). Such tools are designed to assist with a wide range of core legal tasks, from search and summarization of caselaw to document drafting. But the large language models used in these tools are prone to \"hallucinate,\" or make up false information, making their use risky in high-stakes domains. Recently, certain legal research providers have touted methods such as retrieval-augmented generation (RAG) as \"eliminating\" (Casetext, 2023) or \"avoid[ing]\" hallucinations (Thomson Reuters, 2023), or guaranteeing \"hallucination-free\" legal citations (LexisNexis, 2023). Because of the closed nature of these systems, systematically assessing these claims is challenging. In this article, we design and report on the first preregistered empirical evaluation of AI-driven legal research tools. We demonstrate that the providers' claims are overstated. While hallucinations are reduced relative to general-purpose chatbots (GPT-4), we find that the AI research tools made by LexisNexis (Lexis+ AI) and Thomson Reuters (Westlaw AI-Assisted Research and Ask Practical Law AI) each hallucinate between 17% and 33% of the time. We also document substantial differences between systems in responsiveness and accuracy. Our article makes four key contributions. It is the first to assess and report the performance of RAG-based proprietary legal AI tools. Second, it introduces a comprehensive, preregistered dataset for identifying and understanding vulnerabilities in these systems. Third, it proposes a clear typology for differentiating between hallucinations and accurate legal responses. Last, it provides evidence to inform the responsibilities of legal professionals in supervising and verifying AI outputs, which remains a central open question for the responsible integration of AI into law.",
        "authors": [
            "Varun Magesh",
            "Faiz Surani",
            "Matthew Dahl",
            "Mirac Suzgun",
            "Christopher D. Manning",
            "Daniel E. Ho"
        ],
        "submitted_date": "2024-05-30T17:56:05+00:00",
        "pdf_url": "https://arxiv.org/pdf/2405.20362v1",
        "similarity_score": 0.4482147693634033
    },
    {
        "arxiv_id": "2408.06930v2",
        "title": "Diagnosis extraction from unstructured Dutch echocardiogram reports using span- and document-level characteristic classification",
        "abstract": "Clinical machine learning research and AI driven clinical decision support models rely on clinically accurate labels. Manually extracting these labels with the help of clinical specialists is often time-consuming and expensive. This study tests the feasibility of automatic span- and document-level diagnosis extraction from unstructured Dutch echocardiogram reports. We included 115,692 unstructured echocardiogram reports from the UMCU a large university hospital in the Netherlands. A randomly selected subset was manually annotated for the occurrence and severity of eleven commonly described cardiac characteristics. We developed and tested several automatic labelling techniques at both span and document levels, using weighted and macro F1-score, precision, and recall for performance evaluation. We compared the performance of span labelling against document labelling methods, which included both direct document classifiers and indirect document classifiers that rely on span classification results. The SpanCategorizer and MedRoBERTa$.$nl models outperformed all other span and document classifiers, respectively. The weighted F1-score varied between characteristics, ranging from 0.60 to 0.93 in SpanCategorizer and 0.96 to 0.98 in MedRoBERTa$.$nl. Direct document classification was superior to indirect document classification using span classifiers. SetFit achieved competitive document classification performance using only 10% of the training data. Utilizing a reduced label set yielded near-perfect document classification results. We recommend using our published SpanCategorizer and MedRoBERTa$.$nl models for span- and document-level diagnosis extraction from Dutch echocardiography reports. For settings with limited training data, SetFit may be a promising alternative for document classification.",
        "authors": [
            "Bauke Arends",
            "Melle Vessies",
            "Dirk van Osch",
            "Arco Teske",
            "Pim van der Harst",
            "Ren√© van Es",
            "Bram van Es"
        ],
        "submitted_date": "2024-08-13T14:33:32+00:00",
        "pdf_url": "https://arxiv.org/pdf/2408.06930v2",
        "similarity_score": 0.44588202238082886
    },
    {
        "arxiv_id": "2508.04233v1",
        "title": "DocVCE: Diffusion-based Visual Counterfactual Explanations for Document Image Classification",
        "abstract": "As black-box AI-driven decision-making systems become increasingly widespread in modern document processing workflows, improving their transparency and reliability has become critical, especially in high-stakes applications where biases or spurious correlations in decision-making could lead to serious consequences. One vital component often found in such document processing workflows is document image classification, which, despite its widespread use, remains difficult to explain. While some recent works have attempted to explain the decisions of document image classification models through feature-importance maps, these maps are often difficult to interpret and fail to provide insights into the global features learned by the model. In this paper, we aim to bridge this research gap by introducing generative document counterfactuals that provide meaningful insights into the model's decision-making through actionable explanations. In particular, we propose DocVCE, a novel approach that leverages latent diffusion models in combination with classifier guidance to first generate plausible in-distribution visual counterfactual explanations, and then performs hierarchical patch-wise refinement to search for a refined counterfactual that is closest to the target factual image. We demonstrate the effectiveness of our approach through a rigorous qualitative and quantitative assessment on 3 different document classification datasets -- RVL-CDIP, Tobacco3482, and DocLayNet -- and 3 different models -- ResNet, ConvNeXt, and DiT -- using well-established evaluation criteria such as validity, closeness, and realism. To the best of the authors' knowledge, this is the first work to explore generative counterfactual explanations in document image analysis.",
        "authors": [
            "Saifullah Saifullah",
            "Stefan Agne",
            "Andreas Dengel",
            "Sheraz Ahmed"
        ],
        "submitted_date": "2025-08-06T09:15:32+00:00",
        "pdf_url": "https://arxiv.org/pdf/2508.04233v1",
        "similarity_score": 0.444696843624115
    },
    {
        "arxiv_id": "2510.10732v1",
        "title": "When Openness Fails: Lessons from System Safety for Assessing Openness in AI",
        "abstract": "Most frameworks for assessing the openness of AI systems use narrow criteria such as availability of data, model, code, documentation, and licensing terms. However, to evaluate whether the intended effects of openness - such as democratization and autonomy - are realized, we need a more holistic approach that considers the context of release: who will reuse the system, for what purposes, and under what conditions. To this end, we adapt five lessons from system safety that offer guidance on how openness can be evaluated at the system level.",
        "authors": [
            "Tamara Paris",
            "Shalaleh Rismani"
        ],
        "submitted_date": "2025-10-12T18:08:00+00:00",
        "pdf_url": "https://arxiv.org/pdf/2510.10732v1",
        "similarity_score": 0.43985384702682495
    },
    {
        "arxiv_id": "2507.15917v2",
        "title": "HyDRA: A Hybrid-Driven Reasoning Architecture for Verifiable Knowledge Graphs",
        "abstract": "The synergy between symbolic knowledge, often represented by Knowledge Graphs (KGs), and the generative capabilities of neural networks is central to advancing neurosymbolic AI. A primary bottleneck in realizing this potential is the difficulty of automating KG construction, which faces challenges related to output reliability, consistency, and verifiability. These issues can manifest as structural inconsistencies within the generated graphs, such as the formation of disconnected $\\textit{isolated islands}$ of data or the inaccurate conflation of abstract classes with specific instances. To address these challenges, we propose HyDRA, a $\\textbf{Hy}$brid-$\\textbf{D}$riven $\\textbf{R}$easoning $\\textbf{A}$rchitecture designed for verifiable KG automation. Given a domain or an initial set of documents, HyDRA first constructs an ontology via a panel of collaborative neurosymbolic agents. These agents collaboratively agree on a set of competency questions (CQs) that define the scope and requirements the ontology must be able to answer. Given these CQs, we build an ontology graph that subsequently guides the automated extraction of triplets for KG generation from arbitrary documents. Inspired by design-by-contracts (DbC) principles, our method leverages verifiable contracts as the primary control mechanism to steer the generative process of Large Language Models (LLMs). To verify the output of our approach, we extend beyond standard benchmarks and propose an evaluation framework that assesses the functional correctness of the resulting KG by leveraging symbolic verifications as described by the neurosymbolic AI framework, $\\textit{SymbolicAI}$. This work contributes a hybrid-driven architecture for improving the reliability of automated KG construction and the exploration of evaluation methods for measuring the functional integrity of its output. The code is publicly available.",
        "authors": [
            "Adrian Kaiser",
            "Claudiu Leoveanu-Condrei",
            "Ryan Gold",
            "Marius-Constantin Dinu",
            "Markus Hofmarcher"
        ],
        "submitted_date": "2025-07-21T17:57:17+00:00",
        "pdf_url": "https://arxiv.org/pdf/2507.15917v2",
        "similarity_score": 0.43837201595306396
    },
    {
        "arxiv_id": "2306.08122v1",
        "title": "Beyond Black Box AI-Generated Plagiarism Detection: From Sentence to Document Level",
        "abstract": "The increasing reliance on large language models (LLMs) in academic writing has led to a rise in plagiarism. Existing AI-generated text classifiers have limited accuracy and often produce false positives. We propose a novel approach using natural language processing (NLP) techniques, offering quantifiable metrics at both sentence and document levels for easier interpretation by human evaluators. Our method employs a multi-faceted approach, generating multiple paraphrased versions of a given question and inputting them into the LLM to generate answers. By using a contrastive loss function based on cosine similarity, we match generated sentences with those from the student's response. Our approach achieves up to 94% accuracy in classifying human and AI text, providing a robust and adaptable solution for plagiarism detection in academic settings. This method improves with LLM advancements, reducing the need for new model training or reconfiguration, and offers a more transparent way of evaluating and detecting AI-generated text.",
        "authors": [
            "Mujahid Ali Quidwai",
            "Chunhui Li",
            "Parijat Dube"
        ],
        "submitted_date": "2023-06-13T20:34:55+00:00",
        "pdf_url": "https://arxiv.org/pdf/2306.08122v1",
        "similarity_score": 0.43829166889190674
    },
    {
        "arxiv_id": "2007.05902v1",
        "title": "Editable AI: Mixed Human-AI Authoring of Code Patterns",
        "abstract": "Developers authoring HTML documents define elements following patterns which establish and reflect the visual structure of a document, such as making all images in a footer the same height by applying a class to each. To surface these patterns to developers and support developers in authoring consistent with these patterns, we propose a mixed human-AI technique for creating code patterns. Patterns are first learned from individual HTML documents through a decision tree, generating a representation which developers may view and edit. Code patterns are used to offer developers autocomplete suggestions, list examples, and flag violations. To evaluate our technique, we conducted a user study in which 24 participants wrote, edited, and corrected HTML documents. We found that our technique enabled developers to edit and correct documents more quickly and create, edit, and correct documents more successfully.",
        "authors": [
            "Kartik Chugh",
            "Andrea Y. Solis",
            "Thomas D. LaToza"
        ],
        "submitted_date": "2020-07-12T03:49:42+00:00",
        "pdf_url": "https://arxiv.org/pdf/2007.05902v1",
        "similarity_score": 0.4382131099700928
    },
    {
        "arxiv_id": "2512.02665v1",
        "title": "Input Order Shapes LLM Semantic Alignment in Multi-Document Summarization",
        "abstract": "Large language models (LLMs) are now used in settings such as Google's AI Overviews, where it summarizes multiple long documents. However, it remains unclear whether they weight all inputs equally. Focusing on abortion-related news, we construct 40 pro-neutral-con article triplets, permute each triplet into six input orders, and prompt Gemini 2.5 Flash to generate a neutral overview. We evaluate each summary against its source articles using ROUGE-L (lexical overlap), BERTScore (semantic similarity), and SummaC (factual consistency). One-way ANOVA reveals a significant primacy effect for BERTScore across all stances, indicating that summaries are more semantically aligned with the first-seen article. Pairwise comparisons further show that Position 1 differs significantly from Positions 2 and 3, while the latter two do not differ from each other, confirming a selective preference for the first document. The findings present risks for applications that rely on LLM-generated overviews and for agentic AI systems, where the steps involving LLMs can disproportionately influence downstream actions.",
        "authors": [
            "Jing Ma"
        ],
        "submitted_date": "2025-12-02T11:36:13+00:00",
        "pdf_url": "https://arxiv.org/pdf/2512.02665v1",
        "similarity_score": 0.4370489716529846
    },
    {
        "arxiv_id": "2504.09249v1",
        "title": "NoTeS-Bank: Benchmarking Neural Transcription and Search for Scientific Notes Understanding",
        "abstract": "Understanding and reasoning over academic handwritten notes remains a challenge in document AI, particularly for mathematical equations, diagrams, and scientific notations. Existing visual question answering (VQA) benchmarks focus on printed or structured handwritten text, limiting generalization to real-world note-taking. To address this, we introduce NoTeS-Bank, an evaluation benchmark for Neural Transcription and Search in note-based question answering. NoTeS-Bank comprises complex notes across multiple domains, requiring models to process unstructured and multimodal content. The benchmark defines two tasks: (1) Evidence-Based VQA, where models retrieve localized answers with bounding-box evidence, and (2) Open-Domain VQA, where models classify the domain before retrieving relevant documents and answers. Unlike classical Document VQA datasets relying on optical character recognition (OCR) and structured data, NoTeS-BANK demands vision-language fusion, retrieval, and multimodal reasoning. We benchmark state-of-the-art Vision-Language Models (VLMs) and retrieval frameworks, exposing structured transcription and reasoning limitations. NoTeS-Bank provides a rigorous evaluation with NDCG@5, MRR, Recall@K, IoU, and ANLS, establishing a new standard for visual document understanding and reasoning.",
        "authors": [
            "Aniket Pal",
            "Sanket Biswas",
            "Alloy Das",
            "Ayush Lodh",
            "Priyanka Banerjee",
            "Soumitri Chattopadhyay",
            "Dimosthenis Karatzas",
            "Josep Llados",
            "C. V. Jawahar"
        ],
        "submitted_date": "2025-04-12T15:11:32+00:00",
        "pdf_url": "https://arxiv.org/pdf/2504.09249v1",
        "similarity_score": 0.43628162145614624
    },
    {
        "arxiv_id": "2507.05280v1",
        "title": "Hungary and AI: efforts and opportunities in comparison with Singapore",
        "abstract": "The study assesses Hungary's National AI Strategy and its implementation through the analysis of strategic documents, publicly available financial records, and expert interviews with the Hungarian AI Coalition President and Chief Strategic Advisor to the Government Commissioner for AI. 22 goals from Hungary's strategy were evaluated through conceptual, governance, temporal, and financial dimensions before being benchmarked against Singapore's National AI Strategies (NAIS 1.0 and NAIS 2.0). Key findings include an estimated total of EUR 4.65 billion in AI-related public investment in Hungary. Openly available financial data was found for only half of the evaluated goals, and just three projects made up 98\\% of all documented funding. The research also reveals Hungary's implementation challenges, including fragmented execution following ministerial reorganizations and the absence of designated biennial reviews since 2020. Furthermore, the paper provides targeted recommendations for Hungary's forthcoming AI strategy, drawing on Singapore's framework as a reference point. These include adapting to the era of large language models, restructuring the existing triple helix network to foster more effective dialogue and advocacy, and positioning the country as an East-West bridge for automotive AI experimentation.",
        "authors": [
            "Andr√°s Ferenczy"
        ],
        "submitted_date": "2025-07-04T09:12:47+00:00",
        "pdf_url": "https://arxiv.org/pdf/2507.05280v1",
        "similarity_score": 0.43355292081832886
    },
    {
        "arxiv_id": "2309.03213v1",
        "title": "Improving the State of the Art for Training Human-AI Teams: Technical Report #3 -- Analysis of Testbed Alternatives",
        "abstract": "Sonalysts is working on an initiative to expand our current expertise in teaming to Human-Artificial Intelligence (AI) teams by developing original research in this area. To provide a foundation for that research, Sonalysts is investigating the development of a Synthetic Task Environment (STE). In a previous report, we documented the findings of a recent outreach effort in which we asked military Subject Matter Experts (SMEs) and other researchers in the Human-AI teaming domain to identify the qualities that they most valued in a testbed. A surprising finding from that outreach was that several respondents recommended that our team look into existing human-AI teaming testbeds, rather than creating something new. Based on that recommendation, we conducted a systematic investigation of the associated landscape. In this report, we describe the results of that investigation. Building on the survey results, we developed testbed evaluation criteria, identified potential testbeds, and conducted qualitative and quantitative evaluations of candidate testbeds. The evaluation process led to five candidate testbeds for the research team to consider. In the coming months, we will assess the viability of the various alternatives and begin to execute our program of research.",
        "authors": [
            "Lillian Asiala",
            "James E. McCarthy",
            "Lixiao Huang"
        ],
        "submitted_date": "2023-08-29T14:06:30+00:00",
        "pdf_url": "https://arxiv.org/pdf/2309.03213v1",
        "similarity_score": 0.42906078696250916
    },
    {
        "arxiv_id": "2511.18192v2",
        "title": "ARIAL: An Agentic Framework for Document VQA with Precise Answer Localization",
        "abstract": "Document Visual Question Answering (VQA) requires models to not only extract accurate textual answers but also precisely localize them within document images, a capability critical for interpretability in high-stakes applications. However, existing systems achieve strong textual accuracy while producing unreliable spatial grounding, or sacrifice performance for interpretability. We present ARIAL (Agentic Reasoning for Interpretable Answer Localization), a modular framework that orchestrates specialized tools through an LLM-based planning agent to achieve both precise answer extraction and reliable spatial grounding. ARIAL decomposes Document VQA into structured subtasks: OCR-based text extraction with TrOCR, retrieval-augmented context selection using semantic search, answer generation via a fine-tuned Gemma 3-27B model, and explicit bounding-box localization through text-to-region alignment. This modular architecture produces transparent reasoning traces, enabling tool-level auditability and independent component optimization. We evaluate ARIAL on four benchmarks (DocVQA, FUNSD, CORD, and SROIE) using both textual accuracy (ANLS) and spatial precision (mAP at IoU 0.50 to 0.95). ARIAL achieves state-of-the-art results across all datasets: 88.7 ANLS and 50.1 mAP on DocVQA, 90.0 ANLS and 50.3 mAP on FUNSD, 85.5 ANLS and 60.2 mAP on CORD, and 93.1 ANLS on SROIE, surpassing the previous best method (DLaVA) by +2.8 ANLS and +3.9 mAP on DocVQA. Our work demonstrates how agentic orchestration of specialized tools can simultaneously improve performance and interpretability, providing a pathway toward trustworthy, explainable document AI systems.",
        "authors": [
            "Ahmad Mohammadshirazi",
            "Pinaki Prasad Guha Neogi",
            "Dheeraj Kulshrestha",
            "Rajiv Ramnath"
        ],
        "submitted_date": "2025-11-22T21:09:28+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.18192v2",
        "similarity_score": 0.42765098810195923
    },
    {
        "arxiv_id": "2503.13771v1",
        "title": "Towards AI-assisted Academic Writing",
        "abstract": "We present components of an AI-assisted academic writing system including citation recommendation and introduction writing. The system recommends citations by considering the user's current document context to provide relevant suggestions. It generates introductions in a structured fashion, situating the contributions of the research relative to prior work. We demonstrate the effectiveness of the components through quantitative evaluations. Finally, the paper presents qualitative research exploring how researchers incorporate citations into their writing workflows. Our findings indicate that there is demand for precise AI-assisted writing systems and simple, effective methods for meeting those needs.",
        "authors": [
            "Daniel J. Liebling",
            "Malcolm Kane",
            "Madeleine Grunde-Mclaughlin",
            "Ian J. Lang",
            "Subhashini Venugopalan",
            "Michael P. Brenner"
        ],
        "submitted_date": "2025-03-17T23:30:17+00:00",
        "pdf_url": "https://arxiv.org/pdf/2503.13771v1",
        "similarity_score": 0.42627495527267456
    },
    {
        "arxiv_id": "2506.09467v1",
        "title": "ArcNeural: A Multi-Modal Database for the Gen-AI Era",
        "abstract": "ArcNeural introduces a novel multimodal database tailored for the demands of Generative AI and Large Language Models, enabling efficient management of diverse data types such as graphs, vectors, and documents. Its storage-compute separated architecture integrates graph technology, advanced vector indexing, and transaction processing to support real-time analytics and AI-driven applications. Key features include a unified storage layer, adaptive edge collection in MemEngine, and seamless integration of transaction and analytical processing. Experimental evaluations demonstrate ArcNeural's superior performance and scalability compared to state-of-the-art systems. This system bridges structured and unstructured data management, offering a versatile solution for enterprise-grade AI applications.   ArcNeural's design addresses the challenges of multimodal data processing, providing a robust framework for intelligent, data-driven solutions in the Gen AI era.",
        "authors": [
            "Wu Min",
            "Qiao Yuncong",
            "Yu Tan",
            "Chenghu Yang"
        ],
        "submitted_date": "2025-06-11T07:16:29+00:00",
        "pdf_url": "https://arxiv.org/pdf/2506.09467v1",
        "similarity_score": 0.4252529740333557
    },
    {
        "arxiv_id": "2306.03287v1",
        "title": "ICDAR 2023 Competition on Structured Text Extraction from Visually-Rich Document Images",
        "abstract": "Structured text extraction is one of the most valuable and challenging application directions in the field of Document AI. However, the scenarios of past benchmarks are limited, and the corresponding evaluation protocols usually focus on the submodules of the structured text extraction scheme. In order to eliminate these problems, we organized the ICDAR 2023 competition on Structured text extraction from Visually-Rich Document images (SVRD). We set up two tracks for SVRD including Track 1: HUST-CELL and Track 2: Baidu-FEST, where HUST-CELL aims to evaluate the end-to-end performance of Complex Entity Linking and Labeling, and Baidu-FEST focuses on evaluating the performance and generalization of Zero-shot / Few-shot Structured Text extraction from an end-to-end perspective. Compared to the current document benchmarks, our two tracks of competition benchmark enriches the scenarios greatly and contains more than 50 types of visually-rich document images (mainly from the actual enterprise applications). The competition opened on 30th December, 2022 and closed on 24th March, 2023. There are 35 participants and 91 valid submissions received for Track 1, and 15 participants and 26 valid submissions received for Track 2. In this report we will presents the motivation, competition datasets, task definition, evaluation protocol, and submission summaries. According to the performance of the submissions, we believe there is still a large gap on the expected information extraction performance for complex and zero-shot scenarios. It is hoped that this competition will attract many researchers in the field of CV and NLP, and bring some new thoughts to the field of Document AI.",
        "authors": [
            "Wenwen Yu",
            "Chengquan Zhang",
            "Haoyu Cao",
            "Wei Hua",
            "Bohan Li",
            "Huang Chen",
            "Mingyu Liu",
            "Mingrui Chen",
            "Jianfeng Kuang",
            "Mengjun Cheng",
            "Yuning Du",
            "Shikun Feng",
            "Xiaoguang Hu",
            "Pengyuan Lyu",
            "Kun Yao",
            "Yuechen Yu",
            "Yuliang Liu",
            "Wanxiang Che",
            "Errui Ding",
            "Cheng-Lin Liu",
            "Jiebo Luo",
            "Shuicheng Yan",
            "Min Zhang",
            "Dimosthenis Karatzas",
            "Xing Sun",
            "Jingdong Wang",
            "Xiang Bai"
        ],
        "submitted_date": "2023-06-05T22:20:52+00:00",
        "pdf_url": "https://arxiv.org/pdf/2306.03287v1",
        "similarity_score": 0.42289990186691284
    },
    {
        "arxiv_id": "2507.06260v1",
        "title": "Evaluating the Critical Risks of Amazon's Nova Premier under the Frontier Model Safety Framework",
        "abstract": "Nova Premier is Amazon's most capable multimodal foundation model and teacher for model distillation. It processes text, images, and video with a one-million-token context window, enabling analysis of large codebases, 400-page documents, and 90-minute videos in a single prompt. We present the first comprehensive evaluation of Nova Premier's critical risk profile under the Frontier Model Safety Framework. Evaluations target three high-risk domains -- Chemical, Biological, Radiological & Nuclear (CBRN), Offensive Cyber Operations, and Automated AI R&D -- and combine automated benchmarks, expert red-teaming, and uplift studies to determine whether the model exceeds release thresholds. We summarize our methodology and report core findings. Based on this evaluation, we find that Nova Premier is safe for public release as per our commitments made at the 2025 Paris AI Safety Summit. We will continue to enhance our safety evaluation and mitigation pipelines as new risks and capabilities associated with frontier models are identified.",
        "authors": [
            "Satyapriya Krishna",
            "Ninareh Mehrabi",
            "Abhinav Mohanty",
            "Matteo Memelli",
            "Vincent Ponzo",
            "Payal Motwani",
            "Rahul Gupta"
        ],
        "submitted_date": "2025-07-07T13:33:35+00:00",
        "pdf_url": "https://arxiv.org/pdf/2507.06260v1",
        "similarity_score": 0.4204419255256653
    },
    {
        "arxiv_id": "0504061v1",
        "title": "Summarization from Medical Documents: A Survey",
        "abstract": "Objective:   The aim of this paper is to survey the recent work in medical documents summarization.   Background:   During the last decade, documents summarization got increasing attention by the AI research community. More recently it also attracted the interest of the medical research community as well, due to the enormous growth of information that is available to the physicians and researchers in medicine, through the large and growing number of published journals, conference proceedings, medical sites and portals on the World Wide Web, electronic medical records, etc.   Methodology:   This survey gives first a general background on documents summarization, presenting the factors that summarization depends upon, discussing evaluation issues and describing briefly the various types of summarization techniques. It then examines the characteristics of the medical domain through the different types of medical documents. Finally, it presents and discusses the summarization techniques used so far in the medical domain, referring to the corresponding systems and their characteristics.   Discussion and conclusions:   The paper discusses thoroughly the promising paths for future research in medical documents summarization. It mainly focuses on the issue of scaling to large collections of documents in various languages and from different media, on personalization issues, on portability to new sub-domains, and on the integration of summarization technology in practical applications",
        "authors": [
            "Stergos D. Afantenos",
            "Vangelis Karkaletsis",
            "Panagiotis Stamatopoulos"
        ],
        "submitted_date": "2005-04-13T20:02:25+00:00",
        "pdf_url": "https://arxiv.org/pdf/cs/0504061v1",
        "similarity_score": 0.41584062576293945
    },
    {
        "arxiv_id": "2506.03197v3",
        "title": "Infinity Parser: Layout Aware Reinforcement Learning for Scanned Document Parsing",
        "abstract": "Automated parsing of scanned documents into richly structured, machine-readable formats remains a critical bottleneck in Document AI, as traditional multi-stage pipelines suffer from error propagation and limited adaptability to diverse layouts. We introduce layoutRL, an end-to-end reinforcement learning framework that trains models to be explicitly layout-aware by optimizing a composite reward of normalized edit distance, paragraph count accuracy, and reading order preservation. Leveraging our newly released dataset, Infinity-Doc-55K, which combines 55K high-fidelity synthetic scanned document parsing data with expert-filtered real-world documents, we instantiate layoutRL in a vision-language-model-based parser called Infinity-Parser. Evaluated on English and Chinese benchmarks for OCR, table and formula extraction, and reading order detection, Infinity-Parser achieves new state-of-the-art performance in both accuracy and structural fidelity, outpacing specialist pipelines and general-purpose vision-language models. We will publicly release our code and dataset to accelerate progress in robust document understanding.",
        "authors": [
            "Baode Wang",
            "Biao Wu",
            "Weizhen Li",
            "Meng Fang",
            "Zuming Huang",
            "Jun Huang",
            "Haozhe Wang",
            "Yanjie Liang",
            "Ling Chen",
            "Wei Chu",
            "Yuan Qi"
        ],
        "submitted_date": "2025-06-01T15:19:52+00:00",
        "pdf_url": "https://arxiv.org/pdf/2506.03197v3",
        "similarity_score": 0.41538941860198975
    },
    {
        "arxiv_id": "2408.04689v2",
        "title": "Design of a Quality Management System based on the EU Artificial Intelligence Act",
        "abstract": "The EU AI Act mandates that providers and deployers of high-risk AI systems establish a quality management system (QMS). Among other criteria, a QMS shall help verify and document the AI system design and quality and monitor the proper implementation of all high-risk AI system requirements. Current research rarely explores practical solutions for implementing the EU AI Act. Instead, it tends to focus on theoretical concepts. As a result, more attention must be paid to tools that help humans actively check and document AI systems and orchestrate the implementation of all high-risk AI system requirements. Therefore, this paper introduces a new design concept and prototype for a QMS as a microservice Software as a Service web application. It connects directly to the AI system for verification and documentation and enables the orchestration and integration of various sub-services, which can be individually designed, each tailored to specific high-risk AI system requirements. The first version of the prototype connects to the Phi-3-mini-128k-instruct LLM as an example of an AI system and integrates a risk management system and a data management system. The prototype is evaluated through a qualitative assessment of the implemented requirements, a GPU memory and performance analysis, and an evaluation with IT, AI, and legal experts.",
        "authors": [
            "Henryk Mustroph",
            "Stefanie Rinderle-Ma"
        ],
        "submitted_date": "2024-08-08T12:14:02+00:00",
        "pdf_url": "https://arxiv.org/pdf/2408.04689v2",
        "similarity_score": 0.41179999709129333
    },
    {
        "arxiv_id": "2209.09631v2",
        "title": "De-Identification of French Unstructured Clinical Notes for Machine Learning Tasks",
        "abstract": "Unstructured textual data are at the heart of health systems: liaison letters between doctors, operating reports, coding of procedures according to the ICD-10 standard, etc. The details included in these documents make it possible to get to know the patient better, to better manage him or her, to better study the pathologies, to accurately remunerate the associated medical acts\\ldots All this seems to be (at least partially) within reach of today by artificial intelligence techniques. However, for obvious reasons of privacy protection, the designers of these AIs do not have the legal right to access these documents as long as they contain identifying data. De-identifying these documents, i.e. detecting and deleting all identifying information present in them, is a legally necessary step for sharing this data between two complementary worlds. Over the last decade, several proposals have been made to de-identify documents, mainly in English. While the detection scores are often high, the substitution methods are often not very robust to attack. In French, very few methods are based on arbitrary detection and/or substitution rules. In this paper, we propose a new comprehensive de-identification method dedicated to French-language medical documents. Both the approach for the detection of identifying elements (based on deep learning) and their substitution (based on differential privacy) are based on the most proven existing approaches. The result is an approach that effectively protects the privacy of the patients at the heart of these medical documents. The whole approach has been evaluated on a French language medical dataset of a French public hospital and the results are very encouraging.",
        "authors": [
            "Yakini Tchouka",
            "Jean-Fran√ßois Couchot",
            "Maxime Coulmeau",
            "David Laiymani",
            "Philippe Selles",
            "Azzedine Rahmani"
        ],
        "submitted_date": "2022-09-16T13:00:47+00:00",
        "pdf_url": "https://arxiv.org/pdf/2209.09631v2",
        "similarity_score": 0.41079413890838623
    },
    {
        "arxiv_id": "2407.06798v2",
        "title": "It Cannot Be Right If It Was Written by AI: On Lawyers' Preferences of Documents Perceived as Authored by an LLM vs a Human",
        "abstract": "Large Language Models (LLMs) enable a future in which certain types of legal documents may be generated automatically. This has a great potential to streamline legal processes, lower the cost of legal services, and dramatically increase access to justice. While many researchers focus on proposing and evaluating LLM-based applications supporting tasks in the legal domain, there is a notable lack of investigations into how legal professionals perceive content if they believe an LLM has generated it. Yet, this is a critical point as over-reliance or unfounded scepticism may influence whether such documents bring about appropriate legal consequences. This study is the necessary analysis of the ongoing transition towards mature generative AI systems. Specifically, we examined whether the perception of legal documents' by lawyers and law students (n=75) varies based on their assumed origin (human-crafted vs AI-generated). The participants evaluated the documents, focusing on their correctness and language quality. Our analysis revealed a clear preference for documents perceived as crafted by a human over those believed to be generated by AI. At the same time, most participants expect the future in which documents will be generated automatically. These findings could be leveraged by legal practitioners, policymakers, and legislators to implement and adopt legal document generation technology responsibly and to fuel the necessary discussions on how legal processes should be updated to reflect recent technological developments.",
        "authors": [
            "Jakub Harasta",
            "Tereza Novotn√°",
            "Jaromir Savelka"
        ],
        "submitted_date": "2024-07-09T12:11:25+00:00",
        "pdf_url": "https://arxiv.org/pdf/2407.06798v2",
        "similarity_score": 0.40802013874053955
    },
    {
        "arxiv_id": "2409.19104v1",
        "title": "Responsible AI in Open Ecosystems: Reconciling Innovation with Risk Assessment and Disclosure",
        "abstract": "The rapid scaling of AI has spurred a growing emphasis on ethical considerations in both development and practice. This has led to the formulation of increasingly sophisticated model auditing and reporting requirements, as well as governance frameworks to mitigate potential risks to individuals and society. At this critical juncture, we review the practical challenges of promoting responsible AI and transparency in informal sectors like OSS that support vital infrastructure and see widespread use. We focus on how model performance evaluation may inform or inhibit probing of model limitations, biases, and other risks. Our controlled analysis of 7903 Hugging Face projects found that risk documentation is strongly associated with evaluation practices. Yet, submissions (N=789) from the platform's most popular competitive leaderboard showed less accountability among high performers. Our findings can inform AI providers and legal scholars in designing interventions and policies that preserve open-source innovation while incentivizing ethical uptake.",
        "authors": [
            "Mahasweta Chakraborti",
            "Bert Joseph Prestoza",
            "Nicholas Vincent",
            "Seth Frey"
        ],
        "submitted_date": "2024-09-27T19:09:40+00:00",
        "pdf_url": "https://arxiv.org/pdf/2409.19104v1",
        "similarity_score": 0.4077841341495514
    },
    {
        "arxiv_id": "2410.12380v2",
        "title": "Evaluation of Attribution Bias in Generator-Aware Retrieval-Augmented Large Language Models",
        "abstract": "Attributing answers to source documents is an approach used to enhance the verifiability of a model's output in retrieval augmented generation (RAG). Prior work has mainly focused on improving and evaluating the attribution quality of large language models (LLMs) in RAG, but this may come at the expense of inducing biases in the attribution of answers. We define and examine two aspects in the evaluation of LLMs in RAG pipelines, namely attribution sensitivity and bias with respect to authorship information. We explicitly inform an LLM about the authors of source documents, instruct it to attribute its answers, and analyze (i) how sensitive the LLM's output is to the author of source documents, and (ii) whether the LLM exhibits a bias towards human-written or AI-generated source documents. We design an experimental setup in which we use counterfactual evaluation to study three LLMs in terms of their attribution sensitivity and bias in RAG pipelines. Our results show that adding authorship information to source documents can significantly change the attribution quality of LLMs by 3% to 18%. Moreover, we show that LLMs can have an attribution bias towards explicit human authorship, which can serve as a competing hypothesis for findings of prior work that shows that LLM-generated content may be preferred over human-written contents. Our findings indicate that metadata of source documents can influence LLMs' trust, and how they attribute their answers. Furthermore, our research highlights attribution bias and sensitivity as a novel aspect of brittleness in LLMs.",
        "authors": [
            "Amin Abolghasemi",
            "Leif Azzopardi",
            "Seyyed Hadi Hashemi",
            "Maarten de Rijke",
            "Suzan Verberne"
        ],
        "submitted_date": "2024-10-16T08:55:49+00:00",
        "pdf_url": "https://arxiv.org/pdf/2410.12380v2",
        "similarity_score": 0.4066431522369385
    },
    {
        "arxiv_id": "2510.11238v1",
        "title": "Attacks by Content: Automated Fact-checking is an AI Security Issue",
        "abstract": "When AI agents retrieve and reason over external documents, adversaries can manipulate the data they receive to subvert their behaviour. Previous research has studied indirect prompt injection, where the attacker injects malicious instructions. We argue that injection of instructions is not necessary to manipulate agents - attackers could instead supply biased, misleading, or false information. We term this an attack by content. Existing defenses, which focus on detecting hidden commands, are ineffective against attacks by content. To defend themselves and their users, agents must critically evaluate retrieved information, corroborating claims with external evidence and evaluating source trustworthiness. We argue that this is analogous to an existing NLP task, automated fact-checking, which we propose to repurpose as a cognitive self-defense tool for agents.",
        "authors": [
            "Michael Schlichtkrull"
        ],
        "submitted_date": "2025-10-13T10:18:48+00:00",
        "pdf_url": "https://arxiv.org/pdf/2510.11238v1",
        "similarity_score": 0.4065972566604614
    },
    {
        "arxiv_id": "2112.14933v1",
        "title": "RheFrameDetect: A Text Classification System for Automatic Detection of Rhetorical Frames in AI from Open Sources",
        "abstract": "Rhetorical Frames in AI can be thought of as expressions that describe AI development as a competition between two or more actors, such as governments or companies. Examples of such Frames include robotic arms race, AI rivalry, technological supremacy, cyberwarfare dominance and 5G race. Detection of Rhetorical Frames from open sources can help us track the attitudes of governments or companies towards AI, specifically whether attitudes are becoming more cooperative or competitive over time. Given the rapidly increasing volumes of open sources (online news media, twitter, blogs), it is difficult for subject matter experts to identify Rhetorical Frames in (near) real-time. Moreover, these sources are in general unstructured (noisy) and therefore, detecting Frames from these sources will require state-of-the-art text classification techniques. In this paper, we develop RheFrameDetect, a text classification system for (near) real-time capture of Rhetorical Frames from open sources. Given an input document, RheFrameDetect employs text classification techniques at multiple levels (document level and paragraph level) to identify all occurrences of Frames used in the discussion of AI. We performed extensive evaluation of the text classification techniques used in RheFrameDetect against human annotated Frames from multiple news sources. To further demonstrate the effectiveness of RheFrameDetect, we show multiple case studies depicting the Frames identified by RheFrameDetect compared against human annotated Frames.",
        "authors": [
            "Saurav Ghosh",
            "Philippe Loustaunau"
        ],
        "submitted_date": "2021-12-30T05:39:42+00:00",
        "pdf_url": "https://arxiv.org/pdf/2112.14933v1",
        "similarity_score": 0.4056975245475769
    },
    {
        "arxiv_id": "2511.12142v1",
        "title": "MAVIS: A Benchmark for Multimodal Source Attribution in Long-form Visual Question Answering",
        "abstract": "Source attribution aims to enhance the reliability of AI-generated answers by including references for each statement, helping users validate the provided answers. However, existing work has primarily focused on text-only scenario and largely overlooked the role of multimodality. We introduce MAVIS, the first benchmark designed to evaluate multimodal source attribution systems that understand user intent behind visual questions, retrieve multimodal evidence, and generate long-form answers with citations. Our dataset comprises 157K visual QA instances, where each answer is annotated with fact-level citations referring to multimodal documents. We develop fine-grained automatic metrics along three dimensions of informativeness, groundedness, and fluency, and demonstrate their strong correlation with human judgments. Our key findings are threefold: (1) LVLMs with multimodal RAG generate more informative and fluent answers than unimodal RAG, but they exhibit weaker groundedness for image documents than for text documents, a gap amplified in multimodal settings. (2) Given the same multimodal documents, there is a trade-off between informativeness and groundedness across different prompting methods. (3) Our proposed method highlights mitigating contextual bias in interpreting image documents as a crucial direction for future research. The dataset and experimental code are available at https://github.com/seokwon99/MAVIS",
        "authors": [
            "Seokwon Song",
            "Minsu Park",
            "Gunhee Kim"
        ],
        "submitted_date": "2025-11-15T10:14:59+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.12142v1",
        "similarity_score": 0.4047074317932129
    },
    {
        "arxiv_id": "2411.06017v1",
        "title": "Provocation on Expertise in Social Impact Evaluations of Generative AI (and Beyond)",
        "abstract": "Social impact evaluations are emerging as a useful tool to understand, document, and evaluate the societal impacts of generative AI. In this provocation, we begin to think carefully about the types of experts and expertise that are needed to conduct robust social impact evaluations of generative AI. We suggest that doing so will require thoughtfully eliciting and integrating insights from a range of \"domain experts\" and \"experiential experts,\" and close with five open questions.",
        "authors": [
            "Zoe Kahn",
            "Nitin Kohli"
        ],
        "submitted_date": "2024-11-09T00:35:25+00:00",
        "pdf_url": "https://arxiv.org/pdf/2411.06017v1",
        "similarity_score": 0.4040508270263672
    },
    {
        "arxiv_id": "1906.07328v2",
        "title": "Losing Confidence in Quality: Unspoken Evolution of Computer Vision Services",
        "abstract": "Recent advances in artificial intelligence (AI) and machine learning (ML), such as computer vision, are now available as intelligent services and their accessibility and simplicity is compelling. Multiple vendors now offer this technology as cloud services and developers want to leverage these advances to provide value to end-users. However, there is no firm investigation into the maintenance and evolution risks arising from use of these intelligent services; in particular, their behavioural consistency and transparency of their functionality. We evaluated the responses of three different intelligent services (specifically computer vision) over 11 months using 3 different data sets, verifying responses against the respective documentation and assessing evolution risk. We found that there are: (1) inconsistencies in how these services behave; (2) evolution risk in the responses; and (3) a lack of clear communication that documents these risks and inconsistencies. We propose a set of recommendations to both developers and intelligent service providers to inform risk and assist maintainability.",
        "authors": [
            "Alex Cummaudo",
            "Rajesh Vasa",
            "John Grundy",
            "Mohamed Abdelrazek",
            "Andrew Cain"
        ],
        "submitted_date": "2019-06-18T01:11:43+00:00",
        "pdf_url": "https://arxiv.org/pdf/1906.07328v2",
        "similarity_score": 0.4027426242828369
    },
    {
        "arxiv_id": "2504.16573v1",
        "title": "PsyCounAssist: A Full-Cycle AI-Powered Psychological Counseling Assistant System",
        "abstract": "Psychological counseling is a highly personalized and dynamic process that requires therapists to continuously monitor emotional changes, document session insights, and maintain therapeutic continuity. In this paper, we introduce PsyCounAssist, a comprehensive AI-powered counseling assistant system specifically designed to augment psychological counseling practices. PsyCounAssist integrates multimodal emotion recognition combining speech and photoplethysmography (PPG) signals for accurate real-time affective analysis, automated structured session reporting using large language models (LLMs), and personalized AI-generated follow-up support. Deployed on Android-based tablet devices, the system demonstrates practical applicability and flexibility in real-world counseling scenarios. Experimental evaluation confirms the reliability of PPG-based emotional classification and highlights the system's potential for non-intrusive, privacy-aware emotional support. PsyCounAssist represents a novel approach to ethically and effectively integrating AI into psychological counseling workflows.",
        "authors": [
            "Xianghe Liu",
            "Jiaqi Xu",
            "Tao Sun"
        ],
        "submitted_date": "2025-04-23T09:49:05+00:00",
        "pdf_url": "https://arxiv.org/pdf/2504.16573v1",
        "similarity_score": 0.3988693058490753
    },
    {
        "arxiv_id": "2501.03403v3",
        "title": "BoundingDocs: a Unified Dataset for Document Question Answering with Spatial Annotations",
        "abstract": "We present a unified dataset for document Question-Answering (QA), which is obtained combining several public datasets related to Document AI and visually rich document understanding (VRDU). Our main contribution is twofold: on the one hand we reformulate existing Document AI tasks, such as Information Extraction (IE), into a Question-Answering task, making it a suitable resource for training and evaluating Large Language Models; on the other hand, we release the OCR of all the documents and include the exact position of the answer to be found in the document image as a bounding box. Using this dataset, we explore the impact of different prompting techniques (that might include bounding box information) on the performance of open-weight models, identifying the most effective approaches for document comprehension.",
        "authors": [
            "Simone Giovannini",
            "Fabio Coppini",
            "Andrea Gemelli",
            "Simone Marinai"
        ],
        "submitted_date": "2025-01-06T21:46:22+00:00",
        "pdf_url": "https://arxiv.org/pdf/2501.03403v3",
        "similarity_score": 0.39748120307922363
    },
    {
        "arxiv_id": "2411.18947v1",
        "title": "ICLERB: In-Context Learning Embedding and Reranker Benchmark",
        "abstract": "In-Context Learning (ICL) enables Large Language Models (LLMs) to perform new tasks by conditioning on prompts with relevant information. Retrieval-Augmented Generation (RAG) enhances ICL by incorporating retrieved documents into the LLM's context at query time. However, traditional retrieval methods focus on semantic relevance, treating retrieval as a search problem. In this paper, we propose reframing retrieval for ICL as a recommendation problem, aiming to select documents that maximize utility in ICL tasks. We introduce the In-Context Learning Embedding and Reranker Benchmark (ICLERB), a novel evaluation framework that compares retrievers based on their ability to enhance LLM accuracy in ICL settings. Additionally, we propose a novel Reinforcement Learning-to-Rank from AI Feedback (RLRAIF) algorithm, designed to fine-tune retrieval models using minimal feedback from the LLM. Our experimental results reveal notable differences between ICLERB and existing benchmarks, and demonstrate that small models fine-tuned with our RLRAIF algorithm outperform large state-of-the-art retrieval models. These findings highlight the limitations of existing evaluation methods and the need for specialized benchmarks and training strategies adapted to ICL.",
        "authors": [
            "Marie Al Ghossein",
            "Emile Contal",
            "Alexandre Robicquet"
        ],
        "submitted_date": "2024-11-28T06:28:45+00:00",
        "pdf_url": "https://arxiv.org/pdf/2411.18947v1",
        "similarity_score": 0.39468905329704285
    },
    {
        "arxiv_id": "2309.14735v2",
        "title": "Legal Question-Answering in the Indian Context: Efficacy, Challenges, and Potential of Modern AI Models",
        "abstract": "Legal QA platforms bear the promise to metamorphose the manner in which legal experts engage with jurisprudential documents. In this exposition, we embark on a comparative exploration of contemporary AI frameworks, gauging their adeptness in catering to the unique demands of the Indian legal milieu, with a keen emphasis on Indian Legal Question Answering (AILQA). Our discourse zeroes in on an array of retrieval and QA mechanisms, positioning the OpenAI GPT model as a reference point. The findings underscore the proficiency of prevailing AILQA paradigms in decoding natural language prompts and churning out precise responses. The ambit of this study is tethered to the Indian criminal legal landscape, distinguished by its intricate nature and associated logistical constraints. To ensure a holistic evaluation, we juxtapose empirical metrics with insights garnered from seasoned legal practitioners, thereby painting a comprehensive picture of AI's potential and challenges within the realm of Indian legal QA.",
        "authors": [
            "Shubham Kumar Nigam",
            "Shubham Kumar Mishra",
            "Ayush Kumar Mishra",
            "Noel Shallum",
            "Arnab Bhattacharya"
        ],
        "submitted_date": "2023-09-26T07:56:55+00:00",
        "pdf_url": "https://arxiv.org/pdf/2309.14735v2",
        "similarity_score": 0.3942846357822418
    },
    {
        "arxiv_id": "2412.04923v1",
        "title": "HyperGraphOS: A Meta Operating System for Science and Engineering",
        "abstract": "This paper presents HyperGraphOS, an innovative Operating System designed for the scientific and engineering domains. It combines model based engineering, graph modeling, data containers, and computational tools, offering users a dynamic workspace for creating and managing complex models represented as customizable graphs. Using a web based architecture, HyperGraphOS requires only a modern browser to organize knowledge, documents, and content into interconnected models. Domain Specific Languages drive workspace navigation, code generation, AI integration, and process organization.The platform models function as both visual drawings and data structures, enabling dynamic modifications and inspection, both interactively and programmatically. HyperGraphOS was evaluated across various domains, including virtual avatars, robotic task planning using Large Language Models, and meta modeling for feature based code development. Results show significant improvements in flexibility, data management, computation, and document handling.",
        "authors": [
            "Antonello Ceravola",
            "Frank Joublin",
            "Ahmed R. Sadik",
            "Bram Bolder",
            "Juha-Pekka Tolvanen"
        ],
        "submitted_date": "2024-12-06T10:21:41+00:00",
        "pdf_url": "https://arxiv.org/pdf/2412.04923v1",
        "similarity_score": 0.39305686950683594
    },
    {
        "arxiv_id": "2203.04706v2",
        "title": "Data Representativity for Machine Learning and AI Systems",
        "abstract": "Data representativity is crucial when drawing inference from data through machine learning models. Scholars have increased focus on unraveling the bias and fairness in models, also in relation to inherent biases in the input data. However, limited work exists on the representativity of samples (datasets) for appropriate inference in AI systems. This paper reviews definitions and notions of a representative sample and surveys their use in scientific AI literature. We introduce three measurable concepts to help focus the notions and evaluate different data samples. Furthermore, we demonstrate that the contrast between a representative sample in the sense of coverage of the input space, versus a representative sample mimicking the distribution of the target population is of particular relevance when building AI systems. Through empirical demonstrations on US Census data, we evaluate the opposing inherent qualities of these concepts. Finally, we propose a framework of questions for creating and documenting data with data representativity in mind, as an addition to existing dataset documentation templates.",
        "authors": [
            "Line H. Clemmensen",
            "Rune D. Kj√¶rsgaard"
        ],
        "submitted_date": "2022-03-09T13:34:52+00:00",
        "pdf_url": "https://arxiv.org/pdf/2203.04706v2",
        "similarity_score": 0.39152461290359497
    },
    {
        "arxiv_id": "2305.15047v3",
        "title": "Ghostbuster: Detecting Text Ghostwritten by Large Language Models",
        "abstract": "We introduce Ghostbuster, a state-of-the-art system for detecting AI-generated text. Our method works by passing documents through a series of weaker language models, running a structured search over possible combinations of their features, and then training a classifier on the selected features to predict whether documents are AI-generated. Crucially, Ghostbuster does not require access to token probabilities from the target model, making it useful for detecting text generated by black-box models or unknown model versions. In conjunction with our model, we release three new datasets of human- and AI-generated text as detection benchmarks in the domains of student essays, creative writing, and news articles. We compare Ghostbuster to a variety of existing detectors, including DetectGPT and GPTZero, as well as a new RoBERTa baseline. Ghostbuster achieves 99.0 F1 when evaluated across domains, which is 5.9 F1 higher than the best preexisting model. It also outperforms all previous approaches in generalization across writing domains (+7.5 F1), prompting strategies (+2.1 F1), and language models (+4.4 F1). We also analyze the robustness of our system to a variety of perturbations and paraphrasing attacks and evaluate its performance on documents written by non-native English speakers.",
        "authors": [
            "Vivek Verma",
            "Eve Fleisig",
            "Nicholas Tomlin",
            "Dan Klein"
        ],
        "submitted_date": "2023-05-24T11:37:10+00:00",
        "pdf_url": "https://arxiv.org/pdf/2305.15047v3",
        "similarity_score": 0.38816896080970764
    },
    {
        "arxiv_id": "2506.02959v1",
        "title": "HACo-Det: A Study Towards Fine-Grained Machine-Generated Text Detection under Human-AI Coauthoring",
        "abstract": "The misuse of large language models (LLMs) poses potential risks, motivating the development of machine-generated text (MGT) detection. Existing literature primarily concentrates on binary, document-level detection, thereby neglecting texts that are composed jointly by human and LLM contributions. Hence, this paper explores the possibility of fine-grained MGT detection under human-AI coauthoring. We suggest fine-grained detectors can pave pathways toward coauthored text detection with a numeric AI ratio. Specifically, we propose a dataset, HACo-Det, which produces human-AI coauthored texts via an automatic pipeline with word-level attribution labels. We retrofit seven prevailing document-level detectors to generalize them to word-level detection. Then we evaluate these detectors on HACo-Det on both word- and sentence-level detection tasks. Empirical results show that metric-based methods struggle to conduct fine-grained detection with a 0.462 average F1 score, while finetuned models show superior performance and better generalization across domains. However, we argue that fine-grained co-authored text detection is far from solved. We further analyze factors influencing performance, e.g., context window, and highlight the limitations of current methods, pointing to potential avenues for improvement.",
        "authors": [
            "Zhixiong Su",
            "Yichen Wang",
            "Herun Wan",
            "Zhaohan Zhang",
            "Minnan Luo"
        ],
        "submitted_date": "2025-06-03T14:52:44+00:00",
        "pdf_url": "https://arxiv.org/pdf/2506.02959v1",
        "similarity_score": 0.38554656505584717
    },
    {
        "arxiv_id": "2511.04910v2",
        "title": "SDS KoPub VDR: A Benchmark Dataset for Visual Document Retrieval in Korean Public Documents",
        "abstract": "Existing benchmarks for visual document retrieval (VDR) largely overlook non-English languages and the structural complexity of official publications. To address this gap, we introduce SDS KoPub VDR, the first large-scale, public benchmark for retrieving and understanding Korean public documents. The benchmark is built upon 361 real-world documents, including 256 files under the KOGL Type 1 license and 105 from official legal portals, capturing complex visual elements like tables, charts, and multi-column layouts. To establish a reliable evaluation set, we constructed 600 query-page-answer triples. These were initially generated using multimodal models (e.g., GPT-4o) and subsequently underwent human verification to ensure factual accuracy and contextual relevance. The queries span six major public domains and are categorized by the reasoning modality required: text-based, visual-based, and cross-modal. We evaluate SDS KoPub VDR on two complementary tasks: (1) text-only retrieval and (2) multimodal retrieval, which leverages visual features alongside text. This dual-task evaluation reveals substantial performance gaps, particularly in multimodal scenarios requiring cross-modal reasoning, even for state-of-the-art models. As a foundational resource, SDS KoPub VDR enables rigorous and fine-grained evaluation and provides a roadmap for advancing multimodal AI in real-world document intelligence. The dataset is available at https://huggingface.co/datasets/SamsungSDS-Research/SDS-KoPub-VDR-Benchmark.",
        "authors": [
            "Jaehoon Lee",
            "Sohyun Kim",
            "Wanggeun Park",
            "Geon Lee",
            "Seungkyung Kim",
            "Minyoung Lee"
        ],
        "submitted_date": "2025-11-07T01:16:07+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.04910v2",
        "similarity_score": 0.37874022126197815
    },
    {
        "arxiv_id": "2412.03590v1",
        "title": "Enhancing Document AI Data Generation Through Graph-Based Synthetic Layouts",
        "abstract": "The development of robust Document AI models has been constrained by limited access to high-quality, labeled datasets, primarily due to data privacy concerns, scarcity, and the high cost of manual annotation. Traditional methods of synthetic data generation, such as text and image augmentation, have proven effective for increasing data diversity but often fail to capture the complex layout structures present in real world documents. This paper proposes a novel approach to synthetic document layout generation using Graph Neural Networks (GNNs). By representing document elements (e.g., text blocks, images, tables) as nodes in a graph and their spatial relationships as edges, GNNs are trained to generate realistic and diverse document layouts. This method leverages graph-based learning to ensure structural coherence and semantic consistency, addressing the limitations of traditional augmentation techniques. The proposed framework is evaluated on tasks such as document classification, named entity recognition (NER), and information extraction, demonstrating significant performance improvements. Furthermore, we address the computational challenges of GNN based synthetic data generation and propose solutions to mitigate domain adaptation issues between synthetic and real-world datasets. Our experimental results show that graph-augmented document layouts outperform existing augmentation techniques, offering a scalable and flexible solution for training Document AI models.",
        "authors": [
            "Amit Agarwal",
            "Hitesh Patel",
            "Priyaranjan Pattnayak",
            "Srikant Panda",
            "Bhargava Kumar",
            "Tejaswini Kumar"
        ],
        "submitted_date": "2024-11-27T21:15:02+00:00",
        "pdf_url": "https://arxiv.org/pdf/2412.03590v1",
        "similarity_score": 0.37738126516342163
    },
    {
        "arxiv_id": "2412.00151v2",
        "title": "DLaVA: Document Language and Vision Assistant for Answer Localization with Enhanced Interpretability and Trustworthiness",
        "abstract": "Document Visual Question Answering (VQA) demands robust integration of text detection, recognition, and spatial reasoning to interpret complex document layouts. In this work, we introduce DLaVA, a novel, training-free pipeline that leverages Multimodal Large Language Models (MLLMs) for zero-shot answer localization in order to improve trustworthiness, interpretability, and explainability. By leveraging an innovative OCR-free approach that organizes text regions with unique bounding box IDs, the proposed method preserves spatial contexts without relying on iterative OCR or chain-of-thought reasoning, thus substantially reducing the computational complexity. We further enhance the evaluation protocol by integrating Intersection over Union (IoU) metrics alongside Average Normalized Levenshtein Similarity (ANLS), thereby ensuring that not only textual accuracy is considered, but spatial accuracy is taken into account, ultimately reducing the risks of AI hallucinations and improving trustworthiness. Experiments on benchmark datasets demonstrate competitive performance compared to state-of-the-art techniques, with significantly lower computational complexity and enhanced accuracies and reliability for high-stakes applications. The code and datasets utilized in this study for DLaVA are accessible at: https://github.com/ahmad-shirazi/AnnotMLLM.",
        "authors": [
            "Ahmad Mohammadshirazi",
            "Pinaki Prasad Guha Neogi",
            "Ser-Nam Lim",
            "Rajiv Ramnath"
        ],
        "submitted_date": "2024-11-29T06:17:11+00:00",
        "pdf_url": "https://arxiv.org/pdf/2412.00151v2",
        "similarity_score": 0.37677162885665894
    },
    {
        "arxiv_id": "2508.17884v2",
        "title": "PhantomLint: Principled Detection of Hidden LLM Prompts in Structured Documents",
        "abstract": "Hidden LLM prompts have appeared in online documents with increasing frequency. Their goal is to trigger indirect prompt injection attacks while remaining undetected from human oversight, to manipulate LLM-powered automated document processing systems, against applications as diverse as r√©sum√© screeners through to academic peer review processes. Detecting hidden LLM prompts is therefore important for ensuring trust in AI-assisted human decision making.   This paper presents the first principled approach to hidden LLM prompt detection in structured documents. We implement our approach in a prototype tool called PhantomLint. We evaluate PhantomLint against a corpus of 3,402 documents, including both PDF and HTML documents, and covering academic paper preprints, CVs, theses and more. We find that our approach is generally applicable against a wide range of methods for hiding LLM prompts from visual inspection, has a very low false positive rate (approx. 0.092%), is practically useful for detecting hidden LLM prompts in real documents, while achieving acceptable performance.",
        "authors": [
            "Toby Murray"
        ],
        "submitted_date": "2025-08-25T10:45:10+00:00",
        "pdf_url": "https://arxiv.org/pdf/2508.17884v2",
        "similarity_score": 0.3704613447189331
    },
    {
        "arxiv_id": "2206.02628v2",
        "title": "HYCEDIS: HYbrid Confidence Engine for Deep Document Intelligence System",
        "abstract": "Measuring the confidence of AI models is critical for safely deploying AI in real-world industrial systems. One important application of confidence measurement is information extraction from scanned documents. However, there exists no solution to provide reliable confidence score for current state-of-the-art deep-learning-based information extractors. In this paper, we propose a complete and novel architecture to measure confidence of current deep learning models in document information extraction task. Our architecture consists of a Multi-modal Conformal Predictor and a Variational Cluster-oriented Anomaly Detector, trained to faithfully estimate its confidence on its outputs without the need of host models modification. We evaluate our architecture on real-wold datasets, not only outperforming competing confidence estimators by a huge margin but also demonstrating generalization ability to out-of-distribution data.",
        "authors": [
            "Bao-Sinh Nguyen",
            "Quang-Bach Tran",
            "Tuan-Anh Nguyen Dang",
            "Duc Nguyen",
            "Hung Le"
        ],
        "submitted_date": "2022-06-01T09:57:34+00:00",
        "pdf_url": "https://arxiv.org/pdf/2206.02628v2",
        "similarity_score": 0.3674269914627075
    },
    {
        "arxiv_id": "2308.15517v1",
        "title": "Document AI: A Comparative Study of Transformer-Based, Graph-Based Models, and Convolutional Neural Networks For Document Layout Analysis",
        "abstract": "Document AI aims to automatically analyze documents by leveraging natural language processing and computer vision techniques. One of the major tasks of Document AI is document layout analysis, which structures document pages by interpreting the content and spatial relationships of layout, image, and text. This task can be image-centric, wherein the aim is to identify and label various regions such as authors and paragraphs, or text-centric, where the focus is on classifying individual words in a document. Although there are increasingly sophisticated methods for improving layout analysis, doubts remain about the extent to which their findings can be generalized to a broader context. Specifically, prior work developed systems based on very different architectures, such as transformer-based, graph-based, and CNNs. However, no work has mentioned the effectiveness of these models in a comparative analysis. Moreover, while language-independent Document AI models capable of knowledge transfer have been developed, it remains to be investigated to what degree they can effectively transfer knowledge. In this study, we aim to fill these gaps by conducting a comparative evaluation of state-of-the-art models in document layout analysis and investigating the potential of cross-lingual layout analysis by utilizing machine translation techniques.",
        "authors": [
            "Sotirios Kastanas",
            "Shaomu Tan",
            "Yi He"
        ],
        "submitted_date": "2023-08-29T16:58:03+00:00",
        "pdf_url": "https://arxiv.org/pdf/2308.15517v1",
        "similarity_score": 0.3646101951599121
    },
    {
        "arxiv_id": "2511.08639v1",
        "title": "The Journal of Prompt-Engineered Philosophy Or: How I Started to Track AI Assistance and Stopped Worrying About Slop",
        "abstract": "Academic publishing increasingly requires authors to disclose AI assistance, yet imposes reputational costs for doing so--especially when such assistance is substantial. This article analyzes that structural contradiction, showing how incentives discourage transparency in precisely the work where it matters most. Traditional venues cannot resolve this tension through policy tweaks alone, as the underlying prestige economy rewards opacity. To address this, the article proposes an alternative publishing infrastructure: a venue outside prestige systems that enforces mandatory disclosure, enables reproduction-based review, and supports ecological validity through detailed documentation. As a demonstration of this approach, the article itself is presented as an example of AI-assisted scholarship under reasonably detailed disclosure, with representative prompt logs and modification records included. Rather than taking a position for or against AI-assisted scholarship, the article outlines conditions under which such work can be evaluated on its own terms: through transparent documentation, verification-oriented review, and participation by methodologically committed scholars. While focused on AI, the framework speaks to broader questions about how academic systems handle methodological innovation.",
        "authors": [
            "Michele Loi"
        ],
        "submitted_date": "2025-11-10T08:56:21+00:00",
        "pdf_url": "https://arxiv.org/pdf/2511.08639v1",
        "similarity_score": 0.36241576075553894
    },
    {
        "arxiv_id": "2509.17830v2",
        "title": "Fine-Grained Detection of AI-Generated Text Using Sentence-Level Segmentation",
        "abstract": "Generation of Artificial Intelligence (AI) texts in important works has become a common practice that can be used to misuse and abuse AI at various levels. Traditional AI detectors often rely on document-level classification, which struggles to identify AI content in hybrid or slightly edited texts designed to avoid detection, leading to concerns about the model's efficiency, which makes it hard to distinguish between human-written and AI-generated texts. A sentence-level sequence labeling model proposed to detect transitions between human- and AI-generated text, leveraging nuanced linguistic signals overlooked by document-level classifiers. By this method, detecting and segmenting AI and human-written text within a single document at the token-level granularity is achieved. Our model combines the state-of-the-art pre-trained Transformer models, incorporating Neural Networks (NN) and Conditional Random Fields (CRFs). This approach extends the power of transformers to extract semantic and syntactic patterns, and the neural network component to capture enhanced sequence-level representations, thereby improving the boundary predictions by the CRF layer, which enhances sequence recognition and further identification of the partition between Human- and AI-generated texts. The evaluation is performed on two publicly available benchmark datasets containing collaborative human and AI-generated texts. Our experimental comparisons are with zero-shot detectors and the existing state-of-the-art models, along with rigorous ablation studies to justify that this approach, in particular, can accurately detect the spans of AI texts in a completely collaborative text. All our source code and the processed datasets are available in our GitHub repository.",
        "authors": [
            "Lekkala Sai Teja",
            "Annepaka Yadagiri",
            "Partha Pakray",
            "Chukhu Chunka",
            "Mangadoddi Srikar Vardhan"
        ],
        "submitted_date": "2025-09-22T14:22:55+00:00",
        "pdf_url": "https://arxiv.org/pdf/2509.17830v2",
        "similarity_score": 0.359142541885376
    },
    {
        "arxiv_id": "2503.03924v1",
        "title": "De-skilling, Cognitive Offloading, and Misplaced Responsibilities: Potential Ironies of AI-Assisted Design",
        "abstract": "The rapid adoption of generative AI (GenAI) in design has sparked discussions about its benefits and unintended consequences. While AI is often framed as a tool for enhancing productivity by automating routine tasks, historical research on automation warns of paradoxical effects, such as de-skilling and misplaced responsibilities. To assess UX practitioners' perceptions of AI, we analyzed over 120 articles and discussions from UX-focused subreddits. Our findings indicate that while practitioners express optimism about AI reducing repetitive work and augmenting creativity, they also highlight concerns about over-reliance, cognitive offloading, and the erosion of critical design skills. Drawing from human-automation interaction literature, we discuss how these perspectives align with well-documented automation ironies and function allocation challenges. We argue that UX professionals should critically evaluate AI's role beyond immediate productivity gains and consider its long-term implications for creative autonomy and expertise. This study contributes empirical insights into practitioners' perspectives and links them to broader debates on automation in design.",
        "authors": [
            "Prakash Shukla",
            "Phuong Bui",
            "Sean S Levy",
            "Max Kowalski",
            "Ali Baigelenov",
            "Paul Parsons"
        ],
        "submitted_date": "2025-03-05T21:47:16+00:00",
        "pdf_url": "https://arxiv.org/pdf/2503.03924v1",
        "similarity_score": 0.35743027925491333
    },
    {
        "arxiv_id": "2510.22279v1",
        "title": "The AI Tutor in Engineering Education: Design, Results, and Redesign of an Experience in Hydrology at an Argentine University",
        "abstract": "The emergence of Generative Artificial Intelligence (GenAI) has reshaped higher education, presenting both opportunities and ethical-pedagogical challenges. This article presents an empirical case study on the complete cycle (design, initial failure, redesign, and re-evaluation) of an intervention using an AI Tutor (ChatGPT) in the \"Hydrology and Hydraulic Works\" course (Civil Engineering, UTN-FRT, Argentina). The study documents two interventions in the same cohort (n=23). The first resulted in widespread failure (0% pass rate) due to superficial use and serious academic integrity issues (65% similarity, copies > 80%). This failure forced a comprehensive methodological redesign. The second intervention, based on a redesigned prompt (Prompt V2) with strict evidence controls (mandatory Appendix A with exported chat, minimum time $\\geq$ 120 minutes, verifiable numerical exercise) and a refined rubric (Rubric V2), showed significantly better results: a median score of 88/100 and verifiable compliance with genuine interaction processes. Using a mixed-methods approach (reproducible document analysis and rubric analysis), the impact of the redesign on integrity and technical performance is evaluated. The results demonstrate that, without explicit process controls, students prioritize efficiency over deep learning, submitting documents without real traceability. A transferable assessment protocol for STEM courses is proposed, centered on \"auditable personal zones,\" to foster higher-order thinking. The study provides key empirical evidence from the context of a public Latin American university.",
        "authors": [
            "Hugo Roger Paz"
        ],
        "submitted_date": "2025-10-25T12:51:46+00:00",
        "pdf_url": "https://arxiv.org/pdf/2510.22279v1",
        "similarity_score": 0.355901300907135
    },
    {
        "arxiv_id": "2012.00078v1",
        "title": "Why Did the Robot Cross the Road? A User Study of Explanation in Human-Robot Interaction",
        "abstract": "This work documents a pilot user study evaluating the effectiveness of contrastive, causal and example explanations in supporting human understanding of AI in a hypothetical commonplace human robot interaction HRI scenario. In doing so, this work situates explainable AI XAI in the context of the social sciences and suggests that HRI explanations are improved when informed by the social sciences.",
        "authors": [
            "Zachary Taschdjian"
        ],
        "submitted_date": "2020-11-30T20:02:19+00:00",
        "pdf_url": "https://arxiv.org/pdf/2012.00078v1",
        "similarity_score": 0.35493165254592896
    },
    {
        "arxiv_id": "2510.24823v1",
        "title": "Do Chatbots Walk the Talk of Responsible AI?",
        "abstract": "This study examines whether leading AI chatbot companies implement the responsible AI principles they publicly advocate. The authors used a mixed-methods approach analyzing four major chatbots (ChatGPT, Gemini, DeepSeek, and Grok) across company websites, technical documentation, and direct chatbot evaluations. We found significant gaps between corporate rhetoric and practice.",
        "authors": [
            "Susan Ariel Aaronson",
            "Michael Moreno"
        ],
        "submitted_date": "2025-10-28T15:31:24+00:00",
        "pdf_url": "https://arxiv.org/pdf/2510.24823v1",
        "similarity_score": 0.34999197721481323
    },
    {
        "arxiv_id": "2402.06353v3",
        "title": "Copycats: the many lives of a publicly available medical imaging dataset",
        "abstract": "Medical Imaging (MI) datasets are fundamental to artificial intelligence in healthcare. The accuracy, robustness, and fairness of diagnostic algorithms depend on the data (and its quality) used to train and evaluate the models. MI datasets used to be proprietary, but have become increasingly available to the public, including on community-contributed platforms (CCPs) like Kaggle or HuggingFace. While open data is important to enhance the redistribution of data's public value, we find that the current CCP governance model fails to uphold the quality needed and recommended practices for sharing, documenting, and evaluating datasets. In this paper, we conduct an analysis of publicly available machine learning datasets on CCPs, discussing datasets' context, and identifying limitations and gaps in the current CCP landscape. We highlight differences between MI and computer vision datasets, particularly in the potentially harmful downstream effects from poor adoption of recommended dataset management practices. We compare the analyzed datasets across several dimensions, including data sharing, data documentation, and maintenance. We find vague licenses, lack of persistent identifiers and storage, duplicates, and missing metadata, with differences between the platforms. Our research contributes to efforts in responsible data curation and AI algorithms for healthcare.",
        "authors": [
            "Amelia Jim√©nez-S√°nchez",
            "Natalia-Rozalia Avlona",
            "Dovile Juodelyte",
            "Th√©o Sourget",
            "Caroline Vang-Larsen",
            "Anna Rogers",
            "Hubert Dariusz ZajƒÖc",
            "Veronika Cheplygina"
        ],
        "submitted_date": "2024-02-09T12:01:22+00:00",
        "pdf_url": "https://arxiv.org/pdf/2402.06353v3",
        "similarity_score": 0.3486175239086151
    },
    {
        "arxiv_id": "2507.04590v1",
        "title": "VLM2Vec-V2: Advancing Multimodal Embedding for Videos, Images, and Visual Documents",
        "abstract": "Multimodal embedding models have been crucial in enabling various downstream tasks such as semantic similarity, information retrieval, and clustering over different modalities. However, existing multimodal embeddings like VLM2Vec, E5-V, GME are predominantly focused on natural images, with limited support for other visual forms such as videos and visual documents. This restricts their applicability in real-world scenarios, including AI agents, multi-modal search and recommendation, and retrieval-augmented generation (RAG). To close this gap, we propose VLM2Vec-V2, a unified framework for learning embeddings across diverse visual forms. First, we introduce MMEB-V2, a comprehensive benchmark that extends MMEB with five new task types: visual document retrieval, video retrieval, temporal grounding, video classification and video question answering - spanning text, image, video, and visual document inputs. Next, we train VLM2Vec-V2, a general-purpose embedding model that supports text, image, video, and visual document inputs. Extensive experiments show that VLM2Vec-V2 achieves strong performance not only on the newly introduced video and document retrieval tasks, but also improves over prior baselines on the original image benchmarks. Through extensive evaluation, our study offers insights into the generalizability of various multimodal embedding models and highlights effective strategies for unified embedding learning, laying the groundwork for more scalable and adaptable representation learning in both research and real-world settings.",
        "authors": [
            "Rui Meng",
            "Ziyan Jiang",
            "Ye Liu",
            "Mingyi Su",
            "Xinyi Yang",
            "Yuepeng Fu",
            "Can Qin",
            "Zeyuan Chen",
            "Ran Xu",
            "Caiming Xiong",
            "Yingbo Zhou",
            "Wenhu Chen",
            "Semih Yavuz"
        ],
        "submitted_date": "2025-07-07T00:51:57+00:00",
        "pdf_url": "https://arxiv.org/pdf/2507.04590v1",
        "similarity_score": 0.34038737416267395
    },
    {
        "arxiv_id": "2110.09232v1",
        "title": "Accountability in AI: From Principles to Industry-specific Accreditation",
        "abstract": "Recent AI-related scandals have shed a spotlight on accountability in AI, with increasing public interest and concern. This paper draws on literature from public policy and governance to make two contributions. First, we propose an AI accountability ecosystem as a useful lens on the system, with different stakeholders requiring and contributing to specific accountability mechanisms. We argue that the present ecosystem is unbalanced, with a need for improved transparency via AI explainability and adequate documentation and process formalisation to support internal audit, leading up eventually to external accreditation processes. Second, we use a case study in the gambling sector to illustrate in a subset of the overall ecosystem the need for industry-specific accountability principles and processes. We define and evaluate critically the implementation of key accountability principles in the gambling industry, namely addressing algorithmic bias and model explainability, before concluding and discussing directions for future work based on our findings. Keywords: Accountability, Explainable AI, Algorithmic Bias, Regulation.",
        "authors": [
            "Chris Percy",
            "Simo Dragicevic",
            "Sanjoy Sarkar",
            "Artur S. d'Avila Garcez"
        ],
        "submitted_date": "2021-10-08T16:37:11+00:00",
        "pdf_url": "https://arxiv.org/pdf/2110.09232v1",
        "similarity_score": 0.3355519473552704
    },
    {
        "arxiv_id": "2305.15080v2",
        "title": "Visually-Situated Natural Language Understanding with Contrastive Reading Model and Frozen Large Language Models",
        "abstract": "Recent advances in Large Language Models (LLMs) have stimulated a surge of research aimed at extending their applications to the visual domain. While these models exhibit promise in generating abstract image captions and facilitating natural conversations, their performance on text-rich images still requires improvement. In this paper, we introduce Contrastive Reading Model (Cream), a novel neural architecture designed to enhance the language-image understanding capability of LLMs by capturing intricate details that are often overlooked in existing methods. Cream combines vision and auxiliary encoders, fortified by a contrastive feature alignment technique, to achieve a more effective comprehension of language information in visually situated contexts within the images. Our approach bridges the gap between vision and language understanding, paving the way for the development of more sophisticated Document Intelligence Assistants. Through rigorous evaluations across diverse visually-situated language understanding tasks that demand reasoning capabilities, we demonstrate the compelling performance of Cream, positioning it as a prominent model in the field of visual document understanding. We provide our codebase and newly-generated datasets at https://github.com/naver-ai/cream .",
        "authors": [
            "Geewook Kim",
            "Hodong Lee",
            "Daehee Kim",
            "Haeji Jung",
            "Sanghee Park",
            "Yoonsik Kim",
            "Sangdoo Yun",
            "Taeho Kil",
            "Bado Lee",
            "Seunghyun Park"
        ],
        "submitted_date": "2023-05-24T11:59:13+00:00",
        "pdf_url": "https://arxiv.org/pdf/2305.15080v2",
        "similarity_score": 0.33529558777809143
    },
    {
        "arxiv_id": "1806.04497v1",
        "title": "A Virtual Environment with Multi-Robot Navigation, Analytics, and Decision Support for Critical Incident Investigation",
        "abstract": "Accidents and attacks that involve chemical, biological, radiological/nuclear or explosive (CBRNE) substances are rare, but can be of high consequence. Since the investigation of such events is not anybody's routine work, a range of AI techniques can reduce investigators' cognitive load and support decision-making, including: planning the assessment of the scene; ongoing evaluation and updating of risks; control of autonomous vehicles for collecting images and sensor data; reviewing images/videos for items of interest; identification of anomalies; and retrieval of relevant documentation. Because of the rare and high-risk nature of these events, realistic simulations can support the development and evaluation of AI-based tools. We have developed realistic models of CBRNE scenarios and implemented an initial set of tools.",
        "authors": [
            "David L. Smyth",
            "James Fennell",
            "Sai Abinesh",
            "Nazli B. Karimi",
            "Frank G. Glavin",
            "Ihsan Ullah",
            "Brett Drury",
            "Michael G. Madden"
        ],
        "submitted_date": "2018-06-12T13:26:56+00:00",
        "pdf_url": "https://arxiv.org/pdf/1806.04497v1",
        "similarity_score": 0.3307560682296753
    },
    {
        "arxiv_id": "2509.24340v1",
        "title": "humancompatible.detect: a Python Toolkit for Detecting Bias in AI Models",
        "abstract": "There is a strong recent emphasis on trustworthy AI. In particular, international regulations, such as the AI Act, demand that AI practitioners measure data quality on the input and estimate bias on the output of high-risk AI systems. However, there are many challenges involved, including scalability (MMD) and computability (Wasserstein-1) issues of traditional methods for estimating distances on measure spaces. Here, we present humancompatible.detect, a toolkit for bias detection that addresses these challenges. It incorporates two newly developed methods to detect and evaluate bias: maximum subgroup discrepancy (MSD) and subsampled $\\ell_\\infty$ distances. It has an easy-to-use API documented with multiple examples. humancompatible.detect is licensed under the Apache License, Version 2.0.",
        "authors": [
            "German M. Matilla",
            "Jiri Nemecek",
            "Illia Kryvoviaz",
            "Jakub Marecek"
        ],
        "submitted_date": "2025-09-29T06:43:24+00:00",
        "pdf_url": "https://arxiv.org/pdf/2509.24340v1",
        "similarity_score": 0.3300703763961792
    },
    {
        "arxiv_id": "2505.03777v2",
        "title": "MolMole: Molecule Mining from Scientific Literature",
        "abstract": "The extraction of molecular structures and reaction data from scientific documents is challenging due to their varied, unstructured chemical formats and complex document layouts. To address this, we introduce MolMole, a vision-based deep learning framework that unifies molecule detection, reaction diagram parsing, and optical chemical structure recognition (OCSR) into a single pipeline for automating the extraction of chemical data directly from page-level documents. Recognizing the lack of a standard page-level benchmark and evaluation metric, we also present a testset of 550 pages annotated with molecule bounding boxes, reaction labels, and MOLfiles, along with a novel evaluation metric. Experimental results demonstrate that MolMole outperforms existing toolkits on both our benchmark and public datasets. The benchmark testset will be publicly available, and the MolMole toolkit will be accessible soon through an interactive demo on the LG AI Research website. For commercial inquiries, please contact us at \\href{mailto:contact_ddu@lgresearch.ai}{contact\\_ddu@lgresearch.ai}.",
        "authors": [
            "LG AI Research",
            "Sehyun Chun",
            "Jiye Kim",
            "Ahra Jo",
            "Yeonsik Jo",
            "Seungyul Oh",
            "Seungjun Lee",
            "Kwangrok Ryoo",
            "Jongmin Lee",
            "Seung Hwan Kim",
            "Byung Jun Kang",
            "Soonyoung Lee",
            "Jun Ha Park",
            "Chanwoo Moon",
            "Jiwon Ham",
            "Haein Lee",
            "Heejae Han",
            "Jaeseung Byun",
            "Soojong Do",
            "Minju Ha",
            "Dongyun Kim",
            "Kyunghoon Bae",
            "Woohyung Lim",
            "Edward Hwayoung Lee",
            "Yongmin Park",
            "Jeongsang Yu",
            "Gerrard Jeongwon Jo",
            "Yeonjung Hong",
            "Kyungjae Yoo",
            "Sehui Han",
            "Jaewan Lee",
            "Changyoung Park",
            "Kijeong Jeon",
            "Sihyuk Yi"
        ],
        "submitted_date": "2025-04-30T09:30:04+00:00",
        "pdf_url": "https://arxiv.org/pdf/2505.03777v2",
        "similarity_score": 0.32808825373649597
    },
    {
        "arxiv_id": "2304.09374v1",
        "title": "Shuffle & Divide: Contrastive Learning for Long Text",
        "abstract": "We propose a self-supervised learning method for long text documents based on contrastive learning. A key to our method is Shuffle and Divide (SaD), a simple text augmentation algorithm that sets up a pretext task required for contrastive updates to BERT-based document embedding. SaD splits a document into two sub-documents containing randomly shuffled words in the entire documents. The sub-documents are considered positive examples, leaving all other documents in the corpus as negatives. After SaD, we repeat the contrastive update and clustering phases until convergence. It is naturally a time-consuming, cumbersome task to label text documents, and our method can help alleviate human efforts, which are most expensive resources in AI. We have empirically evaluated our method by performing unsupervised text classification on the 20 Newsgroups, Reuters-21578, BBC, and BBCSport datasets. In particular, our method pushes the current state-of-the-art, SS-SB-MT, on 20 Newsgroups by 20.94% in accuracy. We also achieve the state-of-the-art performance on Reuters-21578 and exceptionally-high accuracy performances (over 95%) for unsupervised classification on the BBC and BBCSport datasets.",
        "authors": [
            "Joonseok Lee",
            "Seongho Joe",
            "Kyoungwon Park",
            "Bogun Kim",
            "Hoyoung Kang",
            "Jaeseon Park",
            "Youngjune Gwon"
        ],
        "submitted_date": "2023-04-19T02:02:29+00:00",
        "pdf_url": "https://arxiv.org/pdf/2304.09374v1",
        "similarity_score": 0.3230815529823303
    },
    {
        "arxiv_id": "2310.08560v2",
        "title": "MemGPT: Towards LLMs as Operating Systems",
        "abstract": "Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis. To enable using context beyond limited context windows, we propose virtual context management, a technique drawing inspiration from hierarchical memory systems in traditional operating systems that provide the appearance of large memory resources through data movement between fast and slow memory. Using this technique, we introduce MemGPT (Memory-GPT), a system that intelligently manages different memory tiers in order to effectively provide extended context within the LLM's limited context window, and utilizes interrupts to manage control flow between itself and the user. We evaluate our OS-inspired design in two domains where the limited context windows of modern LLMs severely handicaps their performance: document analysis, where MemGPT is able to analyze large documents that far exceed the underlying LLM's context window, and multi-session chat, where MemGPT can create conversational agents that remember, reflect, and evolve dynamically through long-term interactions with their users. We release MemGPT code and data for our experiments at https://memgpt.ai.",
        "authors": [
            "Charles Packer",
            "Sarah Wooders",
            "Kevin Lin",
            "Vivian Fang",
            "Shishir G. Patil",
            "Ion Stoica",
            "Joseph E. Gonzalez"
        ],
        "submitted_date": "2023-10-12T17:51:32+00:00",
        "pdf_url": "https://arxiv.org/pdf/2310.08560v2",
        "similarity_score": 0.31990140676498413
    },
    {
        "arxiv_id": "1802.06950v1",
        "title": "TAP-DLND 1.0 : A Corpus for Document Level Novelty Detection",
        "abstract": "Detecting novelty of an entire document is an Artificial Intelligence (AI) frontier problem that has widespread NLP applications, such as extractive document summarization, tracking development of news events, predicting impact of scholarly articles, etc. Important though the problem is, we are unaware of any benchmark document level data that correctly addresses the evaluation of automatic novelty detection techniques in a classification framework. To bridge this gap, we present here a resource for benchmarking the techniques for document level novelty detection. We create the resource via event-specific crawling of news documents across several domains in a periodic manner. We release the annotated corpus with necessary statistics and show its use with a developed system for the problem in concern.",
        "authors": [
            "Tirthankar Ghosal",
            "Amitra Salam",
            "Swati Tiwari",
            "Asif Ekbal",
            "Pushpak Bhattacharyya"
        ],
        "submitted_date": "2018-02-20T03:42:11+00:00",
        "pdf_url": "https://arxiv.org/pdf/1802.06950v1",
        "similarity_score": 0.31727802753448486
    },
    {
        "arxiv_id": "2003.08837v1",
        "title": "Vulnerabilities of Connectionist AI Applications: Evaluation and Defence",
        "abstract": "This article deals with the IT security of connectionist artificial intelligence (AI) applications, focusing on threats to integrity, one of the three IT security goals. Such threats are for instance most relevant in prominent AI computer vision applications. In order to present a holistic view on the IT security goal integrity, many additional aspects such as interpretability, robustness and documentation are taken into account. A comprehensive list of threats and possible mitigations is presented by reviewing the state-of-the-art literature. AI-specific vulnerabilities such as adversarial attacks and poisoning attacks as well as their AI-specific root causes are discussed in detail. Additionally and in contrast to former reviews, the whole AI supply chain is analysed with respect to vulnerabilities, including the planning, data acquisition, training, evaluation and operation phases. The discussion of mitigations is likewise not restricted to the level of the AI system itself but rather advocates viewing AI systems in the context of their supply chains and their embeddings in larger IT infrastructures and hardware devices. Based on this and the observation that adaptive attackers may circumvent any single published AI-specific defence to date, the article concludes that single protective measures are not sufficient but rather multiple measures on different levels have to be combined to achieve a minimum level of IT security for AI applications.",
        "authors": [
            "Christian Berghoff",
            "Matthias Neu",
            "Arndt von Twickel"
        ],
        "submitted_date": "2020-03-18T12:33:59+00:00",
        "pdf_url": "https://arxiv.org/pdf/2003.08837v1",
        "similarity_score": 0.31276658177375793
    },
    {
        "arxiv_id": "2509.21325v1",
        "title": "PIR-RAG: A System for Private Information Retrieval in Retrieval-Augmented Generation",
        "abstract": "Retrieval-Augmented Generation (RAG) has become a foundational component of modern AI systems, yet it introduces significant privacy risks by exposing user queries to service providers. To address this, we introduce PIR-RAG, a practical system for privacy-preserving RAG. PIR-RAG employs a novel architecture that uses coarse-grained semantic clustering to prune the search space, combined with a fast, lattice-based Private Information Retrieval (PIR) protocol. This design allows for the efficient retrieval of entire document clusters, uniquely optimizing for the end-to-end RAG workflow where full document content is required. Our comprehensive evaluation against strong baseline architectures, including graph-based PIR and Tiptoe-style private scoring, demonstrates PIR-RAG's scalability and its superior performance in terms of \"RAG-Ready Latency\"-the true end-to-end time required to securely fetch content for an LLM. Our work establishes PIR-RAG as a viable and highly efficient solution for privacy in large-scale AI systems.",
        "authors": [
            "Baiqiang Wang",
            "Qian Lou",
            "Mengxin Zheng",
            "Dongfang Zhao"
        ],
        "submitted_date": "2025-09-01T07:28:35+00:00",
        "pdf_url": "https://arxiv.org/pdf/2509.21325v1",
        "similarity_score": 0.3066929280757904
    },
    {
        "arxiv_id": "2508.01887v1",
        "title": "Complete Evasion, Zero Modification: PDF Attacks on AI Text Detection",
        "abstract": "AI-generated text detectors have become essential tools for maintaining content authenticity, yet their robustness against evasion attacks remains questionable. We present PDFuzz, a novel attack that exploits the discrepancy between visual text layout and extraction order in PDF documents. Our method preserves exact textual content while manipulating character positioning to scramble extraction sequences. We evaluate this approach against the ArguGPT detector using a dataset of human and AI-generated text. Our results demonstrate complete evasion: detector performance drops from (93.6 $\\pm$ 1.4) % accuracy and 0.938 $\\pm$ 0.014 F1 score to random-level performance ((50.4 $\\pm$ 3.2) % accuracy, 0.0 F1 score) while maintaining perfect visual fidelity. Our work reveals a vulnerability in current detection systems that is inherent to PDF document structures and underscores the need for implementing sturdy safeguards against such attacks. We make our code publicly available at https://github.com/ACMCMC/PDFuzz.",
        "authors": [
            "Aldan Creo"
        ],
        "submitted_date": "2025-08-03T18:43:41+00:00",
        "pdf_url": "https://arxiv.org/pdf/2508.01887v1",
        "similarity_score": 0.3038802444934845
    },
    {
        "arxiv_id": "2310.03777v1",
        "title": "PrIeD-KIE: Towards Privacy Preserved Document Key Information Extraction",
        "abstract": "In this paper, we introduce strategies for developing private Key Information Extraction (KIE) systems by leveraging large pretrained document foundation models in conjunction with differential privacy (DP), federated learning (FL), and Differentially Private Federated Learning (DP-FL). Through extensive experimentation on six benchmark datasets (FUNSD, CORD, SROIE, WildReceipts, XFUND, and DOCILE), we demonstrate that large document foundation models can be effectively fine-tuned for the KIE task under private settings to achieve adequate performance while maintaining strong privacy guarantees. Moreover, by thoroughly analyzing the impact of various training and model parameters on model performance, we propose simple yet effective guidelines for achieving an optimal privacy-utility trade-off for the KIE task under global DP. Finally, we introduce FeAm-DP, a novel DP-FL algorithm that enables efficiently upscaling global DP from a standalone context to a multi-client federated environment. We conduct a comprehensive evaluation of the algorithm across various client and privacy settings, and demonstrate its capability to achieve comparable performance and privacy guarantees to standalone DP, even when accommodating an increasing number of participating clients. Overall, our study offers valuable insights into the development of private KIE systems, and highlights the potential of document foundation models for privacy-preserved Document AI applications. To the best of authors' knowledge, this is the first work that explores privacy preserved document KIE using document foundation models.",
        "authors": [
            "Saifullah Saifullah",
            "Stefan Agne",
            "Andreas Dengel",
            "Sheraz Ahmed"
        ],
        "submitted_date": "2023-10-05T12:13:00+00:00",
        "pdf_url": "https://arxiv.org/pdf/2310.03777v1",
        "similarity_score": 0.29249417781829834
    },
    {
        "arxiv_id": "2507.18264v2",
        "title": "Zero-shot OCR Accuracy of Low-Resourced Languages: A Comparative Analysis on Sinhala and Tamil",
        "abstract": "Solving the problem of Optical Character Recognition (OCR) on printed text for Latin and its derivative scripts can now be considered settled due to the volumes of research done on English and other High-Resourced Languages (HRL). However, for Low-Resourced Languages (LRL) that use unique scripts, it remains an open problem. This study presents a comparative analysis of the zero-shot performance of six distinct OCR engines on two LRLs: Sinhala and Tamil. The selected engines include both commercial and open-source systems, aiming to evaluate the strengths of each category. The Cloud Vision API, Surya, Document AI, and Tesseract were evaluated for both Sinhala and Tamil, while Subasa OCR and EasyOCR were examined for only one language due to their limitations. The performance of these systems was rigorously analysed using five measurement techniques to assess accuracy at both the character and word levels. According to the findings, Surya delivered the best performance for Sinhala across all metrics, with a WER of 2.61%. Conversely, Document AI excelled across all metrics for Tamil, highlighted by a very low CER of 0.78%. In addition to the above analysis, we also introduce a novel synthetic Tamil OCR benchmarking dataset.",
        "authors": [
            "Nevidu Jayatilleke",
            "Nisansa de Silva"
        ],
        "submitted_date": "2025-07-24T10:08:43+00:00",
        "pdf_url": "https://arxiv.org/pdf/2507.18264v2",
        "similarity_score": 0.2689116895198822
    },
    {
        "arxiv_id": "2510.09263v1",
        "title": "SynthID-Image: Image watermarking at internet scale",
        "abstract": "We introduce SynthID-Image, a deep learning-based system for invisibly watermarking AI-generated imagery. This paper documents the technical desiderata, threat models, and practical challenges of deploying such a system at internet scale, addressing key requirements of effectiveness, fidelity, robustness, and security. SynthID-Image has been used to watermark over ten billion images and video frames across Google's services and its corresponding verification service is available to trusted testers. For completeness, we present an experimental evaluation of an external model variant, SynthID-O, which is available through partnerships. We benchmark SynthID-O against other post-hoc watermarking methods from the literature, demonstrating state-of-the-art performance in both visual quality and robustness to common image perturbations. While this work centers on visual media, the conclusions on deployment, constraints, and threat modeling generalize to other modalities, including audio. This paper provides a comprehensive documentation for the large-scale deployment of deep learning-based media provenance systems.",
        "authors": [
            "Sven Gowal",
            "Rudy Bunel",
            "Florian Stimberg",
            "David Stutz",
            "Guillermo Ortiz-Jimenez",
            "Christina Kouridi",
            "Mel Vecerik",
            "Jamie Hayes",
            "Sylvestre-Alvise Rebuffi",
            "Paul Bernard",
            "Chris Gamble",
            "Mikl√≥s Z. Horv√°th",
            "Fabian Kaczmarczyck",
            "Alex Kaskasoli",
            "Aleksandar Petrov",
            "Ilia Shumailov",
            "Meghana Thotakuri",
            "Olivia Wiles",
            "Jessica Yung",
            "Zahra Ahmed",
            "Victor Martin",
            "Simon Rosen",
            "Christopher Savƒçak",
            "Armin Senoner",
            "Nidhi Vyas",
            "Pushmeet Kohli"
        ],
        "submitted_date": "2025-10-10T11:03:31+00:00",
        "pdf_url": "https://arxiv.org/pdf/2510.09263v1",
        "similarity_score": 0.25960269570350647
    },
    {
        "arxiv_id": "2502.04365v1",
        "title": "AI-Based Thermal Video Analysis in Privacy-Preserving Healthcare: A Case Study on Detecting Time of Birth",
        "abstract": "Approximately 10% of newborns need some assistance to start breathing and 5\\% proper ventilation. It is crucial that interventions are initiated as soon as possible after birth. Accurate documentation of Time of Birth (ToB) is thereby essential for documenting and improving newborn resuscitation performance. However, current clinical practices rely on manual recording of ToB, typically with minute precision. In this study, we present an AI-driven, video-based system for automated ToB detection using thermal imaging, designed to preserve the privacy of healthcare providers and mothers by avoiding the use of identifiable visual data. Our approach achieves 91.4% precision and 97.4% recall in detecting ToB within thermal video clips during performance evaluation. Additionally, our system successfully identifies ToB in 96% of test cases with an absolute median deviation of 1 second compared to manual annotations. This method offers a reliable solution for improving ToB documentation and enhancing newborn resuscitation outcomes.",
        "authors": [
            "Jorge Garc√≠a-Torres",
            "√òyvind Meinich-Bache",
            "Siren Rettedal",
            "Kjersti Engan"
        ],
        "submitted_date": "2025-02-05T07:01:49+00:00",
        "pdf_url": "https://arxiv.org/pdf/2502.04365v1",
        "similarity_score": 0.24599091708660126
    },
    {
        "arxiv_id": "2212.00822v1",
        "title": "Navigating an Ocean of Video Data: Deep Learning for Humpback Whale Classification in YouTube Videos",
        "abstract": "Image analysis technologies empowered by artificial intelligence (AI) have proved images and videos to be an opportune source of data to learn about humpback whale (Megaptera novaeangliae) population sizes and dynamics. With the advent of social media, platforms such as YouTube present an abundance of video data across spatiotemporal contexts documenting humpback whale encounters from users worldwide. In our work, we focus on automating the classification of YouTube videos as relevant or irrelevant based on whether they document a true humpback whale encounter or not via deep learning. We use a CNN-RNN architecture pretrained on the ImageNet dataset for classification of YouTube videos as relevant or irrelevant. We achieve an average 85.7% accuracy, and 84.7% (irrelevant)/ 86.6% (relevant) F1 scores using five-fold cross validation for evaluation on the dataset. We show that deep learning can be used as a time-efficient step to make social media a viable source of image and video data for biodiversity assessments.",
        "authors": [
            "Michelle Ramirez"
        ],
        "submitted_date": "2022-12-01T19:19:46+00:00",
        "pdf_url": "https://arxiv.org/pdf/2212.00822v1",
        "similarity_score": 0.23669742047786713
    },
    {
        "arxiv_id": "2308.10647v2",
        "title": "bbOCR: An Open-source Multi-domain OCR Pipeline for Bengali Documents",
        "abstract": "Despite the existence of numerous Optical Character Recognition (OCR) tools, the lack of comprehensive open-source systems hampers the progress of document digitization in various low-resource languages, including Bengali. Low-resource languages, especially those with an alphasyllabary writing system, suffer from the lack of large-scale datasets for various document OCR components such as word-level OCR, document layout extraction, and distortion correction; which are available as individual modules in high-resource languages. In this paper, we introduce Bengali$.$AI-BRACU-OCR (bbOCR): an open-source scalable document OCR system that can reconstruct Bengali documents into a structured searchable digitized format that leverages a novel Bengali text recognition model and two novel synthetic datasets. We present extensive component-level and system-level evaluation: both use a novel diversified evaluation dataset and comprehensive evaluation metrics. Our extensive evaluation suggests that our proposed solution is preferable over the current state-of-the-art Bengali OCR systems. The source codes and datasets are available here: https://bengaliai.github.io/bbocr.",
        "authors": [
            "Imam Mohammad Zulkarnain",
            "Shayekh Bin Islam",
            "Md. Zami Al Zunaed Farabe",
            "Md. Mehedi Hasan Shawon",
            "Jawaril Munshad Abedin",
            "Beig Rajibul Hasan",
            "Marsia Haque",
            "Istiak Shihab",
            "Syed Mobassir",
            "MD. Nazmuddoha Ansary",
            "Asif Sushmit",
            "Farig Sadeque"
        ],
        "submitted_date": "2023-08-21T11:35:28+00:00",
        "pdf_url": "https://arxiv.org/pdf/2308.10647v2",
        "similarity_score": 0.22578468918800354
    },
    {
        "arxiv_id": "2408.12491v2",
        "title": "AI in radiological imaging of soft-tissue and bone tumours: a systematic review evaluating against CLAIM and FUTURE-AI guidelines",
        "abstract": "Soft-tissue and bone tumours (STBT) are rare, diagnostically challenging lesions with variable clinical behaviours and treatment approaches. This systematic review provides an overview of Artificial Intelligence (AI) methods using radiological imaging for diagnosis and prognosis of these tumours, highlighting challenges in clinical translation, and evaluating study alignment with the Checklist for AI in Medical Imaging (CLAIM) and the FUTURE-AI international consensus guidelines for trustworthy and deployable AI to promote the clinical translation of AI methods. The review covered literature from several bibliographic databases, including papers published before 17/07/2024. Original research in peer-reviewed journals focused on radiology-based AI for diagnosing or prognosing primary STBT was included. Exclusion criteria were animal, cadaveric, or laboratory studies, and non-English papers. Abstracts were screened by two of three independent reviewers for eligibility. Eligible papers were assessed against guidelines by one of three independent reviewers. The search identified 15,015 abstracts, from which 325 articles were included for evaluation. Most studies performed moderately on CLAIM, averaging a score of 28.9$\\pm$7.5 out of 53, but poorly on FUTURE-AI, averaging 5.1$\\pm$2.1 out of 30. Imaging-AI tools for STBT remain at the proof-of-concept stage, indicating significant room for improvement. Future efforts by AI developers should focus on design (e.g. define unmet clinical need, intended clinical setting and how AI would be integrated in clinical workflow), development (e.g. build on previous work, explainability), evaluation (e.g. evaluating and addressing biases, evaluating AI against best practices), and data reproducibility and availability (making documented code and data publicly available). Following these recommendations could improve clinical translation of AI methods.",
        "authors": [
            "Douwe J. Spaanderman",
            "Matthew Marzetti",
            "Xinyi Wan",
            "Andrew F. Scarsbrook",
            "Philip Robinson",
            "Edwin H. G. Oei",
            "Jacob J. Visser",
            "Robert Hemke",
            "Kirsten van Langevelde",
            "David F. Hanff",
            "Geert J. L. H. van Leenders",
            "Cornelis Verhoef",
            "Dirk J. Gru√ºhagen",
            "Wiro J. Niessen",
            "Stefan Klein",
            "Martijn P. A. Starmans"
        ],
        "submitted_date": "2024-08-22T15:31:48+00:00",
        "pdf_url": "https://arxiv.org/pdf/2408.12491v2",
        "similarity_score": 0.17386090755462646
    },
    {
        "arxiv_id": "2410.03694v1",
        "title": "Making Data: The Work Behind Artificial Intelligence",
        "abstract": "AI generates both enthusiasm and disillusionment, with promises that often go unfulfilled. It is therefore not surprising that human labor, which is its fundamental component, is also subject to these same deceptions. The development of \"smart technologies\" depends, at different stages, on a multitude of precarious, underpaid and invisible workers, who, dispersed globally, carry out repetitive, fragmented activities, paid per task and completed in a few seconds. These are workers who label data to train algorithms, through tasks that require the intuitive, creative and cognitive abilities of human beings, such as categorizing images, classifying advertisements, transcribing audio and video, evaluating advertisements, moderating content on social media, labeling human anatomical points of interest, digitizing documents, etc. This form of work is often referred to as \"microwork\". Our contribution, which documents the conditions of microwork in Brazil and offers portraits of the workers, is a step in the wider effort to overcome the current state of invisibilization. It opens up avenues for future research, with the aim of better characterizing this new form of work, tracing its changes over time in relation to the dynamics of globalization and, ideally, identifying levers for action and transitions.",
        "authors": [
            "Matheus Viana Braz",
            "Paola Tubaro",
            "Antonio A. Casilli"
        ],
        "submitted_date": "2024-09-23T08:12:24+00:00",
        "pdf_url": "https://arxiv.org/pdf/2410.03694v1",
        "similarity_score": 0.13987815380096436
    }
]